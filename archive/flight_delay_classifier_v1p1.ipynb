{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9108241-a945-4da3-bdbb-b0e1bb2b729c",
   "metadata": {},
   "source": [
    "# Flight Delay Classifier--PyTorch Edition\n",
    "As PyTorch has stopped supporting conda packages for their module, I am making this separate notebook to use a PyTorch neural net in the classification of flights as either delayed or on-time. The working process will be the same as flight_delay_classifier_v1, just with the PyTorch framework so we will gloss over some of the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4691af3-f004-4a9e-a4a8-aa05f7592cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import FunctionTransformer, RobustScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce69b47d-9da9-4a83-8b07-a211f0d7c4a7",
   "metadata": {},
   "source": [
    "## Data Prepartion\n",
    "Here we prepare our functions to load and transform our data. We will be making use of sklearn pipelines before converting them to PyTorch tensors.\n",
    "\n",
    "As one other note, we will be dropping the carrier codes from our feature set given that it was much less important than departure date and time for our random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d7e61a5-361a-4c7a-9b11-02c7d3c449c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the PDX files\n",
    "def load_flight_info(data_dir, pat_str):\n",
    "    \"\"\"\n",
    "    Loads a single data frame containing all the data in data_dir with files matching the pat_str\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for entry in os.listdir(data_dir):\n",
    "        # Construct full path\n",
    "        full_path = os.path.join(data_dir, entry)\n",
    "\n",
    "        # Check if it is actually a file and if it matches the pattern\n",
    "        if os.path.isfile(full_path) and fnmatch.fnmatch(entry, pat_str):\n",
    "            df = pd.concat([df, pd.read_csv(full_path, skiprows=7)])\n",
    "            print(f'Loaded {full_path} added to dataframe.')\n",
    "\n",
    "    return df\n",
    "\n",
    "# sine and cosine transformer classes to be used for date and time variables\n",
    "class SinTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, period=1):\n",
    "        self.period = period\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = check_array(X)\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        return np.sin(2*np.pi * X / self.period)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if input_features is None:\n",
    "            # Use feature_names_in_ if available (set during fit)\n",
    "            input_features = getattr(self, \"feature_names_in_\", [f'x{i}' for i in range(self.n_features_in_)])\n",
    "\n",
    "        # Define how feature names are transformed\n",
    "        return [f'{col}_sin' for col in input_features]\n",
    "\n",
    "class CosTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, period=1):\n",
    "        self.period = period\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = check_array(X)\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        return np.cos(2*np.pi * X / self.period)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if input_features is None:\n",
    "            # Use feature_names_in_ if available (set during fit)\n",
    "            input_features = getattr(self, \"feature_names_in_\", [f'x{i}' for i in range(self.n_features_in_)])\n",
    "\n",
    "        # Define how feature names are transformed\n",
    "        return [f'{col}_cos' for col in input_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5efd622b-87dd-4952-b70f-7617d90c6516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_AA_Delays.csv added to dataframe.\n",
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_AS_Delays.csv added to dataframe.\n",
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_B6_Delays.csv added to dataframe.\n",
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_DL_Delays.csv added to dataframe.\n",
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_F9_Delays.csv added to dataframe.\n",
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_G4_Delays.csv added to dataframe.\n",
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_HA_Delays.csv added to dataframe.\n",
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_MQ_2_Delays.csv added to dataframe.\n",
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_MQ_Delays.csv added to dataframe.\n",
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_NK_Delays.csv added to dataframe.\n",
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_OO_Delays.csv added to dataframe.\n",
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_QX_Delays.csv added to dataframe.\n",
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_UA_Delays.csv added to dataframe.\n",
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_WN_Delays.csv added to dataframe.\n",
      "Loaded C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays\\PDX_YV_Delays.csv added to dataframe.\n"
     ]
    }
   ],
   "source": [
    "df = load_flight_info('C:/Users/dloso/Documents/Data Science/flight-delay-forecasting/data/delays', 'PDX*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a592ff9d-3143-49a1-8176-3cf70c8d37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoder for the categorical values\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('encoder', one_hot_encoder)\n",
    "])\n",
    "\n",
    "# Now set up which attributes get which transforms using a ColumnTransformer\n",
    "time_attribs = ['DepartureTimeHour']\n",
    "date_attribs = ['DayOfYear']\n",
    "cat_attribs = ['Carrier Code']#, 'Destination Airport']\n",
    "\n",
    "cyclic_cossin_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('categorical', one_hot_encoder, cat_attribs),\n",
    "        ('day_sin', SinTransformer(period=365), date_attribs),\n",
    "        ('day_cos', CosTransformer(period=365), date_attribs),\n",
    "        ('hour_sin', SinTransformer(period=24), time_attribs),\n",
    "        ('hour_cos', CosTransformer(period=24), time_attribs),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "081ec01b-a3c8-4966-815f-607e93a30215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Delayed'] = np.where(df['Departure delay (Minutes)'] > 0, 1, 0)\n",
    "\n",
    "df.dropna(subset='Departure delay (Minutes)', inplace=True)\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date (MM/DD/YYYY)'])\n",
    "df['DayOfYear'] = df['Date'].dt.dayofyear\n",
    "df['DepartureTimeHour'] = pd.to_datetime(df['Scheduled departure time'], format='%H:%M').dt.hour + pd.to_datetime(df['Scheduled departure time'], format='%H:%M').dt.minute / 60\n",
    "\n",
    "df = df[['DayOfYear', 'DepartureTimeHour', 'Carrier Code', 'Destination Airport', 'Delayed']]\n",
    "df = df[df['Carrier Code'] != ' SOURCE: Bureau of Transportation Statistics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "60abd3c3-9ec0-4141-828a-0c2659ab8082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the destination airport\n",
    "df = df[['DayOfYear', 'DepartureTimeHour', 'Carrier Code', 'Delayed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7f690cba-97d5-49fc-b2c9-d104aba8c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.DataFrame(df['Delayed'])\n",
    "df_features = df.drop('Delayed', axis=1)\n",
    "df_features = cyclic_cossin_transformer.fit_transform(df_features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, df_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56015894-ff3a-4bdf-9246-c0789f109c9a",
   "metadata": {},
   "source": [
    "## PyTorch Conversion\n",
    "Luckily, PyTorch makes it simple to transition between dataframes and their tensors so we can quickly change over our features and labels for training. Note that we did have the OneHotEncoder put out a dense matrix as that is required for the conversion to a PyTorch tensor--sparse matrices are not allowed.\n",
    "\n",
    "We will also build out Dataset and DataLoader classes to make sure that we are handling data efficiently in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d32e9574-d4e3-4230-a9b1-2eca92c39baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train.values).long() # Use .long() for classification labels\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_test_tensor = torch.from_numpy(y_test.values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d69e839e-5c20-40ad-b55c-3e2f4b223bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomTabularDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.length = len(labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = CustomTabularDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = CustomTabularDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff4d13e-9a90-4906-b129-d00265b2ed25",
   "metadata": {},
   "source": [
    "## Building the Neural Net\n",
    "Finally, let's build out the neural net and see how it performs. We'll build it out with 2 hidden layers of 10 nodes each. More detailed investigations of the most accurate and efficient hidden layers layout will wait for a future iteration.\n",
    "\n",
    "We'll use binary cross-entropy for the loss function and Adam as the optimizer as standard choices and then run the training for 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "759c0d72-d63a-4468-8608-ffe287610cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, 10)\n",
    "        self.layer_2 = nn.Linear(10, 10)\n",
    "        self.layer_out = nn.Linear(10, 1) # 1 output unit for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = torch.sigmoid(self.layer_out(x)) # Sigmoid activation for binary output\n",
    "        return x\n",
    "\n",
    "model = BinaryClassifier(input_dim=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eb1b0fb7-d69b-47e2-af26-43075131e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss() # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eafb9a10-07fd-48b2-afcf-967243e4196e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.4585\n",
      "Epoch 6/30, Loss: 0.6080\n",
      "Epoch 11/30, Loss: 0.6455\n",
      "Epoch 16/30, Loss: 0.6248\n",
      "Epoch 21/30, Loss: 0.5758\n",
      "Epoch 26/30, Loss: 0.6656\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Reshape labels to match output shape (e.g., (batch_size, 1))\n",
    "        # labels = labels.unsqueeze(1).float()\n",
    "        labels = labels.float()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db63713d-f26e-4ca2-a728-dd584899065a",
   "metadata": {},
   "source": [
    "## Testing and Evaluating the Model\n",
    "We notice that the loss function does not monotonically decrease with increasing epochs, which suggests a few possibilities:\n",
    "1. The learning rate is too large.\n",
    "2. The loss function is unstable for the problem.\n",
    "3. The data cannot be sufficiently learned from.\n",
    "Having altered the learning rate and the loss function to no change, my conclusion is that there is simply not that much to be learned from this dataset. Interestingly, we arrive at a 68.5% accuracy on the test set, which is nearly identical to every other model we have attempted. This again suggests some limitation in the data (or my transformations of it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "95075309-25f0-4a25-bc0e-385a8c9e1e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 68.47%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Forward pass: get raw predictions (logits)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Determine the predicted class index (highest score)\n",
    "        predicted = torch.round(outputs.data)\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on test set: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8f5c0-7e52-4381-8fd5-2202da720dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
