{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a85689-26b3-4bc8-a054-0939c5ef111f",
   "metadata": {},
   "source": [
    "# Flight Delay Classifier v3\n",
    "This go round, the goal is to do everything I did correclty in v2, but make it cleaner and easier to read. I am also going to be pairing this with my medium write-ups so I will need to make sure I can easily process the data with or without the weather features as that will come in a later post. That means making changes like:\n",
    "* Dropping all unnecessary features from the get-go.\n",
    "* Training and evaluating models in discrete sections that are easily compared.\n",
    "* Writing up proper conclusions at the end.\n",
    "* Using the first person singular instead of the first person plural because I am not royalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88d68e8-892d-4269-8ba1-9c6c1ba32d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d6fcc-eeeb-42dd-b8e2-a7e17c26e676",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "### Loading the Data\n",
    "My first step is to physically load the data. Since last time, I have aggregated all of the flight info for PDX into a single file containing every carrier's data. I will load this and the weather data into two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20279c0d-6c2f-45e0-89ee-7502973c7e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_df = pd.read_csv('data/delays/concatenated_delays/PDX_delays.csv')\n",
    "weather_df = pd.read_csv('data/weather/PDX_weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66981b1d-262d-41ee-9ce3-93a98c8a5327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 594126 entries, 0 to 594125\n",
      "Data columns (total 18 columns):\n",
      " #   Column                                    Non-Null Count   Dtype  \n",
      "---  ------                                    --------------   -----  \n",
      " 0   Unnamed: 0                                594126 non-null  int64  \n",
      " 1   Carrier Code                              594126 non-null  object \n",
      " 2   Date (MM/DD/YYYY)                         594126 non-null  object \n",
      " 3   Flight Number                             594126 non-null  float64\n",
      " 4   Tail Number                               593220 non-null  object \n",
      " 5   Destination Airport                       594126 non-null  object \n",
      " 6   Scheduled departure time                  594126 non-null  object \n",
      " 7   Actual departure time                     594126 non-null  object \n",
      " 8   Scheduled elapsed time (Minutes)          594126 non-null  float64\n",
      " 9   Actual elapsed time (Minutes)             594126 non-null  float64\n",
      " 10  Departure delay (Minutes)                 594126 non-null  float64\n",
      " 11  Wheels-off time                           594126 non-null  object \n",
      " 12  Taxi-Out time (Minutes)                   594126 non-null  float64\n",
      " 13  Delay Carrier (Minutes)                   594126 non-null  float64\n",
      " 14  Delay Weather (Minutes)                   594126 non-null  float64\n",
      " 15  Delay National Aviation System (Minutes)  594126 non-null  float64\n",
      " 16  Delay Security (Minutes)                  594126 non-null  float64\n",
      " 17  Delay Late Aircraft Arrival (Minutes)     594126 non-null  float64\n",
      "dtypes: float64(10), int64(1), object(7)\n",
      "memory usage: 81.6+ MB\n"
     ]
    }
   ],
   "source": [
    "delay_df.info();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65dea9d-45ab-40fa-8068-d62e9642fcf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date (MM/DD/YYYY)</th>\n",
       "      <th>Departure delay (Minutes)</th>\n",
       "      <th>Scheduled departure time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>594126</td>\n",
       "      <td>594126.000000</td>\n",
       "      <td>594126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2023-07-07 13:59:46.839154944</td>\n",
       "      <td>10.814613</td>\n",
       "      <td>1900-01-01 13:23:25.213742848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>1900-01-01 00:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2022-05-20 00:00:00</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1900-01-01 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2023-07-29 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900-01-01 13:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2024-09-02 00:00:00</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1900-01-01 17:35:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2025-09-30 00:00:00</td>\n",
       "      <td>3024.000000</td>\n",
       "      <td>1900-01-01 23:59:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>39.733229</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Date (MM/DD/YYYY)  Departure delay (Minutes)  \\\n",
       "count                         594126              594126.000000   \n",
       "mean   2023-07-07 13:59:46.839154944                  10.814613   \n",
       "min              2021-01-01 00:00:00                 -60.000000   \n",
       "25%              2022-05-20 00:00:00                  -4.000000   \n",
       "50%              2023-07-29 00:00:00                   0.000000   \n",
       "75%              2024-09-02 00:00:00                  11.000000   \n",
       "max              2025-09-30 00:00:00                3024.000000   \n",
       "std                              NaN                  39.733229   \n",
       "\n",
       "            Scheduled departure time  \n",
       "count                         594126  \n",
       "mean   1900-01-01 13:23:25.213742848  \n",
       "min              1900-01-01 00:04:00  \n",
       "25%              1900-01-01 09:00:00  \n",
       "50%              1900-01-01 13:10:00  \n",
       "75%              1900-01-01 17:35:00  \n",
       "max              1900-01-01 23:59:00  \n",
       "std                              NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay_df['Date (MM/DD/YYYY)'] = pd.to_datetime(delay_df['Date (MM/DD/YYYY)'])\n",
    "delay_df['Scheduled departure time'] = pd.to_datetime(delay_df['Scheduled departure time'], format='%H:%M')\n",
    "delay_df[['Date (MM/DD/YYYY)', 'Departure delay (Minutes)', 'Scheduled departure time']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57765d86-3d43-4cfe-bb47-0a4f0c6a9488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping duplicates: 594126 rows.\n",
      "After dropping duplicates: 594035 rows.\n"
     ]
    }
   ],
   "source": [
    "dup_cols = ['Carrier Code', 'Date (MM/DD/YYYY)', 'Flight Number', 'Scheduled departure time']\n",
    "print(f'Before dropping duplicates: {len(delay_df)} rows.')\n",
    "delay_df.drop_duplicates(inplace=True)\n",
    "print(f'After dropping duplicates: {len(delay_df)} rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "212c89d2-b0f0-42fb-a098-993c7b258e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping > 24 hour delays: 594035 rows.\n",
      "After dropping > 24 hour delays: 594005 rows.\n"
     ]
    }
   ],
   "source": [
    "print(f'Before dropping > 24 hour delays: {len(delay_df)} rows.')\n",
    "delay_df = delay_df[delay_df['Departure delay (Minutes)'] < 24*60]\n",
    "print(f'After dropping > 24 hour delays: {len(delay_df)} rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1b891d-9105-4910-8953-a0cea0c921dd",
   "metadata": {},
   "source": [
    "## Examining Possible Features\n",
    "I will be checking for trends among the features and the target variable--the flight delay status. Other delay variables outside of 'Departure delay (Minutes)' will be ignored as these are just subsets of the total delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e5cd929-548b-4197-b72e-9a6650ee8ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.dates as mdates\n",
    "bin_edges = pd.date_range(start=delay_df['Date (MM/DD/YYYY)'].min(), end=delay_df['Date (MM/DD/YYYY)'].max(), freq='2ME')\n",
    "\n",
    "plt.hist(delay_df['Date (MM/DD/YYYY)'], bins=bin_edges, edgecolor='black')\n",
    "plt.xlabel('Departure Date')\n",
    "plt.ylabel('Flight Counts')\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=6))\n",
    "plt.show()\n",
    "plt.savefig('../../../Downloads/Number_of_Flights_by_Date.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dab06448-610c-47f4-b653-36d0cbf298d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_df['Delayed'] = np.where(delay_df['Departure delay (Minutes)'] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fb9351b-64e1-4eae-a039-e1f23a52cc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "delay_df['DayOfYear'] = delay_df['Date (MM/DD/YYYY)'].dt.dayofyear\n",
    "delay_df['Hour'] = delay_df['Scheduled departure time'].dt.hour + delay_df['Scheduled departure time'].dt.minute / 60\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "delay_df.groupby(['Carrier Code', 'Delayed']).size().unstack(fill_value=0)[0].plot(\n",
    "    kind='bar', \n",
    "    stacked=True,\n",
    "    alpha=0.7,\n",
    "    legend=True,\n",
    "    title='Flight Delays by Carrier',\n",
    "    label='0',\n",
    "    ax=axs[0],\n",
    "    color='tab:blue'\n",
    ")\n",
    "delay_df.groupby(['Carrier Code', 'Delayed']).size().unstack(fill_value=0)[1].plot(\n",
    "    kind='bar', \n",
    "    stacked=True,\n",
    "    alpha=0.7,\n",
    "    legend=True,\n",
    "    label='1',\n",
    "    ax=axs[0],\n",
    "    color='tab:orange'\n",
    ")\n",
    "axs[0].set_xlabel('Carrier Code')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].legend(title='Delayed')\n",
    "\n",
    "delay_df.groupby('Delayed')['DayOfYear'].plot(\n",
    "    kind='hist',\n",
    "    stacked=True,\n",
    "    alpha=0.7,\n",
    "    bins=10,  # Adjust the number of bins as needed\n",
    "    # legend=True,\n",
    "    title='Flight Delays by Day of Year',\n",
    "    ax=axs[1]\n",
    ")\n",
    "axs[1].set_xlabel('Day of Year')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "\n",
    "delay_df.groupby('Delayed')['Hour'].plot(\n",
    "    kind='hist',\n",
    "    stacked=True,\n",
    "    alpha=0.7,\n",
    "    bins=10,  # Adjust the number of bins as needed\n",
    "    # legend=True,\n",
    "    title='Flight Delays by Hour',\n",
    "    ax=axs[2]\n",
    ")\n",
    "axs[2].set_xlabel('Hour')\n",
    "axs[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../../Downloads/Delays_by_Carrier_DayOfYear_Hour_PDX.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59f066cd-6262-4c0d-9576-99474fdeba10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Destination Airport  Delayed  Count  Proportion\n",
      "34                  CLE        1      1    1.000000\n",
      "155                 PBI        1      4    0.800000\n",
      "157                 PDX        1    365    0.762004\n",
      "121                 MEM        1   1327    0.647001\n",
      "105                 LGA        1   3874    0.642029\n",
      "24                  BUF        1     17    0.629630\n",
      "127                 MKE        1   1034    0.629337\n",
      "175                 RDU        1    992    0.625473\n",
      "179                 RSW        1    570    0.615551\n",
      "37                  CMH        1   1087    0.613085\n",
      "45                  DAL        1    339    0.603203\n",
      "93                  JAX        1    336    0.600000\n",
      "107                 LGB        1   1433    0.599833\n",
      "28                  BWI        1   4762    0.594433\n",
      "119                 MDW        1   7746    0.594018\n",
      "213                 TPA        1   3499    0.584238\n",
      "91                  IND        1   1589    0.579082\n",
      "32                  CHS        1    928    0.573902\n",
      "159                 PHL        1    318    0.569892\n",
      "111                 LIT        1   2515    0.564534\n"
     ]
    }
   ],
   "source": [
    "dest_act_df = pd.DataFrame(delay_df.groupby('Destination Airport')['Delayed'].value_counts())\n",
    "dest_prop_df = pd.DataFrame(delay_df.groupby('Destination Airport')['Delayed'].value_counts(normalize=True))\n",
    "dest_df = pd.concat([dest_act_df, dest_prop_df], axis=1)\n",
    "dest_df.columns = ['Count', 'Proportion']\n",
    "dest_df = dest_df.reset_index()\n",
    "dest_df = dest_df[dest_df['Delayed'] == 1]\n",
    "print(dest_df.sort_values(by='Proportion', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0058eb08-b85c-4f1a-bf1b-728f6e0309dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Tail Number  Delayed  Count  Proportion\n",
      "2474      N391HA        1     43    0.704918\n",
      "4751      N745SW        1    113    0.693252\n",
      "2404      N382HA        1     38    0.678571\n",
      "6753      N8941Q        1     35    0.673077\n",
      "4501      N711HK        1     84    0.666667\n",
      "2450      N388HA        1     41    0.661290\n",
      "2428      N385HA        1     37    0.660714\n",
      "4566      N717SA        1     98    0.657718\n",
      "2436      N386HA        1     44    0.656716\n",
      "6763      N8946L        1     40    0.655738\n",
      "2513      N396HA        1     34    0.653846\n",
      "6431      N8808Q        1    313    0.653445\n",
      "2388      N380HA        1     30    0.652174\n",
      "6761      N8945Q        1     33    0.647059\n",
      "6425      N8805L        1    309    0.646444\n",
      "6423      N8804L        1    307    0.643606\n",
      "6743      N8938Q        1     32    0.640000\n",
      "6177      N8707P        1    220    0.639535\n",
      "6789      N8957Q        1     23    0.638889\n",
      "6781      N8953Q        1     23    0.638889\n"
     ]
    }
   ],
   "source": [
    "tail_no_act_df = pd.DataFrame(delay_df.groupby('Tail Number')['Delayed'].value_counts())\n",
    "tail_no_prop_df = pd.DataFrame(delay_df.groupby('Tail Number')['Delayed'].value_counts(normalize=True))\n",
    "tail_no_df = pd.concat([tail_no_act_df, tail_no_prop_df], axis=1)\n",
    "tail_no_df.columns = ['Count', 'Proportion']\n",
    "tail_no_df = tail_no_df.reset_index()\n",
    "tail_no_df = tail_no_df[tail_no_df['Delayed'] == 1].sort_values(by='Proportion', ascending=False)\n",
    "print(tail_no_df[tail_no_df['Count'] > 20].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee077b5-708e-4b4a-9358-dcfdda8986ce",
   "metadata": {},
   "source": [
    "## Collinearity\n",
    "Given the large chi-squared value between 'Flight Number' and 'Destination Airport', I am inclined to follow my intuition that these two variables are correlated. The 'Flight Number' is thus unlikely to add additional detail to the model that isn't already captured in 'Destination Airport' (and likely other features as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "487fab97-5e27-463a-9ed8-09e7d2749f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chi-squared Statistic: 25069241.64547756\n",
      "P-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "contingency_table = pd.crosstab(delay_df['Flight Number'], delay_df['Destination Airport'])\n",
    "\n",
    "chi2_statistic, p_value, dof, expected_freq = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f'\\nChi-squared Statistic: {chi2_statistic}')\n",
    "print(f'P-value: {p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e2eed-1789-4214-bff2-cccba722dafe",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "## Reducing Dimensionality\n",
    "I will immediately be dropping features from the dataframe that did not pass the \"smell test\" from the data exploration. Additionally, I am keeping the date column so that I can split by time for training and test data to avoid leakage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64bca2d2-f894-49b4-a80b-aabfb5e6d5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 594005 entries, 0 to 594125\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count   Dtype         \n",
      "---  ------               --------------   -----         \n",
      " 0   Carrier Code         594005 non-null  object        \n",
      " 1   DayOfYear            594005 non-null  int32         \n",
      " 2   Hour                 594005 non-null  float64       \n",
      " 3   Destination Airport  594005 non-null  object        \n",
      " 4   Tail Number          593099 non-null  object        \n",
      " 5   Delayed              594005 non-null  int64         \n",
      " 6   Date (MM/DD/YYYY)    594005 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int32(1), int64(1), object(3)\n",
      "memory usage: 34.0+ MB\n"
     ]
    }
   ],
   "source": [
    "delay_df = delay_df[['Carrier Code', 'DayOfYear', 'Hour', 'Destination Airport', 'Tail Number', 'Delayed', 'Date (MM/DD/YYYY)']]\n",
    "delay_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d4c9d-664d-48fc-9315-14fadd4293be",
   "metadata": {},
   "source": [
    "## NaN Checks\n",
    "Many of the machine learning models I will be testing cannot natively handle NaN values so I'll need to make sure to identify NaNs and either drop or replace them appropriately. Since there are only NaN values in the Tail Number feature, it simplifies my approach. Dropping the rows with NaNs would be acceptable since there are so few relative to the total number of data points, but I can do better in this instance. I will fill the values based on the most common value shared by similar rows. For instance, if Tail Number is NaN, I would fill it with the most common Tail Number from rows with the same Carrier Code and Destination Airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a314df7a-1217-4a95-a1f3-8f100473efaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Carrier Code             0\n",
       "DayOfYear                0\n",
       "Hour                     0\n",
       "Destination Airport      0\n",
       "Tail Number            906\n",
       "Delayed                  0\n",
       "Date (MM/DD/YYYY)        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c03bfa42-1fee-401c-bc27-17fd50530a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class GroupedModeImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_cols, target_col):\n",
    "        self.group_cols = group_cols\n",
    "        self.target_col = target_col\n",
    "        self.impute_map_ = None\n",
    "        self.global_mode_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate mode for each group. Use 'Unknown' if there is no mode\n",
    "        self.impute_map_ = X.groupby(self.group_cols)[self.target_col].apply(\n",
    "            lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown'\n",
    "        ).to_dict()\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        \n",
    "        # Create a key for mapping groups and then fill na values\n",
    "        group_keys = X[self.group_cols].apply(tuple, axis=1)        \n",
    "        mask = X[self.target_col].isna()        \n",
    "        X.loc[mask, self.target_col] = group_keys[mask].map(self.impute_map_)\n",
    "\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        # Define how feature names are transformed\n",
    "        out = [f'{col}' for col in self.group_cols]\n",
    "        out.append(f'{self.target_col}')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72946f32-1764-4a23-bf7c-9cfc8f821ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "impute_tail = GroupedModeImputer(group_cols=['Carrier Code', 'Destination Airport'], target_col='Tail Number')\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('fill_tail', impute_tail),\n",
    "    ('encode', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1dd1eb-3aed-4bca-8561-e6f6fce99394",
   "metadata": {},
   "source": [
    "## Cyclic Transformations of DayOfYear and Hour\n",
    "To ensure that neighboring days and hours are treated similarly even when the values appear far apart (e.g. 2300 and 0100), I will transform them with sine and cosine. This will ensure that the cyclic nature of the data is preserved and appropriately incorporated by the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fe397d7-6ba4-423e-b76b-520b4271ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
    "\n",
    "# sine and cosine transformer classes to be used for date and time variables\n",
    "class SinTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, period=1):\n",
    "        self.period = period\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = check_array(X)\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        return np.sin(2*np.pi * X / self.period)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if input_features is None:\n",
    "            # Use feature_names_in_ if available (set during fit)\n",
    "            input_features = getattr(self, \"feature_names_in_\", [f'x{i}' for i in range(self.n_features_in_)])\n",
    "\n",
    "        # Define how feature names are transformed\n",
    "        return [f'{col}_sin' for col in input_features]\n",
    "\n",
    "class CosTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, period=1):\n",
    "        self.period = period\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = check_array(X)\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        return np.cos(2*np.pi * X / self.period)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if input_features is None:\n",
    "            # Use feature_names_in_ if available (set during fit)\n",
    "            input_features = getattr(self, \"feature_names_in_\", [f'x{i}' for i in range(self.n_features_in_)])\n",
    "\n",
    "        # Define how feature names are transformed\n",
    "        return [f'{col}_cos' for col in input_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b51c67-a8bc-4782-87c9-09ad50137a93",
   "metadata": {},
   "source": [
    "## Column Transformer\n",
    "Finally, toss the categorical pipeline and cyclic transformers into a column transformer to make sure it can be easily run on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f905a48d-3fb2-473c-83d5-d473937cb290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "cat_cols = ['Carrier Code', 'Destination Airport', 'Tail Number']\n",
    "date_cols = ['DayOfYear']\n",
    "time_cols = ['Hour']\n",
    "feature_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('categorical', cat_pipeline, cat_cols),\n",
    "        ('day_sin', SinTransformer(365), date_cols),\n",
    "        ('day_cos', CosTransformer(365), date_cols),\n",
    "        ('hour_sin', SinTransformer(24), time_cols),\n",
    "        ('hour_cos', CosTransformer(24), time_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da9dac-66d4-46ff-9065-a8016c5e4cd8",
   "metadata": {},
   "source": [
    "# Testing Machine Learning Models\n",
    "Now is finally the time to test out some machine learning algorithms on the data. I will be testing 4 models:\n",
    "1. Logistic regression\n",
    "2. Nonlinear SVM\n",
    "3. Random forest\n",
    "4. Neural network.\n",
    "\n",
    "These generally proceed from less complex to more complex with the hope being that there will be an optimum where model complexity meets accuracy. And to give each model the best chance at success, I will be optimizing hyperparameters with RandomizedSearchCV when possible.\n",
    "\n",
    "## Splitting Training and Test Data\n",
    "The first step is to, naturally, split the data into a test set and a training set to avoid leakage and overfitting issues. To prevent data leakage, I will be using the first ~3.75 years as the training data and the final year as the test data yielding a roughly 80%-20% split in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37fd3ff4-3528-49e7-815b-87c824f3bc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7671551586266109\n"
     ]
    }
   ],
   "source": [
    "cutoff_date = delay_df['Date (MM/DD/YYYY)'].max() - pd.offsets.DateOffset(years=1)\n",
    "train = delay_df[delay_df['Date (MM/DD/YYYY)'] < cutoff_date]\n",
    "test = delay_df[delay_df['Date (MM/DD/YYYY)'] >= cutoff_date]\n",
    "\n",
    "X_train = train.drop(['Delayed', 'Date (MM/DD/YYYY)'], axis=1)\n",
    "y_train = train['Delayed']\n",
    "X_test = test.drop(['Delayed', 'Date (MM/DD/YYYY)'], axis=1)\n",
    "y_test = test['Delayed']\n",
    "\n",
    "print(len(X_train) / (len(X_train) + len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d88884f-a0f2-4a50-ba31-28bd1cd8a29e",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Logistic regression is the fundamental classifier, in a sense, making it an ideal starting point for testing. I will create a pipeline from the ColumnTransformer above and the logistic regression function so that it can quickly crank through the training data and be easily reused when more data is available in future iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8790f43-1ef4-47ce-888e-789d6ecdcd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "}\n",
       "\n",
       "#sk-container-id-1.light {\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: black;\n",
       "  --sklearn-color-background: white;\n",
       "  --sklearn-color-border-box: black;\n",
       "  --sklearn-color-icon: #696969;\n",
       "}\n",
       "\n",
       "#sk-container-id-1.dark {\n",
       "  --sklearn-color-text-on-default-background: white;\n",
       "  --sklearn-color-background: #111;\n",
       "  --sklearn-color-border-box: white;\n",
       "  --sklearn-color-icon: #878787;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: center;\n",
       "  justify-content: center;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table {\n",
       "    font-family: monospace;\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table summary::marker {\n",
       "    font-size: 0.7rem;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       "/*\n",
       "    `table td`is set in notebook with right text-align.\n",
       "    We need to overwrite it.\n",
       "*/\n",
       ".estimator-table table td.param {\n",
       "    text-align: left;\n",
       "    position: relative;\n",
       "    padding: 0;\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td.value {\n",
       "    color:rgb(255, 94, 0);\n",
       "    background-color: transparent;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "/*\n",
       "    Styles for parameter documentation links\n",
       "    We need styling for visited so jupyter doesn't overwrite it\n",
       "*/\n",
       "a.param-doc-link,\n",
       "a.param-doc-link:link,\n",
       "a.param-doc-link:visited {\n",
       "    text-decoration: underline dashed;\n",
       "    text-underline-offset: .3em;\n",
       "    color: inherit;\n",
       "    display: block;\n",
       "    padding: .5em;\n",
       "}\n",
       "\n",
       "/* \"hack\" to make the entire area of the cell containing the link clickable */\n",
       "a.param-doc-link::before {\n",
       "    position: absolute;\n",
       "    content: \"\";\n",
       "    inset: 0;\n",
       "}\n",
       "\n",
       ".param-doc-description {\n",
       "    display: none;\n",
       "    position: absolute;\n",
       "    z-index: 9999;\n",
       "    left: 0;\n",
       "    padding: .5ex;\n",
       "    margin-left: 1.5em;\n",
       "    color: var(--sklearn-color-text);\n",
       "    box-shadow: .3em .3em .4em #999;\n",
       "    width: max-content;\n",
       "    text-align: left;\n",
       "    max-height: 10em;\n",
       "    overflow-y: auto;\n",
       "\n",
       "    /* unfitted */\n",
       "    background: var(--sklearn-color-unfitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       "/* Fitted state for parameter tooltips */\n",
       ".fitted .param-doc-description {\n",
       "    /* fitted */\n",
       "    background: var(--sklearn-color-fitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".param-doc-link:hover .param-doc-description {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[(&#x27;data_transformer&#x27;,\n",
       "                                              ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                                                               Pipeline(steps=[(&#x27;fill_tail&#x27;,\n",
       "                                                                                                GroupedModeImputer(group_cols=[&#x27;Carrier &#x27;\n",
       "                                                                                                                               &#x27;Code&#x27;,\n",
       "                                                                                                                               &#x27;Destination &#x27;\n",
       "                                                                                                                               &#x27;Airport&#x27;],\n",
       "                                                                                                                   target_col=&#x27;Tail &#x27;\n",
       "                                                                                                                              &#x27;Number&#x27;)),\n",
       "                                                                                               (&#x27;encode&#x27;,\n",
       "                                                                                                OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                                               [&#x27;Carrier &#x27;\n",
       "                                                                                &#x27;Code&#x27;,\n",
       "                                                                                &#x27;Destination &#x27;\n",
       "                                                                                &#x27;Airport&#x27;,\n",
       "                                                                                &#x27;Tail &#x27;\n",
       "                                                                                &#x27;Number...\n",
       "                                             (&#x27;logistic_clf&#x27;,\n",
       "                                              SGDClassifier(loss=&#x27;log_loss&#x27;,\n",
       "                                                            n_jobs=-1,\n",
       "                                                            penalty=&#x27;elasticnet&#x27;))]),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={&#x27;logistic_clf__alpha&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x000001B1E9E12900&gt;,\n",
       "                                        &#x27;logistic_clf__l1_ratio&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x000001B1E9E6D090&gt;},\n",
       "                   scoring=&#x27;balanced_accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomizedSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">?<span>Documentation for RandomizedSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('estimator',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=estimator,-estimator%20object\">\n",
       "            estimator\n",
       "            <span class=\"param-doc-description\">estimator: estimator object<br><br>An object of that type is instantiated for each grid point.<br>This is assumed to implement the scikit-learn estimator interface.<br>Either estimator needs to provide a ``score`` function,<br>or ``scoring`` must be passed.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">Pipeline(step...lasticnet&#x27;))])</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('param_distributions',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=param_distributions,-dict%20or%20list%20of%20dicts\">\n",
       "            param_distributions\n",
       "            <span class=\"param-doc-description\">param_distributions: dict or list of dicts<br><br>Dictionary with parameters names (`str`) as keys and distributions<br>or lists of parameters to try. Distributions must provide a ``rvs``<br>method for sampling (such as those from scipy.stats.distributions).<br>If a list is given, it is sampled uniformly.<br>If a list of dicts is given, first a dict is sampled uniformly, and<br>then a parameter is sampled using that dict as above.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">{&#x27;logistic_clf__alpha&#x27;: &lt;scipy.stats....001B1E9E12900&gt;, &#x27;logistic_clf__l1_ratio&#x27;: &lt;scipy.stats....001B1E9E6D090&gt;}</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=n_iter,-int%2C%20default%3D10\">\n",
       "            n_iter\n",
       "            <span class=\"param-doc-description\">n_iter: int, default=10<br><br>Number of parameter settings that are sampled. n_iter trades<br>off runtime vs quality of the solution.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">100</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('scoring',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=scoring,-str%2C%20callable%2C%20list%2C%20tuple%20or%20dict%2C%20default%3DNone\">\n",
       "            scoring\n",
       "            <span class=\"param-doc-description\">scoring: str, callable, list, tuple or dict, default=None<br><br>Strategy to evaluate the performance of the cross-validated model on<br>the test set.<br><br>If `scoring` represents a single score, one can use:<br><br>- a single string (see :ref:`scoring_string_names`);<br>- a callable (see :ref:`scoring_callable`) that returns a single value;<br>- `None`, the `estimator`'s<br>  :ref:`default evaluation criterion <scoring_api_overview>` is used.<br><br>If `scoring` represents multiple scores, one can use:<br><br>- a list or tuple of unique strings;<br>- a callable returning a dictionary where the keys are the metric<br>  names and the values are the metric scores;<br>- a dictionary with metric names as keys and callables as values.<br><br>See :ref:`multimetric_grid_search` for an example.<br><br>If None, the estimator's score method is used.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;balanced_accuracy&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=n_jobs,-int%2C%20default%3DNone\">\n",
       "            n_jobs\n",
       "            <span class=\"param-doc-description\">n_jobs: int, default=None<br><br>Number of jobs to run in parallel.<br>``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.<br>``-1`` means using all processors. See :term:`Glossary <n_jobs>`<br>for more details.<br><br>.. versionchanged:: v0.20<br>   `n_jobs` default changed from 1 to None</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">-1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('refit',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=refit,-bool%2C%20str%2C%20or%20callable%2C%20default%3DTrue\">\n",
       "            refit\n",
       "            <span class=\"param-doc-description\">refit: bool, str, or callable, default=True<br><br>Refit an estimator using the best found parameters on the whole<br>dataset.<br><br>For multiple metric evaluation, this needs to be a `str` denoting the<br>scorer that would be used to find the best parameters for refitting<br>the estimator at the end.<br><br>Where there are considerations other than maximum score in<br>choosing a best estimator, ``refit`` can be set to a function which<br>returns the selected ``best_index_`` given the ``cv_results_``. In that<br>case, the ``best_estimator_`` and ``best_params_`` will be set<br>according to the returned ``best_index_`` while the ``best_score_``<br>attribute will not be available.<br><br>The refitted estimator is made available at the ``best_estimator_``<br>attribute and permits using ``predict`` directly on this<br>``RandomizedSearchCV`` instance.<br><br>Also for multiple metric evaluation, the attributes ``best_index_``,<br>``best_score_`` and ``best_params_`` will only be available if<br>``refit`` is set and all of them will be determined w.r.t this specific<br>scorer.<br><br>See ``scoring`` parameter to know more about multiple metric<br>evaluation.<br><br>See :ref:`this example<br><sphx_glr_auto_examples_model_selection_plot_grid_search_refit_callable.py>`<br>for an example of how to use ``refit=callable`` to balance model<br>complexity and cross-validated score.<br><br>.. versionchanged:: 0.20<br>    Support for callable added.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('cv',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=cv,-int%2C%20cross-validation%20generator%20or%20an%20iterable%2C%20default%3DNone\">\n",
       "            cv\n",
       "            <span class=\"param-doc-description\">cv: int, cross-validation generator or an iterable, default=None<br><br>Determines the cross-validation splitting strategy.<br>Possible inputs for cv are:<br><br>- None, to use the default 5-fold cross validation,<br>- integer, to specify the number of folds in a `(Stratified)KFold`,<br>- :term:`CV splitter`,<br>- An iterable yielding (train, test) splits as arrays of indices.<br><br>For integer/None inputs, if the estimator is a classifier and ``y`` is<br>either binary or multiclass, :class:`StratifiedKFold` is used. In all<br>other cases, :class:`KFold` is used. These splitters are instantiated<br>with `shuffle=False` so the splits will be the same across calls.<br><br>Refer :ref:`User Guide <cross_validation>` for the various<br>cross-validation strategies that can be used here.<br><br>.. versionchanged:: 0.22<br>    ``cv`` default value if None changed from 3-fold to 5-fold.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=verbose,-int\">\n",
       "            verbose\n",
       "            <span class=\"param-doc-description\">verbose: int<br><br>Controls the verbosity: the higher, the more messages.<br><br>- >1 : the computation time for each fold and parameter candidate is<br>  displayed;<br>- >2 : the score is also displayed;<br>- >3 : the fold and candidate parameter indexes are also displayed<br>  together with the starting time of the computation.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('pre_dispatch',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=pre_dispatch,-int%2C%20or%20str%2C%20default%3D%272%2An_jobs%27\">\n",
       "            pre_dispatch\n",
       "            <span class=\"param-doc-description\">pre_dispatch: int, or str, default='2*n_jobs'<br><br>Controls the number of jobs that get dispatched during parallel<br>execution. Reducing this number can be useful to avoid an<br>explosion of memory consumption when more jobs get dispatched<br>than CPUs can process. This parameter can be:<br><br>- None, in which case all the jobs are immediately created and spawned. Use<br>  this for lightweight and fast-running jobs, to avoid delays due to on-demand<br>  spawning of the jobs<br>- An int, giving the exact number of total jobs that are spawned<br>- A str, giving an expression as a function of n_jobs, as in '2*n_jobs'</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;2*n_jobs&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=random_state,-int%2C%20RandomState%20instance%20or%20None%2C%20default%3DNone\">\n",
       "            random_state\n",
       "            <span class=\"param-doc-description\">random_state: int, RandomState instance or None, default=None<br><br>Pseudo random number generator state used for random uniform sampling<br>from lists of possible values instead of scipy.stats distributions.<br>Pass an int for reproducible output across multiple<br>function calls.<br>See :term:`Glossary <random_state>`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('error_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=error_score,-%27raise%27%20or%20numeric%2C%20default%3Dnp.nan\">\n",
       "            error_score\n",
       "            <span class=\"param-doc-description\">error_score: 'raise' or numeric, default=np.nan<br><br>Value to assign to the score if an error occurs in estimator fitting.<br>If set to 'raise', the error is raised. If a numeric value is given,<br>FitFailedWarning is raised. This parameter does not affect the refit<br>step, which will always raise the error.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">nan</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('return_train_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=return_train_score,-bool%2C%20default%3DFalse\">\n",
       "            return_train_score\n",
       "            <span class=\"param-doc-description\">return_train_score: bool, default=False<br><br>If ``False``, the ``cv_results_`` attribute will not include training<br>scores.<br>Computing training scores is used to get insights on how different<br>parameter settings impact the overfitting/underfitting trade-off.<br>However computing the scores on the training set can be computationally<br>expensive and is not strictly required to select the parameters that<br>yield the best generalization performance.<br><br>.. versionadded:: 0.19<br><br>.. versionchanged:: 0.21<br>    Default value was changed from ``True`` to ``False``</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: Pipeline</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___\"></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>data_transformer: ColumnTransformer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for data_transformer: ColumnTransformer</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transformers',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=transformers,-list%20of%20tuples\">\n",
       "            transformers\n",
       "            <span class=\"param-doc-description\">transformers: list of tuples<br><br>List of (name, transformer, columns) tuples specifying the<br>transformer objects to be applied to subsets of the data.<br><br>name : str<br>    Like in Pipeline and FeatureUnion, this allows the transformer and<br>    its parameters to be set using ``set_params`` and searched in grid<br>    search.<br>transformer : {'drop', 'passthrough'} or estimator<br>    Estimator must support :term:`fit` and :term:`transform`.<br>    Special-cased strings 'drop' and 'passthrough' are accepted as<br>    well, to indicate to drop the columns or to pass them through<br>    untransformed, respectively.<br>columns :  str, array-like of str, int, array-like of int,                 array-like of bool, slice or callable<br>    Indexes the data on its second axis. Integers are interpreted as<br>    positional columns, while strings can reference DataFrame columns<br>    by name.  A scalar string or int should be used where<br>    ``transformer`` expects X to be a 1d array-like (vector),<br>    otherwise a 2d array will be passed to the transformer.<br>    A callable is passed the input data `X` and can return any of the<br>    above. To select multiple columns by name or dtype, you can use<br>    :obj:`make_column_selector`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">[(&#x27;categorical&#x27;, ...), (&#x27;day_sin&#x27;, ...), ...]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('remainder',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=remainder,-%7B%27drop%27%2C%20%27passthrough%27%7D%20or%20estimator%2C%20default%3D%27drop%27\">\n",
       "            remainder\n",
       "            <span class=\"param-doc-description\">remainder: {'drop', 'passthrough'} or estimator, default='drop'<br><br>By default, only the specified columns in `transformers` are<br>transformed and combined in the output, and the non-specified<br>columns are dropped. (default of ``'drop'``).<br>By specifying ``remainder='passthrough'``, all remaining columns that<br>were not specified in `transformers`, but present in the data passed<br>to `fit` will be automatically passed through. This subset of columns<br>is concatenated with the output of the transformers. For dataframes,<br>extra columns not seen during `fit` will be excluded from the output<br>of `transform`.<br>By setting ``remainder`` to be an estimator, the remaining<br>non-specified columns will use the ``remainder`` estimator. The<br>estimator must support :term:`fit` and :term:`transform`.<br>Note that using this feature requires that the DataFrame columns<br>input at :term:`fit` and :term:`transform` have identical order.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;drop&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sparse_threshold',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=sparse_threshold,-float%2C%20default%3D0.3\">\n",
       "            sparse_threshold\n",
       "            <span class=\"param-doc-description\">sparse_threshold: float, default=0.3<br><br>If the output of the different transformers contains sparse matrices,<br>these will be stacked as a sparse matrix if the overall density is<br>lower than this value. Use ``sparse_threshold=0`` to always return<br>dense.  When the transformed output consists of all dense data, the<br>stacked result will be dense, and this keyword will be ignored.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=n_jobs,-int%2C%20default%3DNone\">\n",
       "            n_jobs\n",
       "            <span class=\"param-doc-description\">n_jobs: int, default=None<br><br>Number of jobs to run in parallel.<br>``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.<br>``-1`` means using all processors. See :term:`Glossary <n_jobs>`<br>for more details.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transformer_weights',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=transformer_weights,-dict%2C%20default%3DNone\">\n",
       "            transformer_weights\n",
       "            <span class=\"param-doc-description\">transformer_weights: dict, default=None<br><br>Multiplicative weights for features per transformer. The output of the<br>transformer is multiplied by these weights. Keys are transformer names,<br>values the weights.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=verbose,-bool%2C%20default%3DFalse\">\n",
       "            verbose\n",
       "            <span class=\"param-doc-description\">verbose: bool, default=False<br><br>If True, the time elapsed while fitting each transformer will be<br>printed as it is completed.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose_feature_names_out',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=verbose_feature_names_out,-bool%2C%20str%20or%20Callable%5B%5Bstr%2C%20str%5D%2C%20str%5D%2C%20default%3DTrue\">\n",
       "            verbose_feature_names_out\n",
       "            <span class=\"param-doc-description\">verbose_feature_names_out: bool, str or Callable[[str, str], str], default=True<br><br>- If True, :meth:`ColumnTransformer.get_feature_names_out` will prefix<br>  all feature names with the name of the transformer that generated that<br>  feature. It is equivalent to setting<br>  `verbose_feature_names_out=\"{transformer_name}__{feature_name}\"`.<br>- If False, :meth:`ColumnTransformer.get_feature_names_out` will not<br>  prefix any feature names and will error if feature names are not<br>  unique.<br>- If ``Callable[[str, str], str]``,<br>  :meth:`ColumnTransformer.get_feature_names_out` will rename all the features<br>  using the name of the transformer. The first argument of the callable is the<br>  transformer name and the second argument is the feature name. The returned<br>  string will be the new feature name.<br>- If ``str``, it must be a string ready for formatting. The given string will<br>  be formatted using two field names: ``transformer_name`` and ``feature_name``.<br>  e.g. ``\"{feature_name}__{transformer_name}\"``. See :meth:`str.format` method<br>  from the standard library for more info.<br><br>.. versionadded:: 1.0<br><br>.. versionchanged:: 1.6<br>    `verbose_feature_names_out` can be a callable or a string to be formatted.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('force_int_remainder_cols',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=force_int_remainder_cols,-bool%2C%20default%3DFalse\">\n",
       "            force_int_remainder_cols\n",
       "            <span class=\"param-doc-description\">force_int_remainder_cols: bool, default=False<br><br>This parameter has no effect.<br><br>.. note::<br>    If you do not access the list of columns for the remainder columns<br>    in the `transformers_` fitted attribute, you do not need to set<br>    this parameter.<br><br>.. versionadded:: 1.5<br><br>.. versionchanged:: 1.7<br>   The default value for `force_int_remainder_cols` will change from<br>   `True` to `False` in version 1.7.<br><br>.. deprecated:: 1.7<br>   `force_int_remainder_cols` is deprecated and will be removed in 1.9.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>categorical</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__categorical__\"><pre>[&#x27;Carrier Code&#x27;, &#x27;Destination Airport&#x27;, &#x27;Tail Number&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GroupedModeImputer</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__categorical__fill_tail__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('group_cols',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">group_cols</td>\n",
       "            <td class=\"value\">[&#x27;Carrier Code&#x27;, &#x27;Destination Airport&#x27;]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('target_col',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">target_col</td>\n",
       "            <td class=\"value\">&#x27;Tail Number&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>OneHotEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__categorical__encode__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('categories',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=categories,-%27auto%27%20or%20a%20list%20of%20array-like%2C%20default%3D%27auto%27\">\n",
       "            categories\n",
       "            <span class=\"param-doc-description\">categories: 'auto' or a list of array-like, default='auto'<br><br>Categories (unique values) per feature:<br><br>- 'auto' : Determine categories automatically from the training data.<br>- list : ``categories[i]`` holds the categories expected in the ith<br>  column. The passed categories should not mix strings and numeric<br>  values within a single feature, and should be sorted in case of<br>  numeric values.<br><br>The used categories can be found in the ``categories_`` attribute.<br><br>.. versionadded:: 0.20</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;auto&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('drop',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=drop,-%7B%27first%27%2C%20%27if_binary%27%7D%20or%20an%20array-like%20of%20shape%20%28n_features%2C%29%2C%20%20%20%20%20%20%20%20%20%20%20%20%20default%3DNone\">\n",
       "            drop\n",
       "            <span class=\"param-doc-description\">drop: {'first', 'if_binary'} or an array-like of shape (n_features,),             default=None<br><br>Specifies a methodology to use to drop one of the categories per<br>feature. This is useful in situations where perfectly collinear<br>features cause problems, such as when feeding the resulting data<br>into an unregularized linear regression model.<br><br>However, dropping one category breaks the symmetry of the original<br>representation and can therefore induce a bias in downstream models,<br>for instance for penalized linear classification or regression models.<br><br>- None : retain all features (the default).<br>- 'first' : drop the first category in each feature. If only one<br>  category is present, the feature will be dropped entirely.<br>- 'if_binary' : drop the first category in each feature with two<br>  categories. Features with 1 or more than 2 categories are<br>  left intact.<br>- array : ``drop[i]`` is the category in feature ``X[:, i]`` that<br>  should be dropped.<br><br>When `max_categories` or `min_frequency` is configured to group<br>infrequent categories, the dropping behavior is handled after the<br>grouping.<br><br>.. versionadded:: 0.21<br>   The parameter `drop` was added in 0.21.<br><br>.. versionchanged:: 0.23<br>   The option `drop='if_binary'` was added in 0.23.<br><br>.. versionchanged:: 1.1<br>    Support for dropping infrequent categories.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sparse_output',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=sparse_output,-bool%2C%20default%3DTrue\">\n",
       "            sparse_output\n",
       "            <span class=\"param-doc-description\">sparse_output: bool, default=True<br><br>When ``True``, it returns a :class:`scipy.sparse.csr_matrix`,<br>i.e. a sparse matrix in \"Compressed Sparse Row\" (CSR) format.<br><br>.. versionadded:: 1.2<br>   `sparse` was renamed to `sparse_output`</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dtype',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=dtype,-number%20type%2C%20default%3Dnp.float64\">\n",
       "            dtype\n",
       "            <span class=\"param-doc-description\">dtype: number type, default=np.float64<br><br>Desired dtype of output.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&lt;class &#x27;numpy.float64&#x27;&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('handle_unknown',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=handle_unknown,-%7B%27error%27%2C%20%27ignore%27%2C%20%27infrequent_if_exist%27%2C%20%27warn%27%7D%2C%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20default%3D%27error%27\">\n",
       "            handle_unknown\n",
       "            <span class=\"param-doc-description\">handle_unknown: {'error', 'ignore', 'infrequent_if_exist', 'warn'},                      default='error'<br><br>Specifies the way unknown categories are handled during :meth:`transform`.<br><br>- 'error' : Raise an error if an unknown category is present during transform.<br>- 'ignore' : When an unknown category is encountered during<br>  transform, the resulting one-hot encoded columns for this feature<br>  will be all zeros. In the inverse transform, an unknown category<br>  will be denoted as None.<br>- 'infrequent_if_exist' : When an unknown category is encountered<br>  during transform, the resulting one-hot encoded columns for this<br>  feature will map to the infrequent category if it exists. The<br>  infrequent category will be mapped to the last position in the<br>  encoding. During inverse transform, an unknown category will be<br>  mapped to the category denoted `'infrequent'` if it exists. If the<br>  `'infrequent'` category does not exist, then :meth:`transform` and<br>  :meth:`inverse_transform` will handle an unknown category as with<br>  `handle_unknown='ignore'`. Infrequent categories exist based on<br>  `min_frequency` and `max_categories`. Read more in the<br>  :ref:`User Guide <encoder_infrequent_categories>`.<br>- 'warn' : When an unknown category is encountered during transform<br>  a warning is issued, and the encoding then proceeds as described for<br>  `handle_unknown=\"infrequent_if_exist\"`.<br><br>.. versionchanged:: 1.1<br>    `'infrequent_if_exist'` was added to automatically handle unknown<br>    categories and infrequent categories.<br><br>.. versionadded:: 1.6<br>   The option `\"warn\"` was added in 1.6.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;ignore&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_frequency',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=min_frequency,-int%20or%20float%2C%20default%3DNone\">\n",
       "            min_frequency\n",
       "            <span class=\"param-doc-description\">min_frequency: int or float, default=None<br><br>Specifies the minimum frequency below which a category will be<br>considered infrequent.<br><br>- If `int`, categories with a smaller cardinality will be considered<br>  infrequent.<br><br>- If `float`, categories with a smaller cardinality than<br>  `min_frequency * n_samples`  will be considered infrequent.<br><br>.. versionadded:: 1.1<br>    Read more in the :ref:`User Guide <encoder_infrequent_categories>`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_categories',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=max_categories,-int%2C%20default%3DNone\">\n",
       "            max_categories\n",
       "            <span class=\"param-doc-description\">max_categories: int, default=None<br><br>Specifies an upper limit to the number of output features for each input<br>feature when considering infrequent categories. If there are infrequent<br>categories, `max_categories` includes the category representing the<br>infrequent categories along with the frequent categories. If `None`,<br>there is no limit to the number of output features.<br><br>.. versionadded:: 1.1<br>    Read more in the :ref:`User Guide <encoder_infrequent_categories>`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('feature_name_combiner',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=feature_name_combiner,-%22concat%22%20or%20callable%2C%20default%3D%22concat%22\">\n",
       "            feature_name_combiner\n",
       "            <span class=\"param-doc-description\">feature_name_combiner: \"concat\" or callable, default=\"concat\"<br><br>Callable with signature `def callable(input_feature, category)` that returns a<br>string. This is used to create feature names to be returned by<br>:meth:`get_feature_names_out`.<br><br>`\"concat\"` concatenates encoded feature name and category with<br>`feature + \"_\" + str(category)`.E.g. feature X with values 1, 6, 7 create<br>feature names `X_1, X_6, X_7`.<br><br>.. versionadded:: 1.3</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;concat&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>day_sin</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__day_sin__\"><pre>[&#x27;DayOfYear&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SinTransformer</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__day_sin__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('period',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">period</td>\n",
       "            <td class=\"value\">365</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>day_cos</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__day_cos__\"><pre>[&#x27;DayOfYear&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>CosTransformer</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__day_cos__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('period',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">period</td>\n",
       "            <td class=\"value\">365</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>hour_sin</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__hour_sin__\"><pre>[&#x27;Hour&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SinTransformer</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__hour_sin__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('period',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">period</td>\n",
       "            <td class=\"value\">24</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>hour_cos</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__hour_cos__\"><pre>[&#x27;Hour&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>CosTransformer</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__hour_cos__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('period',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">period</td>\n",
       "            <td class=\"value\">24</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SGDClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html\">?<span>Documentation for SGDClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___logistic_clf__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('loss',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=loss,-%7B%27hinge%27%2C%20%27log_loss%27%2C%20%27modified_huber%27%2C%20%27squared_hinge%27%2C%20%20%20%20%20%20%20%20%27perceptron%27%2C%20%27squared_error%27%2C%20%27huber%27%2C%20%27epsilon_insensitive%27%2C%20%20%20%20%20%20%20%20%27squared_epsilon_insensitive%27%7D%2C%20default%3D%27hinge%27\">\n",
       "            loss\n",
       "            <span class=\"param-doc-description\">loss: {'hinge', 'log_loss', 'modified_huber', 'squared_hinge',        'perceptron', 'squared_error', 'huber', 'epsilon_insensitive',        'squared_epsilon_insensitive'}, default='hinge'<br><br>The loss function to be used.<br><br>- 'hinge' gives a linear SVM.<br>- 'log_loss' gives logistic regression, a probabilistic classifier.<br>- 'modified_huber' is another smooth loss that brings tolerance to<br>  outliers as well as probability estimates.<br>- 'squared_hinge' is like hinge but is quadratically penalized.<br>- 'perceptron' is the linear loss used by the perceptron algorithm.<br>- The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and<br>  'squared_epsilon_insensitive' are designed for regression but can be useful<br>  in classification as well; see<br>  :class:`~sklearn.linear_model.SGDRegressor` for a description.<br><br>More details about the losses formulas can be found in the :ref:`User Guide<br><sgd_mathematical_formulation>` and you can find a visualisation of the loss<br>functions in<br>:ref:`sphx_glr_auto_examples_linear_model_plot_sgd_loss_functions.py`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;log_loss&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('penalty',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=penalty,-%7B%27l2%27%2C%20%27l1%27%2C%20%27elasticnet%27%2C%20None%7D%2C%20default%3D%27l2%27\">\n",
       "            penalty\n",
       "            <span class=\"param-doc-description\">penalty: {'l2', 'l1', 'elasticnet', None}, default='l2'<br><br>The penalty (aka regularization term) to be used. Defaults to 'l2'<br>which is the standard regularizer for linear SVM models. 'l1' and<br>'elasticnet' might bring sparsity to the model (feature selection)<br>not achievable with 'l2'. No penalty is added when set to `None`.<br><br>You can see a visualisation of the penalties in<br>:ref:`sphx_glr_auto_examples_linear_model_plot_sgd_penalties.py`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;elasticnet&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('alpha',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=alpha,-float%2C%20default%3D0.0001\">\n",
       "            alpha\n",
       "            <span class=\"param-doc-description\">alpha: float, default=0.0001<br><br>Constant that multiplies the regularization term. The higher the<br>value, the stronger the regularization. Also used to compute the<br>learning rate when `learning_rate` is set to 'optimal'.<br>Values must be in the range `[0.0, inf)`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">np.float64(0.0828516864252097)</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('l1_ratio',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=l1_ratio,-float%2C%20default%3D0.15\">\n",
       "            l1_ratio\n",
       "            <span class=\"param-doc-description\">l1_ratio: float, default=0.15<br><br>The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.<br>l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.<br>Only used if `penalty` is 'elasticnet'.<br>Values must be in the range `[0.0, 1.0]` or can be `None` if<br>`penalty` is not `elasticnet`.<br><br>.. versionchanged:: 1.7<br>    `l1_ratio` can be `None` when `penalty` is not \"elasticnet\".</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">np.float64(0.5187363491102487)</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fit_intercept',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=fit_intercept,-bool%2C%20default%3DTrue\">\n",
       "            fit_intercept\n",
       "            <span class=\"param-doc-description\">fit_intercept: bool, default=True<br><br>Whether the intercept should be estimated or not. If False, the<br>data is assumed to be already centered.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=max_iter,-int%2C%20default%3D1000\">\n",
       "            max_iter\n",
       "            <span class=\"param-doc-description\">max_iter: int, default=1000<br><br>The maximum number of passes over the training data (aka epochs).<br>It only impacts the behavior in the ``fit`` method, and not the<br>:meth:`partial_fit` method.<br>Values must be in the range `[1, inf)`.<br><br>.. versionadded:: 0.19</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">1000</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=tol,-float%20or%20None%2C%20default%3D1e-3\">\n",
       "            tol\n",
       "            <span class=\"param-doc-description\">tol: float or None, default=1e-3<br><br>The stopping criterion. If it is not None, training will stop<br>when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive<br>epochs.<br>Convergence is checked against the training loss or the<br>validation loss depending on the `early_stopping` parameter.<br>Values must be in the range `[0.0, inf)`.<br><br>.. versionadded:: 0.19</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('shuffle',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=shuffle,-bool%2C%20default%3DTrue\">\n",
       "            shuffle\n",
       "            <span class=\"param-doc-description\">shuffle: bool, default=True<br><br>Whether or not the training data should be shuffled after each epoch.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=verbose,-int%2C%20default%3D0\">\n",
       "            verbose\n",
       "            <span class=\"param-doc-description\">verbose: int, default=0<br><br>The verbosity level.<br>Values must be in the range `[0, inf)`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('epsilon',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=epsilon,-float%2C%20default%3D0.1\">\n",
       "            epsilon\n",
       "            <span class=\"param-doc-description\">epsilon: float, default=0.1<br><br>Epsilon in the epsilon-insensitive loss functions; only if `loss` is<br>'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.<br>For 'huber', determines the threshold at which it becomes less<br>important to get the prediction exactly right.<br>For epsilon-insensitive, any differences between the current prediction<br>and the correct label are ignored if they are less than this threshold.<br>Values must be in the range `[0.0, inf)`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=n_jobs,-int%2C%20default%3DNone\">\n",
       "            n_jobs\n",
       "            <span class=\"param-doc-description\">n_jobs: int, default=None<br><br>The number of CPUs to use to do the OVA (One Versus All, for<br>multi-class problems) computation.<br>``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.<br>``-1`` means using all processors. See :term:`Glossary <n_jobs>`<br>for more details.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">-1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=random_state,-int%2C%20RandomState%20instance%2C%20default%3DNone\">\n",
       "            random_state\n",
       "            <span class=\"param-doc-description\">random_state: int, RandomState instance, default=None<br><br>Used for shuffling the data, when ``shuffle`` is set to ``True``.<br>Pass an int for reproducible output across multiple function calls.<br>See :term:`Glossary <random_state>`.<br>Integer values must be in the range `[0, 2**32 - 1]`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('learning_rate',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=learning_rate,-str%2C%20default%3D%27optimal%27\">\n",
       "            learning_rate\n",
       "            <span class=\"param-doc-description\">learning_rate: str, default='optimal'<br><br>The learning rate schedule:<br><br>- 'constant': `eta = eta0`<br>- 'optimal': `eta = 1.0 / (alpha * (t + t0))`<br>  where `t0` is chosen by a heuristic proposed by Leon Bottou.<br>- 'invscaling': `eta = eta0 / pow(t, power_t)`<br>- 'adaptive': `eta = eta0`, as long as the training keeps decreasing.<br>  Each time n_iter_no_change consecutive epochs fail to decrease the<br>  training loss by tol or fail to increase validation score by tol if<br>  `early_stopping` is `True`, the current learning rate is divided by 5.<br>- 'pa1': passive-aggressive algorithm 1, see [1]_. Only with `loss='hinge'`.<br>  Update is `w += eta y x` with `eta = min(eta0, loss/||x||**2)`.<br>- 'pa2': passive-aggressive algorithm 2, see [1]_. Only with<br>  `loss='hinge'`.<br>  Update is `w += eta y x` with `eta = hinge_loss / (||x||**2 + 1/(2 eta0))`.<br><br>.. versionadded:: 0.20<br>    Added 'adaptive' option.<br><br>.. versionadded:: 1.8<br>   Added options 'pa1' and 'pa2'</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;optimal&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('eta0',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=eta0,-float%2C%20default%3D0.01\">\n",
       "            eta0\n",
       "            <span class=\"param-doc-description\">eta0: float, default=0.01<br><br>The initial learning rate for the 'constant', 'invscaling' or<br>'adaptive' schedules. The default value is 0.01, but note that eta0 is not used<br>by the default learning rate 'optimal'.<br>Values must be in the range `(0.0, inf)`.<br><br>For PA-1 (`learning_rate=pa1`) and PA-II (`pa2`), it specifies the<br>aggressiveness parameter for the passive-agressive algorithm, see [1] where it<br>is called C:<br><br>- For PA-I it is the maximum step size.<br>- For PA-II it regularizes the step size (the smaller `eta0` the more it<br>  regularizes).<br><br>As a general rule-of-thumb for PA, `eta0` should be small when the data is<br>noisy.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.01</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('power_t',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=power_t,-float%2C%20default%3D0.5\">\n",
       "            power_t\n",
       "            <span class=\"param-doc-description\">power_t: float, default=0.5<br><br>The exponent for inverse scaling learning rate.<br>Values must be in the range `[0.0, inf)`.<br><br>.. deprecated:: 1.8<br>    Negative values for `power_t` are deprecated in version 1.8 and will raise<br>    an error in 1.10. Use values in the range [0.0, inf) instead.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.5</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('early_stopping',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=early_stopping,-bool%2C%20default%3DFalse\">\n",
       "            early_stopping\n",
       "            <span class=\"param-doc-description\">early_stopping: bool, default=False<br><br>Whether to use early stopping to terminate training when validation<br>score is not improving. If set to `True`, it will automatically set aside<br>a stratified fraction of training data as validation and terminate<br>training when validation score returned by the `score` method is not<br>improving by at least tol for n_iter_no_change consecutive epochs.<br><br>See :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_early_stopping.py` for an<br>example of the effects of early stopping.<br><br>.. versionadded:: 0.20<br>    Added 'early_stopping' option</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('validation_fraction',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=validation_fraction,-float%2C%20default%3D0.1\">\n",
       "            validation_fraction\n",
       "            <span class=\"param-doc-description\">validation_fraction: float, default=0.1<br><br>The proportion of training data to set aside as validation set for<br>early stopping. Must be between 0 and 1.<br>Only used if `early_stopping` is True.<br>Values must be in the range `(0.0, 1.0)`.<br><br>.. versionadded:: 0.20<br>    Added 'validation_fraction' option</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_iter_no_change',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=n_iter_no_change,-int%2C%20default%3D5\">\n",
       "            n_iter_no_change\n",
       "            <span class=\"param-doc-description\">n_iter_no_change: int, default=5<br><br>Number of iterations with no improvement to wait before stopping<br>fitting.<br>Convergence is checked against the training loss or the<br>validation loss depending on the `early_stopping` parameter.<br>Integer values must be in the range `[1, max_iter)`.<br><br>.. versionadded:: 0.20<br>    Added 'n_iter_no_change' option</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">5</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=class_weight,-dict%2C%20%7Bclass_label%3A%20weight%7D%20or%20%22balanced%22%2C%20default%3DNone\">\n",
       "            class_weight\n",
       "            <span class=\"param-doc-description\">class_weight: dict, {class_label: weight} or \"balanced\", default=None<br><br>Preset for the class_weight fit parameter.<br><br>Weights associated with classes. If not given, all classes<br>are supposed to have weight one.<br><br>The \"balanced\" mode uses the values of y to automatically adjust<br>weights inversely proportional to class frequencies in the input data<br>as ``n_samples / (n_classes * np.bincount(y))``.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=warm_start,-bool%2C%20default%3DFalse\">\n",
       "            warm_start\n",
       "            <span class=\"param-doc-description\">warm_start: bool, default=False<br><br>When set to True, reuse the solution of the previous call to fit as<br>initialization, otherwise, just erase the previous solution.<br>See :term:`the Glossary <warm_start>`.<br><br>Repeatedly calling fit or partial_fit when warm_start is True can<br>result in a different solution than when calling fit a single time<br>because of the way the data is shuffled.<br>If a dynamic learning rate is used, the learning rate is adapted<br>depending on the number of samples already seen. Calling ``fit`` resets<br>this counter, while ``partial_fit`` will result in increasing the<br>existing counter.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('average',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=average,-bool%20or%20int%2C%20default%3DFalse\">\n",
       "            average\n",
       "            <span class=\"param-doc-description\">average: bool or int, default=False<br><br>When set to `True`, computes the averaged SGD weights across all<br>updates and stores the result in the ``coef_`` attribute. If set to<br>an int greater than 1, averaging will begin once the total number of<br>samples seen reaches `average`. So ``average=10`` will begin<br>averaging after seeing 10 samples.<br>Integer values must be in the range `[1, n_samples]`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling\n",
       "        .textContent.trim().split(' ')[0];\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "\n",
       "\n",
       "/**\n",
       " * Adapted from Skrub\n",
       " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
       " * @returns \"light\" or \"dark\"\n",
       " */\n",
       "function detectTheme(element) {\n",
       "    const body = document.querySelector('body');\n",
       "\n",
       "    // Check VSCode theme\n",
       "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
       "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
       "\n",
       "    if (themeKindAttr && themeNameAttr) {\n",
       "        const themeKind = themeKindAttr.toLowerCase();\n",
       "        const themeName = themeNameAttr.toLowerCase();\n",
       "\n",
       "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
       "            return \"dark\";\n",
       "        }\n",
       "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
       "            return \"light\";\n",
       "        }\n",
       "    }\n",
       "\n",
       "    // Check Jupyter theme\n",
       "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
       "        return 'dark';\n",
       "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
       "        return 'light';\n",
       "    }\n",
       "\n",
       "    // Guess based on a parent element's color\n",
       "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
       "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
       "    if (match) {\n",
       "        const [r, g, b] = [\n",
       "            parseFloat(match[1]),\n",
       "            parseFloat(match[2]),\n",
       "            parseFloat(match[3])\n",
       "        ];\n",
       "\n",
       "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
       "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
       "\n",
       "        if (luma > 180) {\n",
       "            // If the text is very bright we have a dark theme\n",
       "            return 'dark';\n",
       "        }\n",
       "        if (luma < 75) {\n",
       "            // If the text is very dark we have a light theme\n",
       "            return 'light';\n",
       "        }\n",
       "        // Otherwise fall back to the next heuristic.\n",
       "    }\n",
       "\n",
       "    // Fallback to system preference\n",
       "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
       "}\n",
       "\n",
       "\n",
       "function forceTheme(elementId) {\n",
       "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
       "    if (estimatorElement === null) {\n",
       "        console.error(`Element with id ${elementId} not found.`);\n",
       "    } else {\n",
       "        const theme = detectTheme(estimatorElement);\n",
       "        estimatorElement.classList.add(theme);\n",
       "    }\n",
       "}\n",
       "\n",
       "forceTheme('sk-container-id-1');</script></body>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('data_transformer',\n",
       "                                              ColumnTransformer(transformers=[('categorical',\n",
       "                                                                               Pipeline(steps=[('fill_tail',\n",
       "                                                                                                GroupedModeImputer(group_cols=['Carrier '\n",
       "                                                                                                                               'Code',\n",
       "                                                                                                                               'Destination '\n",
       "                                                                                                                               'Airport'],\n",
       "                                                                                                                   target_col='Tail '\n",
       "                                                                                                                              'Number')),\n",
       "                                                                                               ('encode',\n",
       "                                                                                                OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                                               ['Carrier '\n",
       "                                                                                'Code',\n",
       "                                                                                'Destination '\n",
       "                                                                                'Airport',\n",
       "                                                                                'Tail '\n",
       "                                                                                'Number...\n",
       "                                             ('logistic_clf',\n",
       "                                              SGDClassifier(loss='log_loss',\n",
       "                                                            n_jobs=-1,\n",
       "                                                            penalty='elasticnet'))]),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'logistic_clf__alpha': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x000001B1E9E12900>,\n",
       "                                        'logistic_clf__l1_ratio': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x000001B1E9E6D090>},\n",
       "                   scoring='balanced_accuracy')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import loguniform, uniform\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "logistic_pipeline = Pipeline([\n",
    "    ('data_transformer', feature_transformer),\n",
    "    ('logistic_clf', SGDClassifier(loss='log_loss', penalty='elasticnet', n_jobs=-1)) # log_loss makes it a logistic regression\n",
    "])\n",
    "\n",
    "logistic_param_distribs = {'logistic_clf__alpha': loguniform(1e-6, 1),\n",
    "                  'logistic_clf__l1_ratio': uniform(loc=0., scale=1.)}\n",
    "logistic_search = RandomizedSearchCV(logistic_pipeline, param_distributions=logistic_param_distribs, \n",
    "                                n_iter=100, cv=3, n_jobs=-1, scoring='balanced_accuracy')\n",
    "logistic_search.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90724d3b-93ee-460e-8ab1-5e6ccbe3eab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.5967421246321696, 'Precision': 0.5023751125461827, 'Recall': 0.5770581836985789, 'F1': 0.5371330882413962}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "best_logistic_model = logistic_search.best_estimator_\n",
    "logistic_pred = best_logistic_model.predict(X_test)\n",
    "logistic_results = {'Accuracy': accuracy_score(y_test, logistic_pred),\n",
    "                    'Precision': precision_score(y_test, logistic_pred),\n",
    "                    'Recall': recall_score(y_test, logistic_pred),\n",
    "                    'F1': f1_score(y_test, logistic_pred)}\n",
    "print(logistic_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a3b99-412c-4493-9f92-9b0d5fca10c0",
   "metadata": {},
   "source": [
    "## Nonlinear SVM\n",
    "Next up is a nonlinear support vector machine. Given the uncertain nature of the decision boundary, I will be using rbf since some distance similarity bewteen data points is likely to reveal some underlying structure (e.g. the same destination airport by the same carrier should be more similar than one with a different airport and carrier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38d7e883-b57c-4dbd-a5d6-a2acfd9f6112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan\\anaconda3\\envs\\pytorch-env\\Lib\\site-packages\\sklearn\\svm\\_base.py:313: ConvergenceWarning: Solver terminated early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "}\n",
       "\n",
       "#sk-container-id-6.light {\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: black;\n",
       "  --sklearn-color-background: white;\n",
       "  --sklearn-color-border-box: black;\n",
       "  --sklearn-color-icon: #696969;\n",
       "}\n",
       "\n",
       "#sk-container-id-6.dark {\n",
       "  --sklearn-color-text-on-default-background: white;\n",
       "  --sklearn-color-background: #111;\n",
       "  --sklearn-color-border-box: white;\n",
       "  --sklearn-color-icon: #878787;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-6 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: center;\n",
       "  justify-content: center;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-6 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-6 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-6 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table {\n",
       "    font-family: monospace;\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table summary::marker {\n",
       "    font-size: 0.7rem;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       "/*\n",
       "    `table td`is set in notebook with right text-align.\n",
       "    We need to overwrite it.\n",
       "*/\n",
       ".estimator-table table td.param {\n",
       "    text-align: left;\n",
       "    position: relative;\n",
       "    padding: 0;\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td.value {\n",
       "    color:rgb(255, 94, 0);\n",
       "    background-color: transparent;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "/*\n",
       "    Styles for parameter documentation links\n",
       "    We need styling for visited so jupyter doesn't overwrite it\n",
       "*/\n",
       "a.param-doc-link,\n",
       "a.param-doc-link:link,\n",
       "a.param-doc-link:visited {\n",
       "    text-decoration: underline dashed;\n",
       "    text-underline-offset: .3em;\n",
       "    color: inherit;\n",
       "    display: block;\n",
       "    padding: .5em;\n",
       "}\n",
       "\n",
       "/* \"hack\" to make the entire area of the cell containing the link clickable */\n",
       "a.param-doc-link::before {\n",
       "    position: absolute;\n",
       "    content: \"\";\n",
       "    inset: 0;\n",
       "}\n",
       "\n",
       ".param-doc-description {\n",
       "    display: none;\n",
       "    position: absolute;\n",
       "    z-index: 9999;\n",
       "    left: 0;\n",
       "    padding: .5ex;\n",
       "    margin-left: 1.5em;\n",
       "    color: var(--sklearn-color-text);\n",
       "    box-shadow: .3em .3em .4em #999;\n",
       "    width: max-content;\n",
       "    text-align: left;\n",
       "    max-height: 10em;\n",
       "    overflow-y: auto;\n",
       "\n",
       "    /* unfitted */\n",
       "    background: var(--sklearn-color-unfitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       "/* Fitted state for parameter tooltips */\n",
       ".fitted .param-doc-description {\n",
       "    /* fitted */\n",
       "    background: var(--sklearn-color-fitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".param-doc-link:hover .param-doc-description {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[(&#x27;data_transformer&#x27;,\n",
       "                                              ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                                                               Pipeline(steps=[(&#x27;fill_tail&#x27;,\n",
       "                                                                                                GroupedModeImputer(group_cols=[&#x27;Carrier &#x27;\n",
       "                                                                                                                               &#x27;Code&#x27;,\n",
       "                                                                                                                               &#x27;Destination &#x27;\n",
       "                                                                                                                               &#x27;Airport&#x27;],\n",
       "                                                                                                                   target_col=&#x27;Tail &#x27;\n",
       "                                                                                                                              &#x27;Number&#x27;)),\n",
       "                                                                                               (&#x27;encode&#x27;,\n",
       "                                                                                                OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                                               [&#x27;Carrier &#x27;\n",
       "                                                                                &#x27;Code&#x27;,\n",
       "                                                                                &#x27;Destination &#x27;\n",
       "                                                                                &#x27;Airport&#x27;,\n",
       "                                                                                &#x27;Tail &#x27;\n",
       "                                                                                &#x27;Number...\n",
       "                                                                              (&#x27;hour_cos&#x27;,\n",
       "                                                                               CosTransformer(period=24),\n",
       "                                                                               [&#x27;Hour&#x27;])])),\n",
       "                                             (&#x27;svm_clf&#x27;, SVC(max_iter=2000))]),\n",
       "                   n_iter=30, n_jobs=-1,\n",
       "                   param_distributions={&#x27;svm_clf__C&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000012680CFADD0&gt;,\n",
       "                                        &#x27;svm_clf__gamma&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000012680CFA350&gt;},\n",
       "                   scoring=&#x27;balanced_accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-76\" type=\"checkbox\" ><label for=\"sk-estimator-id-76\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomizedSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">?<span>Documentation for RandomizedSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('estimator',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=estimator,-estimator%20object\">\n",
       "            estimator\n",
       "            <span class=\"param-doc-description\">estimator: estimator object<br><br>An object of that type is instantiated for each grid point.<br>This is assumed to implement the scikit-learn estimator interface.<br>Either estimator needs to provide a ``score`` function,<br>or ``scoring`` must be passed.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">Pipeline(step..._iter=2000))])</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('param_distributions',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=param_distributions,-dict%20or%20list%20of%20dicts\">\n",
       "            param_distributions\n",
       "            <span class=\"param-doc-description\">param_distributions: dict or list of dicts<br><br>Dictionary with parameters names (`str`) as keys and distributions<br>or lists of parameters to try. Distributions must provide a ``rvs``<br>method for sampling (such as those from scipy.stats.distributions).<br>If a list is given, it is sampled uniformly.<br>If a list of dicts is given, first a dict is sampled uniformly, and<br>then a parameter is sampled using that dict as above.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">{&#x27;svm_clf__C&#x27;: &lt;scipy.stats....0012680CFADD0&gt;, &#x27;svm_clf__gamma&#x27;: &lt;scipy.stats....0012680CFA350&gt;}</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=n_iter,-int%2C%20default%3D10\">\n",
       "            n_iter\n",
       "            <span class=\"param-doc-description\">n_iter: int, default=10<br><br>Number of parameter settings that are sampled. n_iter trades<br>off runtime vs quality of the solution.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">30</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('scoring',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=scoring,-str%2C%20callable%2C%20list%2C%20tuple%20or%20dict%2C%20default%3DNone\">\n",
       "            scoring\n",
       "            <span class=\"param-doc-description\">scoring: str, callable, list, tuple or dict, default=None<br><br>Strategy to evaluate the performance of the cross-validated model on<br>the test set.<br><br>If `scoring` represents a single score, one can use:<br><br>- a single string (see :ref:`scoring_string_names`);<br>- a callable (see :ref:`scoring_callable`) that returns a single value;<br>- `None`, the `estimator`'s<br>  :ref:`default evaluation criterion <scoring_api_overview>` is used.<br><br>If `scoring` represents multiple scores, one can use:<br><br>- a list or tuple of unique strings;<br>- a callable returning a dictionary where the keys are the metric<br>  names and the values are the metric scores;<br>- a dictionary with metric names as keys and callables as values.<br><br>See :ref:`multimetric_grid_search` for an example.<br><br>If None, the estimator's score method is used.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;balanced_accuracy&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=n_jobs,-int%2C%20default%3DNone\">\n",
       "            n_jobs\n",
       "            <span class=\"param-doc-description\">n_jobs: int, default=None<br><br>Number of jobs to run in parallel.<br>``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.<br>``-1`` means using all processors. See :term:`Glossary <n_jobs>`<br>for more details.<br><br>.. versionchanged:: v0.20<br>   `n_jobs` default changed from 1 to None</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">-1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('refit',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=refit,-bool%2C%20str%2C%20or%20callable%2C%20default%3DTrue\">\n",
       "            refit\n",
       "            <span class=\"param-doc-description\">refit: bool, str, or callable, default=True<br><br>Refit an estimator using the best found parameters on the whole<br>dataset.<br><br>For multiple metric evaluation, this needs to be a `str` denoting the<br>scorer that would be used to find the best parameters for refitting<br>the estimator at the end.<br><br>Where there are considerations other than maximum score in<br>choosing a best estimator, ``refit`` can be set to a function which<br>returns the selected ``best_index_`` given the ``cv_results_``. In that<br>case, the ``best_estimator_`` and ``best_params_`` will be set<br>according to the returned ``best_index_`` while the ``best_score_``<br>attribute will not be available.<br><br>The refitted estimator is made available at the ``best_estimator_``<br>attribute and permits using ``predict`` directly on this<br>``RandomizedSearchCV`` instance.<br><br>Also for multiple metric evaluation, the attributes ``best_index_``,<br>``best_score_`` and ``best_params_`` will only be available if<br>``refit`` is set and all of them will be determined w.r.t this specific<br>scorer.<br><br>See ``scoring`` parameter to know more about multiple metric<br>evaluation.<br><br>See :ref:`this example<br><sphx_glr_auto_examples_model_selection_plot_grid_search_refit_callable.py>`<br>for an example of how to use ``refit=callable`` to balance model<br>complexity and cross-validated score.<br><br>.. versionchanged:: 0.20<br>    Support for callable added.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('cv',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=cv,-int%2C%20cross-validation%20generator%20or%20an%20iterable%2C%20default%3DNone\">\n",
       "            cv\n",
       "            <span class=\"param-doc-description\">cv: int, cross-validation generator or an iterable, default=None<br><br>Determines the cross-validation splitting strategy.<br>Possible inputs for cv are:<br><br>- None, to use the default 5-fold cross validation,<br>- integer, to specify the number of folds in a `(Stratified)KFold`,<br>- :term:`CV splitter`,<br>- An iterable yielding (train, test) splits as arrays of indices.<br><br>For integer/None inputs, if the estimator is a classifier and ``y`` is<br>either binary or multiclass, :class:`StratifiedKFold` is used. In all<br>other cases, :class:`KFold` is used. These splitters are instantiated<br>with `shuffle=False` so the splits will be the same across calls.<br><br>Refer :ref:`User Guide <cross_validation>` for the various<br>cross-validation strategies that can be used here.<br><br>.. versionchanged:: 0.22<br>    ``cv`` default value if None changed from 3-fold to 5-fold.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=verbose,-int\">\n",
       "            verbose\n",
       "            <span class=\"param-doc-description\">verbose: int<br><br>Controls the verbosity: the higher, the more messages.<br><br>- >1 : the computation time for each fold and parameter candidate is<br>  displayed;<br>- >2 : the score is also displayed;<br>- >3 : the fold and candidate parameter indexes are also displayed<br>  together with the starting time of the computation.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('pre_dispatch',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=pre_dispatch,-int%2C%20or%20str%2C%20default%3D%272%2An_jobs%27\">\n",
       "            pre_dispatch\n",
       "            <span class=\"param-doc-description\">pre_dispatch: int, or str, default='2*n_jobs'<br><br>Controls the number of jobs that get dispatched during parallel<br>execution. Reducing this number can be useful to avoid an<br>explosion of memory consumption when more jobs get dispatched<br>than CPUs can process. This parameter can be:<br><br>- None, in which case all the jobs are immediately created and spawned. Use<br>  this for lightweight and fast-running jobs, to avoid delays due to on-demand<br>  spawning of the jobs<br>- An int, giving the exact number of total jobs that are spawned<br>- A str, giving an expression as a function of n_jobs, as in '2*n_jobs'</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;2*n_jobs&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=random_state,-int%2C%20RandomState%20instance%20or%20None%2C%20default%3DNone\">\n",
       "            random_state\n",
       "            <span class=\"param-doc-description\">random_state: int, RandomState instance or None, default=None<br><br>Pseudo random number generator state used for random uniform sampling<br>from lists of possible values instead of scipy.stats distributions.<br>Pass an int for reproducible output across multiple<br>function calls.<br>See :term:`Glossary <random_state>`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('error_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=error_score,-%27raise%27%20or%20numeric%2C%20default%3Dnp.nan\">\n",
       "            error_score\n",
       "            <span class=\"param-doc-description\">error_score: 'raise' or numeric, default=np.nan<br><br>Value to assign to the score if an error occurs in estimator fitting.<br>If set to 'raise', the error is raised. If a numeric value is given,<br>FitFailedWarning is raised. This parameter does not affect the refit<br>step, which will always raise the error.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">nan</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('return_train_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#:~:text=return_train_score,-bool%2C%20default%3DFalse\">\n",
       "            return_train_score\n",
       "            <span class=\"param-doc-description\">return_train_score: bool, default=False<br><br>If ``False``, the ``cv_results_`` attribute will not include training<br>scores.<br>Computing training scores is used to get insights on how different<br>parameter settings impact the overfitting/underfitting trade-off.<br>However computing the scores on the training set can be computationally<br>expensive and is not strictly required to select the parameters that<br>yield the best generalization performance.<br><br>.. versionadded:: 0.19<br><br>.. versionchanged:: 0.21<br>    Default value was changed from ``True`` to ``False``</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-77\" type=\"checkbox\" ><label for=\"sk-estimator-id-77\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: Pipeline</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___\"></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-78\" type=\"checkbox\" ><label for=\"sk-estimator-id-78\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>data_transformer: ColumnTransformer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for data_transformer: ColumnTransformer</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transformers',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=transformers,-list%20of%20tuples\">\n",
       "            transformers\n",
       "            <span class=\"param-doc-description\">transformers: list of tuples<br><br>List of (name, transformer, columns) tuples specifying the<br>transformer objects to be applied to subsets of the data.<br><br>name : str<br>    Like in Pipeline and FeatureUnion, this allows the transformer and<br>    its parameters to be set using ``set_params`` and searched in grid<br>    search.<br>transformer : {'drop', 'passthrough'} or estimator<br>    Estimator must support :term:`fit` and :term:`transform`.<br>    Special-cased strings 'drop' and 'passthrough' are accepted as<br>    well, to indicate to drop the columns or to pass them through<br>    untransformed, respectively.<br>columns :  str, array-like of str, int, array-like of int,                 array-like of bool, slice or callable<br>    Indexes the data on its second axis. Integers are interpreted as<br>    positional columns, while strings can reference DataFrame columns<br>    by name.  A scalar string or int should be used where<br>    ``transformer`` expects X to be a 1d array-like (vector),<br>    otherwise a 2d array will be passed to the transformer.<br>    A callable is passed the input data `X` and can return any of the<br>    above. To select multiple columns by name or dtype, you can use<br>    :obj:`make_column_selector`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">[(&#x27;categorical&#x27;, ...), (&#x27;day_sin&#x27;, ...), ...]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('remainder',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=remainder,-%7B%27drop%27%2C%20%27passthrough%27%7D%20or%20estimator%2C%20default%3D%27drop%27\">\n",
       "            remainder\n",
       "            <span class=\"param-doc-description\">remainder: {'drop', 'passthrough'} or estimator, default='drop'<br><br>By default, only the specified columns in `transformers` are<br>transformed and combined in the output, and the non-specified<br>columns are dropped. (default of ``'drop'``).<br>By specifying ``remainder='passthrough'``, all remaining columns that<br>were not specified in `transformers`, but present in the data passed<br>to `fit` will be automatically passed through. This subset of columns<br>is concatenated with the output of the transformers. For dataframes,<br>extra columns not seen during `fit` will be excluded from the output<br>of `transform`.<br>By setting ``remainder`` to be an estimator, the remaining<br>non-specified columns will use the ``remainder`` estimator. The<br>estimator must support :term:`fit` and :term:`transform`.<br>Note that using this feature requires that the DataFrame columns<br>input at :term:`fit` and :term:`transform` have identical order.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;drop&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sparse_threshold',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=sparse_threshold,-float%2C%20default%3D0.3\">\n",
       "            sparse_threshold\n",
       "            <span class=\"param-doc-description\">sparse_threshold: float, default=0.3<br><br>If the output of the different transformers contains sparse matrices,<br>these will be stacked as a sparse matrix if the overall density is<br>lower than this value. Use ``sparse_threshold=0`` to always return<br>dense.  When the transformed output consists of all dense data, the<br>stacked result will be dense, and this keyword will be ignored.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=n_jobs,-int%2C%20default%3DNone\">\n",
       "            n_jobs\n",
       "            <span class=\"param-doc-description\">n_jobs: int, default=None<br><br>Number of jobs to run in parallel.<br>``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.<br>``-1`` means using all processors. See :term:`Glossary <n_jobs>`<br>for more details.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transformer_weights',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=transformer_weights,-dict%2C%20default%3DNone\">\n",
       "            transformer_weights\n",
       "            <span class=\"param-doc-description\">transformer_weights: dict, default=None<br><br>Multiplicative weights for features per transformer. The output of the<br>transformer is multiplied by these weights. Keys are transformer names,<br>values the weights.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=verbose,-bool%2C%20default%3DFalse\">\n",
       "            verbose\n",
       "            <span class=\"param-doc-description\">verbose: bool, default=False<br><br>If True, the time elapsed while fitting each transformer will be<br>printed as it is completed.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose_feature_names_out',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=verbose_feature_names_out,-bool%2C%20str%20or%20Callable%5B%5Bstr%2C%20str%5D%2C%20str%5D%2C%20default%3DTrue\">\n",
       "            verbose_feature_names_out\n",
       "            <span class=\"param-doc-description\">verbose_feature_names_out: bool, str or Callable[[str, str], str], default=True<br><br>- If True, :meth:`ColumnTransformer.get_feature_names_out` will prefix<br>  all feature names with the name of the transformer that generated that<br>  feature. It is equivalent to setting<br>  `verbose_feature_names_out=\"{transformer_name}__{feature_name}\"`.<br>- If False, :meth:`ColumnTransformer.get_feature_names_out` will not<br>  prefix any feature names and will error if feature names are not<br>  unique.<br>- If ``Callable[[str, str], str]``,<br>  :meth:`ColumnTransformer.get_feature_names_out` will rename all the features<br>  using the name of the transformer. The first argument of the callable is the<br>  transformer name and the second argument is the feature name. The returned<br>  string will be the new feature name.<br>- If ``str``, it must be a string ready for formatting. The given string will<br>  be formatted using two field names: ``transformer_name`` and ``feature_name``.<br>  e.g. ``\"{feature_name}__{transformer_name}\"``. See :meth:`str.format` method<br>  from the standard library for more info.<br><br>.. versionadded:: 1.0<br><br>.. versionchanged:: 1.6<br>    `verbose_feature_names_out` can be a callable or a string to be formatted.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('force_int_remainder_cols',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html#:~:text=force_int_remainder_cols,-bool%2C%20default%3DFalse\">\n",
       "            force_int_remainder_cols\n",
       "            <span class=\"param-doc-description\">force_int_remainder_cols: bool, default=False<br><br>This parameter has no effect.<br><br>.. note::<br>    If you do not access the list of columns for the remainder columns<br>    in the `transformers_` fitted attribute, you do not need to set<br>    this parameter.<br><br>.. versionadded:: 1.5<br><br>.. versionchanged:: 1.7<br>   The default value for `force_int_remainder_cols` will change from<br>   `True` to `False` in version 1.7.<br><br>.. deprecated:: 1.7<br>   `force_int_remainder_cols` is deprecated and will be removed in 1.9.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-79\" type=\"checkbox\" ><label for=\"sk-estimator-id-79\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>categorical</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__categorical__\"><pre>[&#x27;Carrier Code&#x27;, &#x27;Destination Airport&#x27;, &#x27;Tail Number&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-80\" type=\"checkbox\" ><label for=\"sk-estimator-id-80\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GroupedModeImputer</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__categorical__fill_tail__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('group_cols',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">group_cols</td>\n",
       "            <td class=\"value\">[&#x27;Carrier Code&#x27;, &#x27;Destination Airport&#x27;]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('target_col',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">target_col</td>\n",
       "            <td class=\"value\">&#x27;Tail Number&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-81\" type=\"checkbox\" ><label for=\"sk-estimator-id-81\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>OneHotEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__categorical__encode__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('categories',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=categories,-%27auto%27%20or%20a%20list%20of%20array-like%2C%20default%3D%27auto%27\">\n",
       "            categories\n",
       "            <span class=\"param-doc-description\">categories: 'auto' or a list of array-like, default='auto'<br><br>Categories (unique values) per feature:<br><br>- 'auto' : Determine categories automatically from the training data.<br>- list : ``categories[i]`` holds the categories expected in the ith<br>  column. The passed categories should not mix strings and numeric<br>  values within a single feature, and should be sorted in case of<br>  numeric values.<br><br>The used categories can be found in the ``categories_`` attribute.<br><br>.. versionadded:: 0.20</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;auto&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('drop',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=drop,-%7B%27first%27%2C%20%27if_binary%27%7D%20or%20an%20array-like%20of%20shape%20%28n_features%2C%29%2C%20%20%20%20%20%20%20%20%20%20%20%20%20default%3DNone\">\n",
       "            drop\n",
       "            <span class=\"param-doc-description\">drop: {'first', 'if_binary'} or an array-like of shape (n_features,),             default=None<br><br>Specifies a methodology to use to drop one of the categories per<br>feature. This is useful in situations where perfectly collinear<br>features cause problems, such as when feeding the resulting data<br>into an unregularized linear regression model.<br><br>However, dropping one category breaks the symmetry of the original<br>representation and can therefore induce a bias in downstream models,<br>for instance for penalized linear classification or regression models.<br><br>- None : retain all features (the default).<br>- 'first' : drop the first category in each feature. If only one<br>  category is present, the feature will be dropped entirely.<br>- 'if_binary' : drop the first category in each feature with two<br>  categories. Features with 1 or more than 2 categories are<br>  left intact.<br>- array : ``drop[i]`` is the category in feature ``X[:, i]`` that<br>  should be dropped.<br><br>When `max_categories` or `min_frequency` is configured to group<br>infrequent categories, the dropping behavior is handled after the<br>grouping.<br><br>.. versionadded:: 0.21<br>   The parameter `drop` was added in 0.21.<br><br>.. versionchanged:: 0.23<br>   The option `drop='if_binary'` was added in 0.23.<br><br>.. versionchanged:: 1.1<br>    Support for dropping infrequent categories.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sparse_output',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=sparse_output,-bool%2C%20default%3DTrue\">\n",
       "            sparse_output\n",
       "            <span class=\"param-doc-description\">sparse_output: bool, default=True<br><br>When ``True``, it returns a :class:`scipy.sparse.csr_matrix`,<br>i.e. a sparse matrix in \"Compressed Sparse Row\" (CSR) format.<br><br>.. versionadded:: 1.2<br>   `sparse` was renamed to `sparse_output`</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dtype',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=dtype,-number%20type%2C%20default%3Dnp.float64\">\n",
       "            dtype\n",
       "            <span class=\"param-doc-description\">dtype: number type, default=np.float64<br><br>Desired dtype of output.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&lt;class &#x27;numpy.float64&#x27;&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('handle_unknown',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=handle_unknown,-%7B%27error%27%2C%20%27ignore%27%2C%20%27infrequent_if_exist%27%2C%20%27warn%27%7D%2C%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20default%3D%27error%27\">\n",
       "            handle_unknown\n",
       "            <span class=\"param-doc-description\">handle_unknown: {'error', 'ignore', 'infrequent_if_exist', 'warn'},                      default='error'<br><br>Specifies the way unknown categories are handled during :meth:`transform`.<br><br>- 'error' : Raise an error if an unknown category is present during transform.<br>- 'ignore' : When an unknown category is encountered during<br>  transform, the resulting one-hot encoded columns for this feature<br>  will be all zeros. In the inverse transform, an unknown category<br>  will be denoted as None.<br>- 'infrequent_if_exist' : When an unknown category is encountered<br>  during transform, the resulting one-hot encoded columns for this<br>  feature will map to the infrequent category if it exists. The<br>  infrequent category will be mapped to the last position in the<br>  encoding. During inverse transform, an unknown category will be<br>  mapped to the category denoted `'infrequent'` if it exists. If the<br>  `'infrequent'` category does not exist, then :meth:`transform` and<br>  :meth:`inverse_transform` will handle an unknown category as with<br>  `handle_unknown='ignore'`. Infrequent categories exist based on<br>  `min_frequency` and `max_categories`. Read more in the<br>  :ref:`User Guide <encoder_infrequent_categories>`.<br>- 'warn' : When an unknown category is encountered during transform<br>  a warning is issued, and the encoding then proceeds as described for<br>  `handle_unknown=\"infrequent_if_exist\"`.<br><br>.. versionchanged:: 1.1<br>    `'infrequent_if_exist'` was added to automatically handle unknown<br>    categories and infrequent categories.<br><br>.. versionadded:: 1.6<br>   The option `\"warn\"` was added in 1.6.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;ignore&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_frequency',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=min_frequency,-int%20or%20float%2C%20default%3DNone\">\n",
       "            min_frequency\n",
       "            <span class=\"param-doc-description\">min_frequency: int or float, default=None<br><br>Specifies the minimum frequency below which a category will be<br>considered infrequent.<br><br>- If `int`, categories with a smaller cardinality will be considered<br>  infrequent.<br><br>- If `float`, categories with a smaller cardinality than<br>  `min_frequency * n_samples`  will be considered infrequent.<br><br>.. versionadded:: 1.1<br>    Read more in the :ref:`User Guide <encoder_infrequent_categories>`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_categories',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=max_categories,-int%2C%20default%3DNone\">\n",
       "            max_categories\n",
       "            <span class=\"param-doc-description\">max_categories: int, default=None<br><br>Specifies an upper limit to the number of output features for each input<br>feature when considering infrequent categories. If there are infrequent<br>categories, `max_categories` includes the category representing the<br>infrequent categories along with the frequent categories. If `None`,<br>there is no limit to the number of output features.<br><br>.. versionadded:: 1.1<br>    Read more in the :ref:`User Guide <encoder_infrequent_categories>`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('feature_name_combiner',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.preprocessing.OneHotEncoder.html#:~:text=feature_name_combiner,-%22concat%22%20or%20callable%2C%20default%3D%22concat%22\">\n",
       "            feature_name_combiner\n",
       "            <span class=\"param-doc-description\">feature_name_combiner: \"concat\" or callable, default=\"concat\"<br><br>Callable with signature `def callable(input_feature, category)` that returns a<br>string. This is used to create feature names to be returned by<br>:meth:`get_feature_names_out`.<br><br>`\"concat\"` concatenates encoded feature name and category with<br>`feature + \"_\" + str(category)`.E.g. feature X with values 1, 6, 7 create<br>feature names `X_1, X_6, X_7`.<br><br>.. versionadded:: 1.3</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;concat&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-82\" type=\"checkbox\" ><label for=\"sk-estimator-id-82\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>day_sin</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__day_sin__\"><pre>[&#x27;DayOfYear&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-83\" type=\"checkbox\" ><label for=\"sk-estimator-id-83\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SinTransformer</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__day_sin__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('period',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">period</td>\n",
       "            <td class=\"value\">365</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-84\" type=\"checkbox\" ><label for=\"sk-estimator-id-84\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>day_cos</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__day_cos__\"><pre>[&#x27;DayOfYear&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-85\" type=\"checkbox\" ><label for=\"sk-estimator-id-85\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>CosTransformer</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__day_cos__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('period',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">period</td>\n",
       "            <td class=\"value\">365</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-86\" type=\"checkbox\" ><label for=\"sk-estimator-id-86\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>hour_sin</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__hour_sin__\"><pre>[&#x27;Hour&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-87\" type=\"checkbox\" ><label for=\"sk-estimator-id-87\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SinTransformer</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__hour_sin__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('period',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">period</td>\n",
       "            <td class=\"value\">24</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-88\" type=\"checkbox\" ><label for=\"sk-estimator-id-88\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>hour_cos</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__hour_cos__\"><pre>[&#x27;Hour&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-89\" type=\"checkbox\" ><label for=\"sk-estimator-id-89\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>CosTransformer</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___data_transformer__hour_cos__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('period',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">period</td>\n",
       "            <td class=\"value\">24</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-90\" type=\"checkbox\" ><label for=\"sk-estimator-id-90\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___svm_clf__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=C,-float%2C%20default%3D1.0\">\n",
       "            C\n",
       "            <span class=\"param-doc-description\">C: float, default=1.0<br><br>Regularization parameter. The strength of the regularization is<br>inversely proportional to C. Must be strictly positive. The penalty<br>is a squared l2 penalty. For an intuitive visualization of the effects<br>of scaling the regularization parameter C, see<br>:ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">np.float64(0....4670348495157)</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('kernel',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=kernel,-%7B%27linear%27%2C%20%27poly%27%2C%20%27rbf%27%2C%20%27sigmoid%27%2C%20%27precomputed%27%7D%20or%20callable%2C%20%20%20%20%20%20%20%20%20%20default%3D%27rbf%27\">\n",
       "            kernel\n",
       "            <span class=\"param-doc-description\">kernel: {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'<br><br>Specifies the kernel type to be used in the algorithm. If<br>none is given, 'rbf' will be used. If a callable is given it is used to<br>pre-compute the kernel matrix from data matrices; that matrix should be<br>an array of shape ``(n_samples, n_samples)``. For an intuitive<br>visualization of different kernel types see<br>:ref:`sphx_glr_auto_examples_svm_plot_svm_kernels.py`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;rbf&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('degree',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=degree,-int%2C%20default%3D3\">\n",
       "            degree\n",
       "            <span class=\"param-doc-description\">degree: int, default=3<br><br>Degree of the polynomial kernel function ('poly').<br>Must be non-negative. Ignored by all other kernels.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('gamma',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=gamma,-%7B%27scale%27%2C%20%27auto%27%7D%20or%20float%2C%20default%3D%27scale%27\">\n",
       "            gamma\n",
       "            <span class=\"param-doc-description\">gamma: {'scale', 'auto'} or float, default='scale'<br><br>Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.<br><br>- if ``gamma='scale'`` (default) is passed then it uses<br>  1 / (n_features * X.var()) as value of gamma,<br>- if 'auto', uses 1 / n_features<br>- if float, must be non-negative.<br><br>.. versionchanged:: 0.22<br>   The default value of ``gamma`` changed from 'auto' to 'scale'.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">np.float64(0....2903203123675)</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('coef0',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=coef0,-float%2C%20default%3D0.0\">\n",
       "            coef0\n",
       "            <span class=\"param-doc-description\">coef0: float, default=0.0<br><br>Independent term in kernel function.<br>It is only significant in 'poly' and 'sigmoid'.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('shrinking',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=shrinking,-bool%2C%20default%3DTrue\">\n",
       "            shrinking\n",
       "            <span class=\"param-doc-description\">shrinking: bool, default=True<br><br>Whether to use the shrinking heuristic.<br>See the :ref:`User Guide <shrinking_svm>`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('probability',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=probability,-bool%2C%20default%3DFalse\">\n",
       "            probability\n",
       "            <span class=\"param-doc-description\">probability: bool, default=False<br><br>Whether to enable probability estimates. This must be enabled prior<br>to calling `fit`, will slow down that method as it internally uses<br>5-fold cross-validation, and `predict_proba` may be inconsistent with<br>`predict`. Read more in the :ref:`User Guide <scores_probabilities>`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=tol,-float%2C%20default%3D1e-3\">\n",
       "            tol\n",
       "            <span class=\"param-doc-description\">tol: float, default=1e-3<br><br>Tolerance for stopping criterion.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('cache_size',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=cache_size,-float%2C%20default%3D200\">\n",
       "            cache_size\n",
       "            <span class=\"param-doc-description\">cache_size: float, default=200<br><br>Specify the size of the kernel cache (in MB).</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">200</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=class_weight,-dict%20or%20%27balanced%27%2C%20default%3DNone\">\n",
       "            class_weight\n",
       "            <span class=\"param-doc-description\">class_weight: dict or 'balanced', default=None<br><br>Set the parameter C of class i to class_weight[i]*C for<br>SVC. If not given, all classes are supposed to have<br>weight one.<br>The \"balanced\" mode uses the values of y to automatically adjust<br>weights inversely proportional to class frequencies in the input data<br>as ``n_samples / (n_classes * np.bincount(y))``.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=verbose,-bool%2C%20default%3DFalse\">\n",
       "            verbose\n",
       "            <span class=\"param-doc-description\">verbose: bool, default=False<br><br>Enable verbose output. Note that this setting takes advantage of a<br>per-process runtime setting in libsvm that, if enabled, may not work<br>properly in a multithreaded context.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=max_iter,-int%2C%20default%3D-1\">\n",
       "            max_iter\n",
       "            <span class=\"param-doc-description\">max_iter: int, default=-1<br><br>Hard limit on iterations within solver, or -1 for no limit.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">2000</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('decision_function_shape',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=decision_function_shape,-%7B%27ovo%27%2C%20%27ovr%27%7D%2C%20default%3D%27ovr%27\">\n",
       "            decision_function_shape\n",
       "            <span class=\"param-doc-description\">decision_function_shape: {'ovo', 'ovr'}, default='ovr'<br><br>Whether to return a one-vs-rest ('ovr') decision function of shape<br>(n_samples, n_classes) as all other classifiers, or the original<br>one-vs-one ('ovo') decision function of libsvm which has shape<br>(n_samples, n_classes * (n_classes - 1) / 2). However, note that<br>internally, one-vs-one ('ovo') is always used as a multi-class strategy<br>to train models; an ovr matrix is only constructed from the ovo matrix.<br>The parameter is ignored for binary classification.<br><br>.. versionchanged:: 0.19<br>    decision_function_shape is 'ovr' by default.<br><br>.. versionadded:: 0.17<br>   *decision_function_shape='ovr'* is recommended.<br><br>.. versionchanged:: 0.17<br>   Deprecated *decision_function_shape='ovo' and None*.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;ovr&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('break_ties',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=break_ties,-bool%2C%20default%3DFalse\">\n",
       "            break_ties\n",
       "            <span class=\"param-doc-description\">break_ties: bool, default=False<br><br>If true, ``decision_function_shape='ovr'``, and number of classes > 2,<br>:term:`predict` will break ties according to the confidence values of<br>:term:`decision_function`; otherwise the first class among the tied<br>classes is returned. Please note that breaking ties comes at a<br>relatively high computational cost compared to a simple predict. See<br>:ref:`sphx_glr_auto_examples_svm_plot_svm_tie_breaking.py` for an<br>example of its usage with ``decision_function_shape='ovr'``.<br><br>.. versionadded:: 0.22</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=random_state,-int%2C%20RandomState%20instance%20or%20None%2C%20default%3DNone\">\n",
       "            random_state\n",
       "            <span class=\"param-doc-description\">random_state: int, RandomState instance or None, default=None<br><br>Controls the pseudo random number generation for shuffling the data for<br>probability estimates. Ignored when `probability` is False.<br>Pass an int for reproducible output across multiple function calls.<br>See :term:`Glossary <random_state>`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling\n",
       "        .textContent.trim().split(' ')[0];\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "\n",
       "\n",
       "/**\n",
       " * Adapted from Skrub\n",
       " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
       " * @returns \"light\" or \"dark\"\n",
       " */\n",
       "function detectTheme(element) {\n",
       "    const body = document.querySelector('body');\n",
       "\n",
       "    // Check VSCode theme\n",
       "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
       "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
       "\n",
       "    if (themeKindAttr && themeNameAttr) {\n",
       "        const themeKind = themeKindAttr.toLowerCase();\n",
       "        const themeName = themeNameAttr.toLowerCase();\n",
       "\n",
       "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
       "            return \"dark\";\n",
       "        }\n",
       "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
       "            return \"light\";\n",
       "        }\n",
       "    }\n",
       "\n",
       "    // Check Jupyter theme\n",
       "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
       "        return 'dark';\n",
       "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
       "        return 'light';\n",
       "    }\n",
       "\n",
       "    // Guess based on a parent element's color\n",
       "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
       "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
       "    if (match) {\n",
       "        const [r, g, b] = [\n",
       "            parseFloat(match[1]),\n",
       "            parseFloat(match[2]),\n",
       "            parseFloat(match[3])\n",
       "        ];\n",
       "\n",
       "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
       "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
       "\n",
       "        if (luma > 180) {\n",
       "            // If the text is very bright we have a dark theme\n",
       "            return 'dark';\n",
       "        }\n",
       "        if (luma < 75) {\n",
       "            // If the text is very dark we have a light theme\n",
       "            return 'light';\n",
       "        }\n",
       "        // Otherwise fall back to the next heuristic.\n",
       "    }\n",
       "\n",
       "    // Fallback to system preference\n",
       "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
       "}\n",
       "\n",
       "\n",
       "function forceTheme(elementId) {\n",
       "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
       "    if (estimatorElement === null) {\n",
       "        console.error(`Element with id ${elementId} not found.`);\n",
       "    } else {\n",
       "        const theme = detectTheme(estimatorElement);\n",
       "        estimatorElement.classList.add(theme);\n",
       "    }\n",
       "}\n",
       "\n",
       "forceTheme('sk-container-id-6');</script></body>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('data_transformer',\n",
       "                                              ColumnTransformer(transformers=[('categorical',\n",
       "                                                                               Pipeline(steps=[('fill_tail',\n",
       "                                                                                                GroupedModeImputer(group_cols=['Carrier '\n",
       "                                                                                                                               'Code',\n",
       "                                                                                                                               'Destination '\n",
       "                                                                                                                               'Airport'],\n",
       "                                                                                                                   target_col='Tail '\n",
       "                                                                                                                              'Number')),\n",
       "                                                                                               ('encode',\n",
       "                                                                                                OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                                               ['Carrier '\n",
       "                                                                                'Code',\n",
       "                                                                                'Destination '\n",
       "                                                                                'Airport',\n",
       "                                                                                'Tail '\n",
       "                                                                                'Number...\n",
       "                                                                              ('hour_cos',\n",
       "                                                                               CosTransformer(period=24),\n",
       "                                                                               ['Hour'])])),\n",
       "                                             ('svm_clf', SVC(max_iter=2000))]),\n",
       "                   n_iter=30, n_jobs=-1,\n",
       "                   param_distributions={'svm_clf__C': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000012680CFADD0>,\n",
       "                                        'svm_clf__gamma': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000012680CFA350>},\n",
       "                   scoring='balanced_accuracy')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('data_transformer', feature_transformer),\n",
    "    ('svm_clf', SVC(kernel='rbf', max_iter=2000)) # radial basis function kernel\n",
    "])\n",
    "\n",
    "svm_param_distribs = {'svm_clf__C': uniform(loc=0., scale=1.),\n",
    "                      'svm_clf__gamma': loguniform(1e-6, 1)}\n",
    "svm_search = RandomizedSearchCV(svm_pipeline, param_distributions=svm_param_distribs, \n",
    "                                n_iter=30, cv=3, n_jobs=-1, scoring='balanced_accuracy')\n",
    "svm_search.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01e126fd-3fc9-4cb1-bc23-ebb04c874f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.5799900224855579, 'Precision': 0.4833291884274227, 'Recall': 0.5198195467270555, 'F1': 0.5009106842159524}\n"
     ]
    }
   ],
   "source": [
    "best_svm_model = svm_search.best_estimator_\n",
    "svm_pred = best_svm_model.predict(X_test)\n",
    "svm_results = {'Accuracy': accuracy_score(y_test, svm_pred),\n",
    "                    'Precision': precision_score(y_test, svm_pred),\n",
    "                    'Recall': recall_score(y_test, svm_pred),\n",
    "                    'F1': f1_score(y_test, svm_pred)}\n",
    "print(svm_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef1b91-3e5e-4885-8ec2-da13e1fa6e28",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Now on to an ensemble method: the random forest. This group of decision trees should be able to pare down to the features that are most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d8a0e1a-333e-4273-be0a-5795d178cdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del logistic_search\n",
    "del best_logistic_model\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8a69c13-45b2-41d9-8199-ee413b12d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def train_forest():\n",
    "    rnd_forest_pipeline = Pipeline([\n",
    "        ('data_transformer', feature_transformer),\n",
    "        ('rnd_forest_clf', RandomForestClassifier(criterion='gini', n_jobs=-1))\n",
    "    ])\n",
    "    \n",
    "    # We will be tuning our decision tree\n",
    "    param_distribs = {'rnd_forest_clf__min_samples_split': randint(low=10, high=30),\n",
    "                      'rnd_forest_clf__min_samples_leaf': randint(low=10, high=30),\n",
    "                      'rnd_forest_clf__max_leaf_nodes': randint(low=100, high=300),\n",
    "                      'rnd_forest_clf__max_depth': randint(low=10, high=30),\n",
    "                      'rnd_forest_clf__max_features': randint(low=100, high=300)}\n",
    "    rnd_forest_search = RandomizedSearchCV(rnd_forest_pipeline, param_distributions=param_distribs, \n",
    "                                    n_iter=60, cv=3, scoring='balanced_accuracy')\n",
    "    rnd_forest_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    return rnd_forest_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c82c75fd-dfb7-4c0e-80a6-e115d2ef999a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.6704528200938464, 'Precision': 0.6088344423025102, 'Recall': 0.5237424439649793, 'F1': 0.5630919059852}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "best_forest_model = train_forest()\n",
    "forest_pred = best_forest_model.predict(X_test)\n",
    "forest_results = {'Accuracy': accuracy_score(y_test, forest_pred),\n",
    "               'Precision': precision_score(y_test, forest_pred),\n",
    "               'Recall': recall_score(y_test, forest_pred),\n",
    "               'F1': f1_score(y_test, forest_pred)}\n",
    "print(forest_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cec856-d319-4d6c-8dba-128ccbd12ce4",
   "metadata": {},
   "source": [
    "## Aside: Most Important Features\n",
    "If we take a moment to explore the most important features from the random forest classifier, there are some interesting pieces of information. First, the date and time feature prominently in the top 10 most important features, which is to be expected. Next, there are several carriers that seem to be important for classifying a flight as on-time or delayed. It is not surprising that SouthWest featuers most prominently given its large proportion of delayed flights. Finally, the destination airport does factor in although that appears to be nearly an order of magnitude less important than the day, time, and carrier, in most instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17d3d872-6ac4-47cf-96b6-c5b185ffc3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical__Carrier Code_WN            0.281515\n",
      "hour_sin__Hour_sin                      0.200593\n",
      "categorical__Carrier Code_AS            0.093664\n",
      "hour_cos__Hour_cos                      0.064638\n",
      "categorical__Carrier Code_OO            0.059814\n",
      "categorical__Carrier Code_QX            0.057196\n",
      "categorical__Destination Airport_SEA    0.044340\n",
      "categorical__Carrier Code_DL            0.028912\n",
      "day_cos__DayOfYear_cos                  0.022718\n",
      "categorical__Destination Airport_MDW    0.009551\n",
      "categorical__Destination Airport_LGA    0.009378\n",
      "day_sin__DayOfYear_sin                  0.008862\n",
      "categorical__Destination Airport_GEG    0.007694\n",
      "categorical__Destination Airport_BOI    0.006444\n",
      "categorical__Carrier Code_AA            0.005622\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Use the name given in the 'steps' list ('preprocessor' in this case)\n",
    "feature_names = best_forest_model.named_steps['data_transformer'].get_feature_names_out(input_features=X_train.columns)\n",
    "rnd_forest_importances = best_forest_model.named_steps['rnd_forest_clf'].feature_importances_\n",
    "feature_importances_series = pd.Series(rnd_forest_importances, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "print(feature_importances_series[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43224a0b-601a-4973-a524-67f4305447d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.74     82230\n",
      "           1       0.61      0.52      0.56     56081\n",
      "\n",
      "    accuracy                           0.67    138311\n",
      "   macro avg       0.66      0.65      0.65    138311\n",
      "weighted avg       0.67      0.67      0.67    138311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest_report = classification_report(y_test, forest_pred)\n",
    "print(forest_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e662d87-8449-432e-bfc8-f4608a69b54b",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "Finally, we will move on to a neural network. This requires transforming our data using our existing ColumnTransformer and then transforming those columns into PyTorch tensors. (There will be an intermediary step of converting the sparse matrices to dense ones that numpy can handle.) Then I'll just make a simple NN with 2 hidden layers of 25 nodes each running the Adam optimizer. After running it for 100 epochs, I will see how it is looking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "1920fc5e-7a1d-4772-826b-53ade4a078fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import from_numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# features_trans_df = best_forest_model.named_steps['data_transformer'].transform(X_train)\n",
    "# X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(features_trans_df, labels_df, test_size=0.2)\n",
    "X_train_nn = best_forest_model.named_steps['data_transformer'].transform(X_train)\n",
    "X_test_nn = best_forest_model.named_steps['data_transformer'].transform(X_test)\n",
    "\n",
    "# Create dataset instances\n",
    "X_train_nn, X_val_nn, y_train_nn, y_val_nn = train_test_split(X_train_nn, y_train, test_size=0.20, stratify=y_train)\n",
    "\n",
    "X_train_tensor = from_numpy(X_train_nn.toarray()).float()\n",
    "y_train_tensor = from_numpy(y_train_nn.values).long() # Use .long() for classification labels\n",
    "\n",
    "X_val_tensor = from_numpy(X_val_nn.toarray()).float()\n",
    "y_val_tensor = from_numpy(y_val_nn.values).long()\n",
    "\n",
    "X_test_tensor = from_numpy(X_test_nn.toarray()).float()\n",
    "y_test_tensor = from_numpy(y_test.values).long()\n",
    "\n",
    "# X_train_tensor = from_numpy(X_train.values.astype(np.float32)).float()\n",
    "# y_train_tensor = from_numpy(y_train.values).long() # Use .long() for classification labels\n",
    "\n",
    "# X_test_tensor = from_numpy(X_test.values.astype(np.float32)).float()\n",
    "# y_test_tensor = from_numpy(y_test.values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a435a12-fc92-49bf-8348-68a3110dcc37",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FlightNNClf(nn.Module):\n",
    "    def __init__(self, input_dim=10, hidden_dim1=25, hidden_dim2=25, dropout_rate1=0.5, dropout_rate2=0.5, nonlin=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout1 = nn.Dropout(dropout_rate1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate2)\n",
    "        \n",
    "        self.output = nn.Linear(hidden_dim2, 1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.fc1(X.float())\n",
    "        X = self.nonlin(X)\n",
    "        X = self.dropout1(X)\n",
    "        \n",
    "        X = self.fc2(X)\n",
    "        X = self.nonlin(X)\n",
    "        X = self.dropout2(X)\n",
    "\n",
    "        X = self.output(X)\n",
    "        \n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cbd7cba-a6e1-411a-9779-4339994b08e4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5508\u001b[0m       \u001b[32m0.5600\u001b[0m        \u001b[35m2.6765\u001b[0m  10.5221\n",
      "      2        \u001b[36m0.5382\u001b[0m       0.5209        2.8675  12.4641\n",
      "      3        \u001b[36m0.5349\u001b[0m       0.5283        3.7768  12.4287\n",
      "      4        \u001b[36m0.5332\u001b[0m       0.4980        3.4580  12.4462\n",
      "      5        \u001b[36m0.5322\u001b[0m       0.4857        4.1224  12.4361\n",
      "      6        \u001b[36m0.5315\u001b[0m       0.4863        4.6975  12.4588\n",
      "      7        \u001b[36m0.5305\u001b[0m       0.4675        4.7428  12.4663\n",
      "      8        \u001b[36m0.5289\u001b[0m       0.4698        4.7508  12.4257\n",
      "      9        \u001b[36m0.5281\u001b[0m       0.4637        5.2188  12.4270\n",
      "     10        0.5288       0.4656        5.6451  12.3868\n",
      "     11        \u001b[36m0.5275\u001b[0m       0.4629        5.8639  12.4375\n",
      "     12        0.5276       0.4757        4.9997  12.4026\n",
      "     13        \u001b[36m0.5261\u001b[0m       0.4729        5.3794  12.4011\n",
      "     14        0.5275       0.4731        6.4598  12.4881\n",
      "     15        0.5261       0.4598        7.1748  12.4177\n",
      "     16        \u001b[36m0.5259\u001b[0m       0.4646        6.6831  12.4176\n",
      "     17        \u001b[36m0.5255\u001b[0m       0.4686        7.2039  12.4469\n",
      "     18        0.5259       0.4574        7.5680  12.4068\n",
      "     19        0.5256       0.4570        8.1231  12.4171\n",
      "     20        \u001b[36m0.5251\u001b[0m       0.4710        9.0016  12.6255\n",
      "     21        0.5256       0.4682        7.5694  12.8853\n",
      "     22        0.5256       0.4665        6.7614  12.6396\n",
      "     23        0.5254       0.4655        7.4453  13.2449\n",
      "     24        \u001b[36m0.5242\u001b[0m       0.4676        6.5128  12.5165\n",
      "     25        0.5251       0.4618        7.4260  12.6085\n",
      "     26        0.5245       0.4724        8.8303  12.4802\n",
      "     27        0.5245       0.4603        8.7224  12.4991\n",
      "     28        \u001b[36m0.5242\u001b[0m       0.4757        8.3942  12.4883\n",
      "     29        \u001b[36m0.5239\u001b[0m       0.4711        9.3669  12.4734\n",
      "     30        0.5241       0.4722       10.5138  12.4696\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5675\u001b[0m       \u001b[32m0.5211\u001b[0m        \u001b[35m0.7843\u001b[0m  12.6248\n",
      "      2        \u001b[36m0.5436\u001b[0m       0.5155        0.9008  12.4669\n",
      "      3        \u001b[36m0.5383\u001b[0m       0.4797        1.1368  12.4274\n",
      "      4        \u001b[36m0.5346\u001b[0m       0.5120        1.0213  12.5272\n",
      "      5        \u001b[36m0.5320\u001b[0m       0.4902        1.4077  12.4734\n",
      "      6        \u001b[36m0.5311\u001b[0m       0.4901        1.4362  12.4536\n",
      "      7        \u001b[36m0.5298\u001b[0m       0.4904        1.4751  12.4846\n",
      "      8        \u001b[36m0.5288\u001b[0m       0.4765        1.5028  12.6795\n",
      "      9        \u001b[36m0.5276\u001b[0m       0.4880        1.9692  13.7423\n",
      "     10        \u001b[36m0.5266\u001b[0m       0.4801        1.4147  13.4175\n",
      "     11        \u001b[36m0.5254\u001b[0m       0.4851        2.1226  12.4640\n",
      "     12        0.5264       0.4836        1.8854  12.4173\n",
      "     13        \u001b[36m0.5250\u001b[0m       0.4872        2.0273  12.4589\n",
      "     14        \u001b[36m0.5246\u001b[0m       0.4789        2.3835  12.6345\n",
      "     15        \u001b[36m0.5233\u001b[0m       0.4879        1.8837  12.7547\n",
      "     16        0.5236       0.4926        1.7241  12.4300\n",
      "     17        \u001b[36m0.5229\u001b[0m       0.4875        1.9968  12.4439\n",
      "     18        \u001b[36m0.5221\u001b[0m       0.4828        2.4093  12.4458\n",
      "     19        \u001b[36m0.5220\u001b[0m       0.4885        2.0392  12.4122\n",
      "     20        0.5221       0.4802        2.5170  12.4336\n",
      "     21        0.5226       0.4833        2.1413  12.4385\n",
      "     22        0.5225       0.4816        2.2233  12.4255\n",
      "     23        \u001b[36m0.5214\u001b[0m       0.4799        2.6025  12.3984\n",
      "     24        \u001b[36m0.5212\u001b[0m       0.4816        3.2438  12.3952\n",
      "     25        \u001b[36m0.5204\u001b[0m       0.4873        2.6424  12.4615\n",
      "     26        0.5208       0.4734        2.5665  12.4412\n",
      "     27        0.5206       0.4851        3.0779  12.4227\n",
      "     28        \u001b[36m0.5203\u001b[0m       0.4815        2.8417  12.4447\n",
      "     29        0.5206       0.4819        2.9021  12.3961\n",
      "     30        0.5209       0.4794        2.8106  12.4382\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4834\u001b[0m       \u001b[32m0.5790\u001b[0m        \u001b[35m0.7112\u001b[0m  12.5902\n",
      "      2        \u001b[36m0.4761\u001b[0m       \u001b[32m0.5800\u001b[0m        0.7157  12.4113\n",
      "      3        \u001b[36m0.4750\u001b[0m       \u001b[32m0.5804\u001b[0m        0.7160  12.4339\n",
      "      4        \u001b[36m0.4747\u001b[0m       \u001b[32m0.5886\u001b[0m        0.7189  12.4094\n",
      "      5        \u001b[36m0.4738\u001b[0m       \u001b[32m0.5892\u001b[0m        \u001b[35m0.7020\u001b[0m  12.4347\n",
      "      6        \u001b[36m0.4735\u001b[0m       \u001b[32m0.5947\u001b[0m        0.7053  12.4105\n",
      "      7        \u001b[36m0.4732\u001b[0m       0.5947        \u001b[35m0.6964\u001b[0m  12.3728\n",
      "      8        0.4735       0.5947        0.7089  12.4339\n",
      "      9        0.4733       0.5944        \u001b[35m0.6841\u001b[0m  12.4140\n",
      "     10        0.4735       \u001b[32m0.6006\u001b[0m        0.6956  12.3818\n",
      "     11        \u001b[36m0.4730\u001b[0m       0.5956        0.7005  12.3659\n",
      "     12        0.4731       0.5858        0.7111  10.5340\n",
      "     13        0.4730       0.5934        0.7267  10.7310\n",
      "     14        \u001b[36m0.4728\u001b[0m       0.5971        0.6893  10.5529\n",
      "     15        \u001b[36m0.4726\u001b[0m       0.5984        0.6913  10.5198\n",
      "     16        \u001b[36m0.4722\u001b[0m       0.5945        0.7204  10.5656\n",
      "     17        0.4726       0.5957        0.7019  10.6729\n",
      "     18        0.4728       0.5928        0.7112  10.6992\n",
      "     19        0.4729       0.5970        0.6852  10.5415\n",
      "     20        0.4732       0.5899        0.7236  10.6817\n",
      "     21        0.4727       0.5930        0.7133  10.5806\n",
      "     22        0.4729       0.5968        0.7145  10.5691\n",
      "     23        0.4727       0.5943        0.6959  10.4766\n",
      "     24        0.4728       0.5932        0.6960  10.5328\n",
      "     25        0.4722       0.5946        0.6990  10.6634\n",
      "     26        0.4729       0.5943        0.7352  10.7293\n",
      "     27        0.4722       0.5941        0.7015  10.8280\n",
      "     28        0.4726       0.5917        0.7024  10.5692\n",
      "     29        \u001b[36m0.4721\u001b[0m       0.5210        0.7071  10.5598\n",
      "     30        \u001b[36m0.4718\u001b[0m       0.5189        0.7109  10.7038\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5317\u001b[0m       \u001b[32m0.4925\u001b[0m        \u001b[35m2.7575\u001b[0m  10.6414\n",
      "      2        \u001b[36m0.5206\u001b[0m       \u001b[32m0.4935\u001b[0m        4.0074  10.4265\n",
      "      3        \u001b[36m0.5181\u001b[0m       \u001b[32m0.5024\u001b[0m        4.6011  10.6009\n",
      "      4        \u001b[36m0.5168\u001b[0m       0.4795        5.3163  10.7449\n",
      "      5        \u001b[36m0.5144\u001b[0m       0.4751        5.0384  10.7216\n",
      "      6        \u001b[36m0.5141\u001b[0m       0.4798        5.6218  10.6007\n",
      "      7        \u001b[36m0.5130\u001b[0m       0.4791        5.2522  10.6622\n",
      "      8        \u001b[36m0.5120\u001b[0m       0.4751        5.5832  10.6080\n",
      "      9        \u001b[36m0.5119\u001b[0m       0.4945        5.4203  10.7530\n",
      "     10        \u001b[36m0.5107\u001b[0m       0.4902        6.0782  10.6725\n",
      "     11        \u001b[36m0.5107\u001b[0m       0.4842        6.8938  10.4602\n",
      "     12        \u001b[36m0.5095\u001b[0m       0.4796        8.3104  10.7459\n",
      "     13        0.5098       0.4892        8.3641  10.6174\n",
      "     14        \u001b[36m0.5094\u001b[0m       0.4799        6.7447  10.7043\n",
      "     15        \u001b[36m0.5091\u001b[0m       0.4940        9.6719  10.4683\n",
      "     16        \u001b[36m0.5082\u001b[0m       0.4741        7.8525  10.4448\n",
      "     17        \u001b[36m0.5078\u001b[0m       0.4799        8.5463  10.7233\n",
      "     18        0.5087       0.4758        7.2923  10.6657\n",
      "     19        \u001b[36m0.5070\u001b[0m       0.4734        8.4103  10.5943\n",
      "     20        0.5075       0.4704        9.1287  10.7234\n",
      "     21        0.5073       0.4696       12.2130  10.5871\n",
      "     22        0.5082       0.4723        9.5817  10.7844\n",
      "     23        0.5086       0.4841       10.1711  10.5964\n",
      "     24        \u001b[36m0.5064\u001b[0m       0.4794        9.0231  10.4155\n",
      "     25        0.5071       0.4673       11.7946  10.6552\n",
      "     26        0.5067       0.4861        9.8445  10.7095\n",
      "     27        0.5066       0.4710       13.8285  10.6391\n",
      "     28        \u001b[36m0.5060\u001b[0m       0.4647       12.5659  10.5865\n",
      "     29        0.5061       0.4645       12.5891  10.6034\n",
      "     30        0.5063       0.4648       12.6378  10.6628\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5636\u001b[0m       \u001b[32m0.5662\u001b[0m        \u001b[35m0.6827\u001b[0m  10.6156\n",
      "      2        \u001b[36m0.5457\u001b[0m       \u001b[32m0.5768\u001b[0m        0.6840  10.5023\n",
      "      3        \u001b[36m0.5404\u001b[0m       \u001b[32m0.5780\u001b[0m        0.6833  10.6902\n",
      "      4        \u001b[36m0.5367\u001b[0m       0.5669        0.6883  10.6865\n",
      "      5        \u001b[36m0.5341\u001b[0m       0.5612        0.6923  10.5906\n",
      "      6        \u001b[36m0.5330\u001b[0m       0.5705        0.6915  10.5859\n",
      "      7        \u001b[36m0.5307\u001b[0m       0.5598        0.6924  10.6245\n",
      "      8        \u001b[36m0.5299\u001b[0m       0.5694        0.6848  10.7244\n",
      "      9        \u001b[36m0.5288\u001b[0m       0.5555        0.6996  10.8340\n",
      "     10        0.5293       0.5565        0.6937  10.4980\n",
      "     11        \u001b[36m0.5285\u001b[0m       0.5633        0.6972  10.6327\n",
      "     12        \u001b[36m0.5280\u001b[0m       0.5747        0.6976  10.5500\n",
      "     13        \u001b[36m0.5273\u001b[0m       0.5535        0.6915  10.7270\n",
      "     14        \u001b[36m0.5259\u001b[0m       0.5695        0.6834  10.6642\n",
      "     15        0.5265       0.5719        \u001b[35m0.6780\u001b[0m  10.5629\n",
      "     16        0.5267       0.5672        0.6799  10.6303\n",
      "     17        \u001b[36m0.5257\u001b[0m       0.5644        0.6865  10.7942\n",
      "     18        \u001b[36m0.5252\u001b[0m       0.5580        0.6865  10.6435\n",
      "     19        \u001b[36m0.5252\u001b[0m       0.5602        0.6844  10.6388\n",
      "     20        \u001b[36m0.5251\u001b[0m       0.5503        0.6860  10.6812\n",
      "     21        \u001b[36m0.5232\u001b[0m       0.5579        0.6843  10.6991\n",
      "     22        0.5235       0.5458        0.6893  10.8517\n",
      "     23        0.5233       0.5516        0.6819  10.5569\n",
      "     24        \u001b[36m0.5228\u001b[0m       0.5533        0.6868  10.5213\n",
      "     25        \u001b[36m0.5226\u001b[0m       0.5617        \u001b[35m0.6766\u001b[0m  10.7946\n",
      "     26        0.5229       0.5551        0.6779  10.6956\n",
      "     27        \u001b[36m0.5225\u001b[0m       0.5563        0.6785  10.5104\n",
      "     28        \u001b[36m0.5221\u001b[0m       0.5593        0.6778  10.4903\n",
      "     29        \u001b[36m0.5218\u001b[0m       0.5614        \u001b[35m0.6760\u001b[0m  10.5072\n",
      "     30        0.5222       0.5497        0.6853  10.6853\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4748\u001b[0m       \u001b[32m0.5816\u001b[0m        \u001b[35m0.7150\u001b[0m  10.4749\n",
      "      2        \u001b[36m0.4680\u001b[0m       \u001b[32m0.6047\u001b[0m        \u001b[35m0.6755\u001b[0m  10.5169\n",
      "      3        \u001b[36m0.4677\u001b[0m       0.5822        0.7060  10.6419\n",
      "      4        \u001b[36m0.4667\u001b[0m       0.5864        0.6941  10.6926\n",
      "      5        \u001b[36m0.4665\u001b[0m       0.5848        0.7001  10.7494\n",
      "      6        \u001b[36m0.4657\u001b[0m       0.5801        0.6929  10.6015\n",
      "      7        \u001b[36m0.4656\u001b[0m       0.5892        0.6871  10.5506\n",
      "      8        0.4659       0.5876        0.7048  10.6041\n",
      "      9        \u001b[36m0.4654\u001b[0m       0.5905        0.6941  10.6471\n",
      "     10        \u001b[36m0.4653\u001b[0m       0.5949        0.6861  10.6048\n",
      "     11        0.4655       0.5847        0.7000  10.7326\n",
      "     12        \u001b[36m0.4648\u001b[0m       0.5895        0.7067  10.5759\n",
      "     13        \u001b[36m0.4647\u001b[0m       0.5891        0.7028  10.7170\n",
      "     14        0.4651       0.5946        0.7011  10.4675\n",
      "     15        \u001b[36m0.4644\u001b[0m       0.5881        0.7023  10.5007\n",
      "     16        0.4651       0.5800        0.7085  10.4969\n",
      "     17        0.4651       0.5922        0.7162  10.6613\n",
      "     18        0.4646       0.5881        0.6979  10.6203\n",
      "     19        \u001b[36m0.4641\u001b[0m       0.5885        0.7114  10.6889\n",
      "     20        0.4644       0.5861        0.6987  10.6486\n",
      "     21        0.4648       0.5877        0.7093  10.7257\n",
      "     22        \u001b[36m0.4638\u001b[0m       0.5849        0.6981  10.5337\n",
      "     23        0.4642       0.5918        0.6979  10.5337\n",
      "     24        0.4639       0.5880        0.7040  10.5588\n",
      "     25        0.4641       0.5813        0.7134  10.7469\n",
      "     26        \u001b[36m0.4634\u001b[0m       0.5834        0.6995  10.5811\n",
      "     27        0.4640       0.5823        0.7144  10.5858\n",
      "     28        0.4636       0.5892        0.7082  10.5509\n",
      "     29        0.4639       0.5935        0.7063  10.7503\n",
      "     30        0.4639       0.5936        0.7059  10.7545\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5385\u001b[0m       \u001b[32m0.4888\u001b[0m        \u001b[35m2.2973\u001b[0m  10.5894\n",
      "      2        \u001b[36m0.5270\u001b[0m       0.4858        3.0985  10.4675\n",
      "      3        \u001b[36m0.5241\u001b[0m       0.4753        3.3041  10.6897\n",
      "      4        \u001b[36m0.5229\u001b[0m       0.4670        4.0685  10.6775\n",
      "      5        \u001b[36m0.5218\u001b[0m       0.4748        4.0133  10.6430\n",
      "      6        \u001b[36m0.5211\u001b[0m       0.4624        5.2105  10.5855\n",
      "      7        \u001b[36m0.5207\u001b[0m       0.4722        5.1018  10.6720\n",
      "      8        \u001b[36m0.5197\u001b[0m       0.4632        5.8099  10.6852\n",
      "      9        \u001b[36m0.5191\u001b[0m       0.4759        4.9841  10.7049\n",
      "     10        0.5194       0.4699        5.9356  10.6247\n",
      "     11        \u001b[36m0.5190\u001b[0m       0.4686        5.5265  10.4690\n",
      "     12        0.5196       0.4649        5.7516  10.5335\n",
      "     13        \u001b[36m0.5181\u001b[0m       0.4636        5.7409  10.6762\n",
      "     14        \u001b[36m0.5166\u001b[0m       0.4585        5.9830  10.5696\n",
      "     15        \u001b[36m0.5158\u001b[0m       0.4695        6.5014  10.5960\n",
      "     16        0.5159       0.4653        5.9485  10.8282\n",
      "     17        0.5165       0.4665        7.2717  10.6033\n",
      "     18        \u001b[36m0.5143\u001b[0m       0.4596        7.2052  10.6359\n",
      "     19        \u001b[36m0.5141\u001b[0m       0.4558        7.7623  10.5673\n",
      "     20        0.5142       0.4639        8.4415  10.7549\n",
      "     21        0.5153       0.4665        7.1889  10.7604\n",
      "     22        \u001b[36m0.5139\u001b[0m       0.4729        6.7296  10.5971\n",
      "     23        0.5141       0.4608        8.6770  10.4481\n",
      "     24        0.5140       0.4563        8.7165  10.5491\n",
      "     25        \u001b[36m0.5136\u001b[0m       0.4713        7.6560  10.8175\n",
      "     26        \u001b[36m0.5131\u001b[0m       0.4615        8.6126  10.7628\n",
      "     27        0.5135       0.4723        7.5954  10.5995\n",
      "     28        0.5139       0.4574        9.0833  10.6756\n",
      "     29        0.5137       0.4601        9.1409  10.6716\n",
      "     30        \u001b[36m0.5131\u001b[0m       0.4707        9.5389  10.6526\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5611\u001b[0m       \u001b[32m0.5803\u001b[0m        \u001b[35m0.6738\u001b[0m  10.5615\n",
      "      2        \u001b[36m0.5399\u001b[0m       \u001b[32m0.5897\u001b[0m        0.6753  10.5803\n",
      "      3        \u001b[36m0.5331\u001b[0m       0.5477        0.7296  10.7158\n",
      "      4        \u001b[36m0.5306\u001b[0m       0.5293        0.7505  10.7265\n",
      "      5        \u001b[36m0.5286\u001b[0m       0.5266        0.7532  10.5969\n",
      "      6        \u001b[36m0.5272\u001b[0m       0.5235        0.7542  10.7302\n",
      "      7        \u001b[36m0.5256\u001b[0m       0.5299        0.7359  10.5407\n",
      "      8        \u001b[36m0.5246\u001b[0m       0.4860        0.9010  10.6742\n",
      "      9        \u001b[36m0.5236\u001b[0m       0.5012        0.8605  10.6090\n",
      "     10        \u001b[36m0.5234\u001b[0m       0.4789        1.0135  10.6668\n",
      "     11        \u001b[36m0.5228\u001b[0m       0.4797        1.1196  10.5561\n",
      "     12        \u001b[36m0.5216\u001b[0m       0.4812        1.0427  10.8236\n",
      "     13        \u001b[36m0.5211\u001b[0m       0.4905        1.0744  10.7391\n",
      "     14        0.5213       0.4756        0.9629  10.5644\n",
      "     15        \u001b[36m0.5201\u001b[0m       0.4760        1.4638  10.7050\n",
      "     16        \u001b[36m0.5199\u001b[0m       0.4822        0.9984  10.5746\n",
      "     17        \u001b[36m0.5196\u001b[0m       0.4721        1.3226  10.7861\n",
      "     18        \u001b[36m0.5192\u001b[0m       0.4738        1.1525  10.6334\n",
      "     19        0.5197       0.4775        1.5762  10.7316\n",
      "     20        \u001b[36m0.5191\u001b[0m       0.4712        1.4775  10.5963\n",
      "     21        \u001b[36m0.5177\u001b[0m       0.4716        1.7290  10.7508\n",
      "     22        0.5189       0.4705        1.4865  10.6476\n",
      "     23        \u001b[36m0.5173\u001b[0m       0.4781        1.2656  10.6590\n",
      "     24        0.5178       0.4694        2.0680  10.6666\n",
      "     25        \u001b[36m0.5171\u001b[0m       0.4744        1.8001  10.6332\n",
      "     26        \u001b[36m0.5163\u001b[0m       0.4676        2.3258  10.5476\n",
      "     27        0.5166       0.4717        1.8377  10.5656\n",
      "     28        \u001b[36m0.5160\u001b[0m       0.4738        1.6371  10.6734\n",
      "     29        \u001b[36m0.5155\u001b[0m       0.4711        1.7049  10.6249\n",
      "     30        0.5156       0.4703        1.7963  10.7254\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4745\u001b[0m       \u001b[32m0.5081\u001b[0m        \u001b[35m0.7170\u001b[0m  10.4880\n",
      "      2        \u001b[36m0.4685\u001b[0m       \u001b[32m0.5310\u001b[0m        \u001b[35m0.7019\u001b[0m  10.6329\n",
      "      3        \u001b[36m0.4670\u001b[0m       \u001b[32m0.5578\u001b[0m        \u001b[35m0.6986\u001b[0m  10.7185\n",
      "      4        0.4671       0.5327        0.7098  10.6612\n",
      "      5        \u001b[36m0.4663\u001b[0m       0.5417        0.7031  10.5725\n",
      "      6        \u001b[36m0.4658\u001b[0m       0.5527        \u001b[35m0.6982\u001b[0m  10.6454\n",
      "      7        \u001b[36m0.4657\u001b[0m       0.5300        0.7049  10.7180\n",
      "      8        0.4658       0.5319        0.7053  10.5588\n",
      "      9        \u001b[36m0.4657\u001b[0m       0.5159        0.7185  10.4279\n",
      "     10        \u001b[36m0.4654\u001b[0m       0.5421        0.7091  10.6814\n",
      "     11        \u001b[36m0.4652\u001b[0m       0.5297        0.7136  10.4988\n",
      "     12        0.4659       \u001b[32m0.5635\u001b[0m        0.6993  10.7123\n",
      "     13        0.4654       0.5442        0.7011  10.6786\n",
      "     14        0.4655       0.5579        0.6989  10.4766\n",
      "     15        0.4662       \u001b[32m0.6055\u001b[0m        0.7018  10.5363\n",
      "     16        0.4659       0.5567        0.7025  10.6592\n",
      "     17        0.4653       0.5144        0.7202  10.6622\n",
      "     18        0.4654       0.5055        0.7144  10.5684\n",
      "     19        0.4655       0.5361        0.7097  10.5801\n",
      "     20        \u001b[36m0.4652\u001b[0m       0.5663        \u001b[35m0.6874\u001b[0m  10.7350\n",
      "     21        0.4655       0.5293        0.7202  10.6988\n",
      "     22        0.4654       0.4962        0.7226  10.5453\n",
      "     23        0.4657       0.5144        0.7169  10.5976\n",
      "     24        0.4656       0.5292        0.7067  10.4907\n",
      "     25        0.4661       0.5181        0.7195  10.6786\n",
      "     26        \u001b[36m0.4650\u001b[0m       0.4926        0.7258  10.6126\n",
      "     27        0.4652       0.5459        0.7129  10.5719\n",
      "     28        0.4654       0.5143        0.7238  10.4774\n",
      "     29        \u001b[36m0.4647\u001b[0m       0.5447        0.7162  10.6724\n",
      "     30        0.4649       0.5413        0.7104  10.6881\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5323\u001b[0m       \u001b[32m0.4874\u001b[0m        \u001b[35m2.1577\u001b[0m  10.5091\n",
      "      2        \u001b[36m0.5206\u001b[0m       0.4664        2.8487  10.6977\n",
      "      3        \u001b[36m0.5174\u001b[0m       0.4652        3.3212  10.7837\n",
      "      4        \u001b[36m0.5160\u001b[0m       0.4577        2.9963  10.6938\n",
      "      5        \u001b[36m0.5156\u001b[0m       0.4659        3.5805  10.5228\n",
      "      6        \u001b[36m0.5143\u001b[0m       0.4671        3.8952  10.7445\n",
      "      7        \u001b[36m0.5137\u001b[0m       0.4605        3.9953  10.7292\n",
      "      8        \u001b[36m0.5129\u001b[0m       0.4581        5.0500  10.5363\n",
      "      9        \u001b[36m0.5122\u001b[0m       0.4530        5.5271  10.5417\n",
      "     10        \u001b[36m0.5107\u001b[0m       0.4569        5.7064  10.6515\n",
      "     11        0.5111       0.4678        4.4200  10.7081\n",
      "     12        \u001b[36m0.5103\u001b[0m       0.4596        5.2744  10.7269\n",
      "     13        \u001b[36m0.5101\u001b[0m       0.4609        6.2932  10.6518\n",
      "     14        0.5106       0.4581        5.1428  10.6849\n",
      "     15        0.5104       0.4593        6.0850  10.5998\n",
      "     16        \u001b[36m0.5093\u001b[0m       0.4649        6.0488  10.7767\n",
      "     17        \u001b[36m0.5083\u001b[0m       0.4769        5.2423  10.5954\n",
      "     18        0.5084       0.4586        6.1008  10.5729\n",
      "     19        0.5087       0.4549        6.7420  10.6641\n",
      "     20        \u001b[36m0.5082\u001b[0m       0.4612        8.8126  10.6590\n",
      "     21        0.5086       0.4595        7.6629  10.6848\n",
      "     22        \u001b[36m0.5079\u001b[0m       0.4575        8.4911  10.5773\n",
      "     23        \u001b[36m0.5073\u001b[0m       0.4578        9.8250  10.6085\n",
      "     24        0.5078       0.4636        6.7739  10.5193\n",
      "     25        \u001b[36m0.5065\u001b[0m       0.4566       11.2583  10.7261\n",
      "     26        0.5066       0.4565        9.3954  10.6271\n",
      "     27        0.5077       0.4522       13.1390  10.5854\n",
      "     28        0.5077       0.4518       10.3809  10.6783\n",
      "     29        0.5072       0.4584       11.1357  10.7145\n",
      "     30        \u001b[36m0.5063\u001b[0m       0.4648       11.6624  10.6226\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5529\u001b[0m       \u001b[32m0.5488\u001b[0m        \u001b[35m0.6937\u001b[0m  10.6309\n",
      "      2        \u001b[36m0.5344\u001b[0m       \u001b[32m0.5753\u001b[0m        \u001b[35m0.6808\u001b[0m  10.7335\n",
      "      3        \u001b[36m0.5297\u001b[0m       0.5614        0.7005  10.6784\n",
      "      4        \u001b[36m0.5277\u001b[0m       0.5694        \u001b[35m0.6790\u001b[0m  10.8013\n",
      "      5        \u001b[36m0.5265\u001b[0m       0.5551        0.6973  10.4567\n",
      "      6        \u001b[36m0.5244\u001b[0m       0.5648        0.6840  10.6794\n",
      "      7        \u001b[36m0.5228\u001b[0m       0.5560        0.6963  10.7977\n",
      "      8        \u001b[36m0.5226\u001b[0m       0.5614        0.6997  10.5005\n",
      "      9        \u001b[36m0.5221\u001b[0m       0.5636        0.6887  10.5821\n",
      "     10        \u001b[36m0.5207\u001b[0m       0.5611        0.6901  10.6995\n",
      "     11        \u001b[36m0.5200\u001b[0m       0.5568        0.6804  10.8058\n",
      "     12        0.5201       0.5597        0.6894  10.7656\n",
      "     13        \u001b[36m0.5189\u001b[0m       0.5539        0.6871  10.4019\n",
      "     14        \u001b[36m0.5182\u001b[0m       0.5715        0.6811  10.5792\n",
      "     15        0.5187       0.5638        \u001b[35m0.6790\u001b[0m  10.7906\n",
      "     16        \u001b[36m0.5177\u001b[0m       0.5620        \u001b[35m0.6782\u001b[0m  10.7437\n",
      "     17        \u001b[36m0.5168\u001b[0m       0.5631        \u001b[35m0.6743\u001b[0m  10.8362\n",
      "     18        0.5171       0.5595        0.6916  10.8251\n",
      "     19        \u001b[36m0.5160\u001b[0m       0.5667        \u001b[35m0.6737\u001b[0m  10.5717\n",
      "     20        0.5169       0.5577        0.6878  10.6277\n",
      "     21        0.5168       0.5427        0.7074  10.7875\n",
      "     22        \u001b[36m0.5155\u001b[0m       0.5584        0.6746  10.4605\n",
      "     23        0.5161       0.5463        0.6988  10.5937\n",
      "     24        \u001b[36m0.5151\u001b[0m       0.5449        0.6835  10.6699\n",
      "     25        \u001b[36m0.5151\u001b[0m       0.5590        0.6823  10.6899\n",
      "     26        0.5158       0.5570        0.6824  10.5358\n",
      "     27        0.5153       0.5400        0.7056  10.5957\n",
      "     28        \u001b[36m0.5149\u001b[0m       0.5499        0.6898  10.5887\n",
      "     29        \u001b[36m0.5143\u001b[0m       0.5562        0.6791  10.7326\n",
      "     30        0.5151       0.5620        \u001b[35m0.6719\u001b[0m  10.6589\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4728\u001b[0m       \u001b[32m0.5711\u001b[0m        \u001b[35m0.7050\u001b[0m  10.4899\n",
      "      2        \u001b[36m0.4675\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m0.6830\u001b[0m  10.6877\n",
      "      3        \u001b[36m0.4670\u001b[0m       0.5909        0.6995  10.7684\n",
      "      4        \u001b[36m0.4666\u001b[0m       0.5788        0.7111  10.5920\n",
      "      5        \u001b[36m0.4657\u001b[0m       0.5895        0.6889  10.5825\n",
      "      6        0.4657       0.5925        0.7080  10.6723\n",
      "      7        0.4658       0.5839        0.7181  10.7154\n",
      "      8        0.4657       0.5799        0.6970  10.4876\n",
      "      9        \u001b[36m0.4645\u001b[0m       0.5781        0.7014  10.5887\n",
      "     10        0.4654       0.5814        0.7234  10.7321\n",
      "     11        0.4655       0.5885        0.6929  10.7390\n",
      "     12        0.4652       0.5883        0.7138  10.7379\n",
      "     13        0.4647       0.5939        0.6966  10.4980\n",
      "     14        \u001b[36m0.4642\u001b[0m       0.5845        0.7044  10.5635\n",
      "     15        0.4643       0.5811        0.7195  10.7205\n",
      "     16        \u001b[36m0.4639\u001b[0m       0.5863        0.6965  10.5606\n",
      "     17        0.4641       0.5870        0.7062  10.5972\n",
      "     18        0.4643       0.5890        0.6991  10.6351\n",
      "     19        0.4650       0.5909        0.7189  10.6411\n",
      "     20        0.4647       0.5911        0.6973  10.7739\n",
      "     21        0.4644       0.5912        0.7206  10.6667\n",
      "     22        0.4642       0.5951        0.7017  10.5464\n",
      "     23        0.4643       \u001b[32m0.5980\u001b[0m        0.7008  10.6774\n",
      "     24        0.4646       0.5927        0.7025  10.6710\n",
      "     25        0.4652       0.5885        0.7148  10.6945\n",
      "     26        0.4643       0.5895        0.7188  10.6602\n",
      "     27        0.4649       0.5922        0.7011  10.6586\n",
      "     28        0.4642       0.5945        0.7009  10.5685\n",
      "     29        0.4642       0.5919        0.7095  10.7283\n",
      "     30        0.4640       0.5937        0.7090  10.5510\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5476\u001b[0m       \u001b[32m0.4821\u001b[0m        \u001b[35m2.3673\u001b[0m  10.7272\n",
      "      2        \u001b[36m0.5357\u001b[0m       0.4554        2.6552  10.9609\n",
      "      3        \u001b[36m0.5315\u001b[0m       0.4476        3.1977  10.7856\n",
      "      4        \u001b[36m0.5302\u001b[0m       0.4454        2.8646  10.7151\n",
      "      5        \u001b[36m0.5277\u001b[0m       0.4464        2.9837  10.6641\n",
      "      6        \u001b[36m0.5258\u001b[0m       0.4407        2.5048  10.8994\n",
      "      7        \u001b[36m0.5252\u001b[0m       0.4449        3.6503  10.7584\n",
      "      8        \u001b[36m0.5233\u001b[0m       0.4436        4.2679  10.6660\n",
      "      9        \u001b[36m0.5225\u001b[0m       0.4464        4.5175  10.8202\n",
      "     10        \u001b[36m0.5205\u001b[0m       0.4451        4.1745  10.8670\n",
      "     11        \u001b[36m0.5197\u001b[0m       0.4438        5.0824  10.9759\n",
      "     12        \u001b[36m0.5194\u001b[0m       0.4431        3.8751  10.7161\n",
      "     13        \u001b[36m0.5187\u001b[0m       0.4490        5.1440  10.7149\n",
      "     14        \u001b[36m0.5185\u001b[0m       0.4407        5.6038  10.7520\n",
      "     15        \u001b[36m0.5177\u001b[0m       0.4397        5.5895  10.8454\n",
      "     16        \u001b[36m0.5173\u001b[0m       0.4457        4.4565  10.9052\n",
      "     17        \u001b[36m0.5170\u001b[0m       0.4529        6.1761  10.7169\n",
      "     18        \u001b[36m0.5169\u001b[0m       0.4434        5.9255  10.7817\n",
      "     19        \u001b[36m0.5154\u001b[0m       0.4416        7.1007  10.8305\n",
      "     20        \u001b[36m0.5149\u001b[0m       0.4409        5.5150  10.9048\n",
      "     21        \u001b[36m0.5140\u001b[0m       0.4425        6.1748  10.6658\n",
      "     22        0.5148       0.4438        6.4770  10.7981\n",
      "     23        0.5148       0.4415        7.1895  10.7595\n",
      "     24        0.5147       0.4416        6.6094  10.8911\n",
      "     25        0.5144       0.4421        6.5755  10.8138\n",
      "     26        0.5143       0.4430        6.6762  10.8224\n",
      "     27        \u001b[36m0.5123\u001b[0m       0.4419        5.7463  10.9645\n",
      "     28        0.5129       0.4404        6.9031  10.8075\n",
      "     29        0.5132       0.4468        6.4468  10.7875\n",
      "     30        0.5129       0.4383        7.0731  10.7748\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5414\u001b[0m       \u001b[32m0.5115\u001b[0m        \u001b[35m0.9204\u001b[0m  11.0496\n",
      "      2        \u001b[36m0.5201\u001b[0m       0.4958        1.2314  10.8280\n",
      "      3        \u001b[36m0.5130\u001b[0m       0.4994        1.2448  10.8018\n",
      "      4        \u001b[36m0.5080\u001b[0m       0.4940        1.4423  10.7483\n",
      "      5        \u001b[36m0.5055\u001b[0m       0.4917        1.9544  10.7555\n",
      "      6        \u001b[36m0.5030\u001b[0m       0.4820        2.0203  11.0170\n",
      "      7        \u001b[36m0.5001\u001b[0m       0.4836        1.6286  10.7593\n",
      "      8        \u001b[36m0.4980\u001b[0m       0.4785        2.3309  10.7465\n",
      "      9        \u001b[36m0.4969\u001b[0m       0.4809        2.4457  10.7569\n",
      "     10        \u001b[36m0.4955\u001b[0m       0.4769        2.6717  10.9595\n",
      "     11        \u001b[36m0.4944\u001b[0m       0.4813        3.0385  10.7689\n",
      "     12        \u001b[36m0.4941\u001b[0m       0.4815        2.5617  10.6694\n",
      "     13        \u001b[36m0.4919\u001b[0m       0.4801        3.4842  10.9201\n",
      "     14        \u001b[36m0.4919\u001b[0m       0.4841        2.7225  10.9222\n",
      "     15        \u001b[36m0.4890\u001b[0m       0.4795        3.2169  11.0146\n",
      "     16        0.4898       0.4804        3.6564  10.6332\n",
      "     17        0.4893       0.4787        3.0630  10.8212\n",
      "     18        \u001b[36m0.4887\u001b[0m       0.4796        2.9368  10.7806\n",
      "     19        \u001b[36m0.4884\u001b[0m       0.4811        4.4870  10.8636\n",
      "     20        \u001b[36m0.4871\u001b[0m       0.4818        3.1329  10.8028\n",
      "     21        \u001b[36m0.4870\u001b[0m       0.4783        3.2546  10.9123\n",
      "     22        \u001b[36m0.4846\u001b[0m       0.4773        4.5179  10.8962\n",
      "     23        0.4855       0.4785        3.7884  10.7752\n",
      "     24        0.4848       0.4806        3.9057  10.8118\n",
      "     25        0.4849       0.4803        4.6726  10.8613\n",
      "     26        \u001b[36m0.4838\u001b[0m       0.4742        6.6845  10.8360\n",
      "     27        \u001b[36m0.4835\u001b[0m       0.4783        5.1432  10.9780\n",
      "     28        \u001b[36m0.4829\u001b[0m       0.4760        4.6345  10.8155\n",
      "     29        \u001b[36m0.4824\u001b[0m       0.4785        5.2895  10.7464\n",
      "     30        \u001b[36m0.4823\u001b[0m       0.4759        7.0330  10.7790\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4764\u001b[0m       \u001b[32m0.5924\u001b[0m        \u001b[35m0.6870\u001b[0m  10.6698\n",
      "      2        \u001b[36m0.4676\u001b[0m       \u001b[32m0.5940\u001b[0m        \u001b[35m0.6797\u001b[0m  10.7467\n",
      "      3        0.4676       0.5780        0.6959  10.8148\n",
      "      4        \u001b[36m0.4662\u001b[0m       0.5939        \u001b[35m0.6793\u001b[0m  10.7728\n",
      "      5        \u001b[36m0.4653\u001b[0m       0.5826        0.6945  10.9721\n",
      "      6        \u001b[36m0.4644\u001b[0m       0.5606        0.6982  10.7729\n",
      "      7        \u001b[36m0.4637\u001b[0m       0.5693        0.6999  10.7277\n",
      "      8        0.4641       0.5610        0.7045  10.8381\n",
      "      9        0.4637       0.5653        0.7239  10.8232\n",
      "     10        \u001b[36m0.4635\u001b[0m       0.5819        0.6941  10.8904\n",
      "     11        \u001b[36m0.4628\u001b[0m       0.5573        0.7024  10.6176\n",
      "     12        \u001b[36m0.4626\u001b[0m       0.5724        0.6899  10.7405\n",
      "     13        \u001b[36m0.4621\u001b[0m       0.5652        0.7052  10.9471\n",
      "     14        0.4623       0.5749        0.7131  10.7349\n",
      "     15        0.4622       0.5764        0.6924  10.7104\n",
      "     16        \u001b[36m0.4619\u001b[0m       0.5711        0.6998  10.8296\n",
      "     17        \u001b[36m0.4615\u001b[0m       0.5810        0.6997  10.8320\n",
      "     18        0.4615       0.5620        0.7121  10.9740\n",
      "     19        0.4622       0.5647        0.6993  10.7768\n",
      "     20        \u001b[36m0.4615\u001b[0m       0.5612        0.7002  10.4395\n",
      "     21        \u001b[36m0.4599\u001b[0m       0.5737        0.6966  10.8851\n",
      "     22        0.4616       0.5905        \u001b[35m0.6768\u001b[0m  10.8649\n",
      "     23        0.4606       0.5857        0.6912  10.9133\n",
      "     24        \u001b[36m0.4599\u001b[0m       0.5786        0.6900  10.6617\n",
      "     25        \u001b[36m0.4593\u001b[0m       0.5758        0.6899  10.7203\n",
      "     26        0.4594       0.5839        0.7017  10.9657\n",
      "     27        0.4594       0.5627        0.7202  10.7052\n",
      "     28        \u001b[36m0.4591\u001b[0m       0.5721        0.7021  10.7380\n",
      "     29        \u001b[36m0.4588\u001b[0m       0.5720        0.7722  10.6154\n",
      "     30        \u001b[36m0.4584\u001b[0m       0.5779        0.6959  10.8532\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5165\u001b[0m       \u001b[32m0.4682\u001b[0m        \u001b[35m3.2028\u001b[0m  10.7297\n",
      "      2        \u001b[36m0.5017\u001b[0m       0.4541        4.0402  10.7873\n",
      "      3        \u001b[36m0.4973\u001b[0m       0.4443        5.7253  10.7505\n",
      "      4        \u001b[36m0.4957\u001b[0m       0.4487        5.6719  10.8189\n",
      "      5        \u001b[36m0.4928\u001b[0m       0.4486        7.3050  10.9425\n",
      "      6        \u001b[36m0.4921\u001b[0m       0.4480        6.0802  10.6892\n",
      "      7        \u001b[36m0.4905\u001b[0m       0.4483        9.1943  10.8220\n",
      "      8        \u001b[36m0.4890\u001b[0m       0.4493        9.6977  10.8716\n",
      "      9        \u001b[36m0.4878\u001b[0m       0.4524       10.5617  10.8856\n",
      "     10        \u001b[36m0.4873\u001b[0m       0.4479        9.6192  10.6978\n",
      "     11        \u001b[36m0.4855\u001b[0m       0.4434       10.9889  10.6126\n",
      "     12        \u001b[36m0.4853\u001b[0m       0.4428       11.4518  10.9307\n",
      "     13        \u001b[36m0.4839\u001b[0m       0.4456       13.5515  10.8660\n",
      "     14        \u001b[36m0.4822\u001b[0m       0.4543       13.7982  10.9200\n",
      "     15        0.4828       0.4446       12.1626  10.6202\n",
      "     16        \u001b[36m0.4821\u001b[0m       0.4459       13.2442  10.7680\n",
      "     17        \u001b[36m0.4818\u001b[0m       0.4425       12.1976  10.8496\n",
      "     18        0.4820       0.4486       13.5165  10.8200\n",
      "     19        \u001b[36m0.4800\u001b[0m       0.4456       14.9885  10.8285\n",
      "     20        \u001b[36m0.4795\u001b[0m       0.4444       14.2291  10.7684\n",
      "     21        0.4800       0.4453       13.1114  10.9228\n",
      "     22        \u001b[36m0.4793\u001b[0m       0.4402       16.0281  10.7932\n",
      "     23        \u001b[36m0.4784\u001b[0m       0.4453       18.4010  10.7543\n",
      "     24        \u001b[36m0.4783\u001b[0m       0.4475       14.2127  10.6532\n",
      "     25        \u001b[36m0.4766\u001b[0m       0.4478       17.3665  10.9086\n",
      "     26        0.4770       0.4366       18.9243  11.0259\n",
      "     27        0.4769       0.4446       17.8853  10.5502\n",
      "     28        \u001b[36m0.4758\u001b[0m       0.4447       20.0105  10.7427\n",
      "     29        0.4760       0.4457       18.6405  10.7845\n",
      "     30        \u001b[36m0.4744\u001b[0m       0.4446       17.0358  10.9570\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5338\u001b[0m       \u001b[32m0.5055\u001b[0m        \u001b[35m0.7867\u001b[0m  10.8703\n",
      "      2        \u001b[36m0.5111\u001b[0m       0.4906        1.1125  10.9070\n",
      "      3        \u001b[36m0.5037\u001b[0m       0.4916        1.2793  10.8099\n",
      "      4        \u001b[36m0.4987\u001b[0m       0.4952        1.4386  11.0408\n",
      "      5        \u001b[36m0.4943\u001b[0m       0.5010        1.4324  10.7076\n",
      "      6        \u001b[36m0.4924\u001b[0m       0.4823        2.0420  10.7623\n",
      "      7        \u001b[36m0.4886\u001b[0m       0.4758        2.5085  10.8900\n",
      "      8        \u001b[36m0.4878\u001b[0m       0.4868        2.3154  10.7010\n",
      "      9        \u001b[36m0.4867\u001b[0m       0.4821        2.7667  10.8677\n",
      "     10        \u001b[36m0.4855\u001b[0m       0.4750        3.1371  10.6869\n",
      "     11        \u001b[36m0.4836\u001b[0m       0.4753        4.5100  10.8350\n",
      "     12        \u001b[36m0.4826\u001b[0m       0.4777        4.6084  11.0409\n",
      "     13        \u001b[36m0.4817\u001b[0m       0.4777        3.6782  10.7580\n",
      "     14        \u001b[36m0.4805\u001b[0m       0.4851        3.2194  10.6700\n",
      "     15        \u001b[36m0.4804\u001b[0m       0.4781        3.3919  10.9165\n",
      "     16        \u001b[36m0.4795\u001b[0m       0.4833        3.8386  10.8784\n",
      "     17        \u001b[36m0.4778\u001b[0m       0.4786        3.1456  10.9194\n",
      "     18        \u001b[36m0.4777\u001b[0m       0.4779        3.7108  10.6615\n",
      "     19        \u001b[36m0.4774\u001b[0m       0.4767        5.0504  10.7030\n",
      "     20        \u001b[36m0.4761\u001b[0m       0.4794        4.9795  11.0791\n",
      "     21        \u001b[36m0.4761\u001b[0m       0.4806        3.9704  10.9295\n",
      "     22        \u001b[36m0.4756\u001b[0m       0.4804        4.6033  10.8922\n",
      "     23        0.4757       0.4750        6.2014  10.6851\n",
      "     24        \u001b[36m0.4750\u001b[0m       0.4752        4.8445  10.8705\n",
      "     25        \u001b[36m0.4742\u001b[0m       0.4770        6.9774  10.7610\n",
      "     26        \u001b[36m0.4739\u001b[0m       0.4750        7.7104  10.8885\n",
      "     27        \u001b[36m0.4730\u001b[0m       0.4731        8.7555  10.6483\n",
      "     28        0.4731       0.4738        6.2685  10.7570\n",
      "     29        \u001b[36m0.4726\u001b[0m       0.4777        5.8865  11.0552\n",
      "     30        0.4732       0.4755        8.3242  10.7812\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4668\u001b[0m       \u001b[32m0.5937\u001b[0m        \u001b[35m0.6898\u001b[0m  10.7494\n",
      "      2        \u001b[36m0.4591\u001b[0m       0.5922        \u001b[35m0.6783\u001b[0m  10.9881\n",
      "      3        \u001b[36m0.4576\u001b[0m       0.5819        0.6941  10.7273\n",
      "      4        \u001b[36m0.4564\u001b[0m       0.5798        0.6910  10.8495\n",
      "      5        \u001b[36m0.4547\u001b[0m       0.5762        0.7048  10.6967\n",
      "      6        \u001b[36m0.4544\u001b[0m       0.5927        0.6938  10.7502\n",
      "      7        \u001b[36m0.4536\u001b[0m       0.5904        0.6814  10.7976\n",
      "      8        \u001b[36m0.4530\u001b[0m       0.5875        0.6821  10.7880\n",
      "      9        \u001b[36m0.4520\u001b[0m       0.5678        0.6984  10.9028\n",
      "     10        0.4521       0.5705        0.6972  10.5699\n",
      "     11        0.4526       0.5818        0.7106  10.8621\n",
      "     12        \u001b[36m0.4513\u001b[0m       0.5817        0.7002  10.8196\n",
      "     13        \u001b[36m0.4503\u001b[0m       0.5823        0.7157  10.7421\n",
      "     14        0.4506       0.5849        0.6998  10.8684\n",
      "     15        \u001b[36m0.4496\u001b[0m       0.5800        0.7327  10.8341\n",
      "     16        0.4502       0.5743        0.7042  11.0063\n",
      "     17        \u001b[36m0.4492\u001b[0m       0.5802        0.7092  10.7083\n",
      "     18        0.4493       0.5799        0.7433  10.6590\n",
      "     19        \u001b[36m0.4488\u001b[0m       0.5757        0.8249  11.0133\n",
      "     20        0.4491       0.5738        0.7903  10.8430\n",
      "     21        0.4488       0.5791        0.8040  10.8797\n",
      "     22        0.4489       0.5790        0.7692  10.6377\n",
      "     23        \u001b[36m0.4484\u001b[0m       0.5782        0.7678  10.8116\n",
      "     24        \u001b[36m0.4482\u001b[0m       0.5722        0.8523  10.8722\n",
      "     25        \u001b[36m0.4478\u001b[0m       0.5710        0.8491  10.7773\n",
      "     26        \u001b[36m0.4474\u001b[0m       0.5651        0.9865  10.7358\n",
      "     27        \u001b[36m0.4470\u001b[0m       0.5644        0.8812  10.7354\n",
      "     28        0.4476       0.5696        0.8377  10.9746\n",
      "     29        \u001b[36m0.4470\u001b[0m       0.5596        0.8885  10.8159\n",
      "     30        0.4470       0.5655        0.9142  10.8088\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5140\u001b[0m       \u001b[32m0.4677\u001b[0m        \u001b[35m3.5231\u001b[0m  10.9039\n",
      "      2        \u001b[36m0.5008\u001b[0m       0.4582        3.9479  10.8433\n",
      "      3        \u001b[36m0.4980\u001b[0m       0.4606        4.7395  10.8077\n",
      "      4        \u001b[36m0.4961\u001b[0m       0.4490        4.9090  10.8942\n",
      "      5        \u001b[36m0.4934\u001b[0m       0.4618        5.7400  10.7697\n",
      "      6        \u001b[36m0.4917\u001b[0m       0.4546        6.4105  11.0595\n",
      "      7        \u001b[36m0.4912\u001b[0m       0.4622        5.0680  10.8483\n",
      "      8        \u001b[36m0.4895\u001b[0m       0.4663        6.3756  10.7270\n",
      "      9        \u001b[36m0.4874\u001b[0m       0.4575        6.5116  10.7583\n",
      "     10        \u001b[36m0.4873\u001b[0m       0.4558        9.8321  10.8589\n",
      "     11        \u001b[36m0.4864\u001b[0m       0.4551        8.1417  10.9346\n",
      "     12        \u001b[36m0.4851\u001b[0m       0.4574       10.4781  10.8631\n",
      "     13        \u001b[36m0.4841\u001b[0m       0.4633       11.3186  10.6711\n",
      "     14        \u001b[36m0.4831\u001b[0m       0.4548       11.3427  10.8747\n",
      "     15        \u001b[36m0.4828\u001b[0m       \u001b[32m0.4680\u001b[0m       11.8307  10.8032\n",
      "     16        \u001b[36m0.4821\u001b[0m       0.4644       10.5164  10.7878\n",
      "     17        \u001b[36m0.4818\u001b[0m       0.4610       10.9835  10.6781\n",
      "     18        \u001b[36m0.4801\u001b[0m       0.4651       12.9104  10.7680\n",
      "     19        0.4805       0.4599       13.2415  10.8279\n",
      "     20        \u001b[36m0.4796\u001b[0m       0.4635       11.5500  10.8234\n",
      "     21        \u001b[36m0.4789\u001b[0m       \u001b[32m0.4784\u001b[0m       11.2746  10.6564\n",
      "     22        \u001b[36m0.4781\u001b[0m       0.4625       13.5140  10.7110\n",
      "     23        0.4787       0.4551       17.7572  10.9758\n",
      "     24        \u001b[36m0.4780\u001b[0m       0.4620       14.0483  10.8351\n",
      "     25        \u001b[36m0.4775\u001b[0m       0.4645       16.2858  10.7599\n",
      "     26        \u001b[36m0.4772\u001b[0m       0.4633       16.7002  10.7157\n",
      "     27        \u001b[36m0.4771\u001b[0m       0.4624       15.6071  10.8809\n",
      "     28        0.4772       0.4532       15.1317  10.9739\n",
      "     29        \u001b[36m0.4764\u001b[0m       0.4621       17.3450  10.7076\n",
      "     30        \u001b[36m0.4762\u001b[0m       0.4577       19.4456  10.7705\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5290\u001b[0m       \u001b[32m0.5493\u001b[0m        \u001b[35m0.8235\u001b[0m  11.0876\n",
      "      2        \u001b[36m0.5079\u001b[0m       0.5084        0.9866  10.8340\n",
      "      3        \u001b[36m0.5017\u001b[0m       0.5051        1.1250  10.7024\n",
      "      4        \u001b[36m0.4974\u001b[0m       0.4860        1.6750  10.6553\n",
      "      5        \u001b[36m0.4942\u001b[0m       0.4898        1.6441  10.8871\n",
      "      6        \u001b[36m0.4921\u001b[0m       0.4928        1.8889  10.8950\n",
      "      7        \u001b[36m0.4895\u001b[0m       0.4799        2.5363  10.7233\n",
      "      8        \u001b[36m0.4878\u001b[0m       0.4806        2.1968  10.7558\n",
      "      9        \u001b[36m0.4856\u001b[0m       0.4858        2.3762  10.7735\n",
      "     10        \u001b[36m0.4843\u001b[0m       0.4812        3.0062  10.9498\n",
      "     11        \u001b[36m0.4836\u001b[0m       0.4825        3.5144  10.7085\n",
      "     12        \u001b[36m0.4822\u001b[0m       0.4852        2.9108  10.7851\n",
      "     13        \u001b[36m0.4801\u001b[0m       0.4825        3.1298  10.7698\n",
      "     14        \u001b[36m0.4796\u001b[0m       0.4840        3.9887  10.9334\n",
      "     15        0.4800       0.4886        3.6040  10.7747\n",
      "     16        \u001b[36m0.4784\u001b[0m       0.4765        4.4505  10.7230\n",
      "     17        \u001b[36m0.4774\u001b[0m       0.4811        3.7840  10.6590\n",
      "     18        \u001b[36m0.4763\u001b[0m       0.4817        3.4274  10.8706\n",
      "     19        0.4767       0.4804        4.6800  10.8591\n",
      "     20        \u001b[36m0.4757\u001b[0m       0.4777        4.8234  10.7526\n",
      "     21        \u001b[36m0.4753\u001b[0m       0.4781        5.8415  10.7126\n",
      "     22        \u001b[36m0.4742\u001b[0m       0.4788        5.6598  10.7969\n",
      "     23        \u001b[36m0.4733\u001b[0m       0.4791        5.3346  10.9675\n",
      "     24        0.4736       0.4851        5.2279  10.6852\n",
      "     25        \u001b[36m0.4723\u001b[0m       0.4794        6.6542  10.7670\n",
      "     26        0.4726       0.4795        6.3530  10.7459\n",
      "     27        \u001b[36m0.4718\u001b[0m       0.4792        7.1297  10.9088\n",
      "     28        0.4724       0.4816        6.4487  10.7506\n",
      "     29        \u001b[36m0.4713\u001b[0m       0.4760        6.1995  10.6750\n",
      "     30        \u001b[36m0.4703\u001b[0m       0.4811        7.1791  10.9993\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4643\u001b[0m       \u001b[32m0.5757\u001b[0m        \u001b[35m0.7100\u001b[0m  10.9045\n",
      "      2        \u001b[36m0.4586\u001b[0m       0.5755        \u001b[35m0.7027\u001b[0m  10.7396\n",
      "      3        \u001b[36m0.4579\u001b[0m       \u001b[32m0.5777\u001b[0m        \u001b[35m0.6960\u001b[0m  10.7628\n",
      "      4        \u001b[36m0.4563\u001b[0m       \u001b[32m0.5825\u001b[0m        0.7007  10.7415\n",
      "      5        \u001b[36m0.4554\u001b[0m       0.5821        0.7097  10.7832\n",
      "      6        \u001b[36m0.4547\u001b[0m       0.5822        \u001b[35m0.6894\u001b[0m  10.5841\n",
      "      7        \u001b[36m0.4541\u001b[0m       \u001b[32m0.5915\u001b[0m        0.6990  10.7204\n",
      "      8        \u001b[36m0.4540\u001b[0m       \u001b[32m0.5949\u001b[0m        \u001b[35m0.6797\u001b[0m  10.6646\n",
      "      9        \u001b[36m0.4533\u001b[0m       0.5940        0.6888  10.9580\n",
      "     10        \u001b[36m0.4528\u001b[0m       \u001b[32m0.5980\u001b[0m        0.7073  10.7463\n",
      "     11        \u001b[36m0.4526\u001b[0m       0.5864        0.6951  10.7605\n",
      "     12        \u001b[36m0.4522\u001b[0m       0.5931        0.7027  10.6598\n",
      "     13        \u001b[36m0.4522\u001b[0m       0.5936        0.7170  10.8389\n",
      "     14        \u001b[36m0.4515\u001b[0m       0.5942        0.6946  10.9375\n",
      "     15        \u001b[36m0.4515\u001b[0m       0.5888        0.7306  10.6992\n",
      "     16        0.4515       0.5926        0.7357  10.7623\n",
      "     17        0.4518       0.5910        0.7257  10.7566\n",
      "     18        \u001b[36m0.4505\u001b[0m       0.5863        0.7187  10.8515\n",
      "     19        0.4510       0.5828        0.7627  10.7124\n",
      "     20        0.4509       0.5895        0.7731  10.6091\n",
      "     21        \u001b[36m0.4503\u001b[0m       0.5894        0.7910  10.8413\n",
      "     22        \u001b[36m0.4500\u001b[0m       0.5936        0.7989  10.8849\n",
      "     23        \u001b[36m0.4491\u001b[0m       0.5916        0.8409  10.9317\n",
      "     24        0.4497       0.5898        0.7553  10.6444\n",
      "     25        0.4497       0.5801        1.0102  10.7414\n",
      "     26        0.4493       0.5880        0.9558  10.8079\n",
      "     27        0.4496       0.5499        0.7637  10.8679\n",
      "     28        \u001b[36m0.4488\u001b[0m       0.5874        0.9434  10.7656\n",
      "     29        \u001b[36m0.4486\u001b[0m       0.5830        0.9851  10.7077\n",
      "     30        0.4490       0.5866        0.9368  10.9303\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5129\u001b[0m       \u001b[32m0.4640\u001b[0m        \u001b[35m3.4446\u001b[0m  10.8275\n",
      "      2        \u001b[36m0.4995\u001b[0m       0.4494        4.1734  10.7115\n",
      "      3        \u001b[36m0.4969\u001b[0m       0.4528        4.2949  10.8411\n",
      "      4        \u001b[36m0.4941\u001b[0m       0.4508        6.0020  10.8569\n",
      "      5        \u001b[36m0.4931\u001b[0m       0.4592        5.3030  10.8730\n",
      "      6        \u001b[36m0.4913\u001b[0m       0.4536        5.2652  10.7144\n",
      "      7        \u001b[36m0.4890\u001b[0m       0.4447        7.8542  10.7095\n",
      "      8        0.4892       0.4548        5.0716  10.9394\n",
      "      9        \u001b[36m0.4876\u001b[0m       0.4505        7.6547  10.9275\n",
      "     10        \u001b[36m0.4859\u001b[0m       0.4444        9.1315  10.7791\n",
      "     11        0.4863       0.4531        7.0496  10.6848\n",
      "     12        \u001b[36m0.4855\u001b[0m       0.4446        9.0227  10.7977\n",
      "     13        \u001b[36m0.4848\u001b[0m       0.4485        9.0972  10.9585\n",
      "     14        \u001b[36m0.4830\u001b[0m       0.4483       13.7918  10.6678\n",
      "     15        \u001b[36m0.4823\u001b[0m       0.4516        9.3483  10.7290\n",
      "     16        0.4828       0.4509       10.5507  10.9241\n",
      "     17        \u001b[36m0.4819\u001b[0m       0.4481       10.7055  10.8600\n",
      "     18        0.4821       0.4460        8.8395  10.9110\n",
      "     19        \u001b[36m0.4816\u001b[0m       0.4477        9.9811  10.6941\n",
      "     20        \u001b[36m0.4806\u001b[0m       0.4451       18.5151  10.8013\n",
      "     21        \u001b[36m0.4797\u001b[0m       0.4488       11.2557  10.8436\n",
      "     22        \u001b[36m0.4795\u001b[0m       0.4478       13.6505  10.7943\n",
      "     23        0.4798       0.4491       11.6492  10.7646\n",
      "     24        0.4803       0.4486       14.9912  10.6999\n",
      "     25        \u001b[36m0.4792\u001b[0m       0.4480       13.9546  10.9097\n",
      "     26        \u001b[36m0.4783\u001b[0m       0.4465       13.1739  10.8124\n",
      "     27        0.4785       0.4457       16.1960  10.7253\n",
      "     28        \u001b[36m0.4781\u001b[0m       0.4474       13.5546  10.8207\n",
      "     29        \u001b[36m0.4780\u001b[0m       0.4447       19.9754  10.8654\n",
      "     30        \u001b[36m0.4779\u001b[0m       0.4425       21.3656  10.9585\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5291\u001b[0m       \u001b[32m0.5151\u001b[0m        \u001b[35m1.0379\u001b[0m  10.5667\n",
      "      2        \u001b[36m0.5057\u001b[0m       0.5086        1.2208  10.6928\n",
      "      3        \u001b[36m0.4993\u001b[0m       0.4948        1.5894  10.8851\n",
      "      4        \u001b[36m0.4959\u001b[0m       0.5057        1.4211  10.7814\n",
      "      5        \u001b[36m0.4922\u001b[0m       0.4856        1.8736  10.7950\n",
      "      6        \u001b[36m0.4896\u001b[0m       0.4914        2.2116  10.6315\n",
      "      7        \u001b[36m0.4880\u001b[0m       0.4972        1.8782  10.9991\n",
      "      8        \u001b[36m0.4865\u001b[0m       0.4935        2.0842  10.9357\n",
      "      9        \u001b[36m0.4852\u001b[0m       0.4841        2.2334  10.8433\n",
      "     10        \u001b[36m0.4841\u001b[0m       0.4795        2.7221  10.6160\n",
      "     11        \u001b[36m0.4821\u001b[0m       0.4812        3.2627  10.8275\n",
      "     12        \u001b[36m0.4817\u001b[0m       0.4827        3.0641  11.0903\n",
      "     13        \u001b[36m0.4812\u001b[0m       0.4835        2.9598  10.6512\n",
      "     14        \u001b[36m0.4796\u001b[0m       0.4840        3.6488  10.6754\n",
      "     15        \u001b[36m0.4792\u001b[0m       0.4794        3.3446  10.7346\n",
      "     16        \u001b[36m0.4786\u001b[0m       0.4856        3.1156  10.8670\n",
      "     17        \u001b[36m0.4771\u001b[0m       0.4777        4.8873  10.9898\n",
      "     18        \u001b[36m0.4767\u001b[0m       0.4830        4.0698  10.7396\n",
      "     19        \u001b[36m0.4764\u001b[0m       0.4822        4.6998  10.7779\n",
      "     20        \u001b[36m0.4757\u001b[0m       0.4805        4.9473  10.7585\n",
      "     21        0.4761       0.4782        5.8792  10.8891\n",
      "     22        \u001b[36m0.4739\u001b[0m       0.4748        7.2525  10.8508\n",
      "     23        0.4744       0.4761        6.0714  10.7075\n",
      "     24        \u001b[36m0.4734\u001b[0m       0.4778        7.3506  10.9583\n",
      "     25        \u001b[36m0.4729\u001b[0m       0.4821        7.2953  10.8287\n",
      "     26        \u001b[36m0.4728\u001b[0m       0.4791        8.3878  10.7341\n",
      "     27        \u001b[36m0.4726\u001b[0m       0.4768       10.5924  10.8099\n",
      "     28        \u001b[36m0.4711\u001b[0m       0.4776        7.7818  10.7375\n",
      "     29        0.4724       0.4780        8.6202  10.9602\n",
      "     30        \u001b[36m0.4711\u001b[0m       0.4775        9.5724  10.9265\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4637\u001b[0m       \u001b[32m0.5551\u001b[0m        \u001b[35m0.7106\u001b[0m  10.8446\n",
      "      2        \u001b[36m0.4583\u001b[0m       \u001b[32m0.5974\u001b[0m        \u001b[35m0.6965\u001b[0m  10.7917\n",
      "      3        \u001b[36m0.4575\u001b[0m       0.5939        0.6981  10.9174\n",
      "      4        \u001b[36m0.4566\u001b[0m       0.5527        0.7058  10.9069\n",
      "      5        \u001b[36m0.4559\u001b[0m       0.5944        0.7037  10.8117\n",
      "      6        \u001b[36m0.4548\u001b[0m       0.5915        0.7030  10.8926\n",
      "      7        \u001b[36m0.4542\u001b[0m       0.5921        0.7075  10.8724\n",
      "      8        0.4544       0.5934        0.6975  10.9189\n",
      "      9        \u001b[36m0.4540\u001b[0m       0.5630        0.7055  10.6389\n",
      "     10        \u001b[36m0.4535\u001b[0m       0.5973        0.7054  10.7519\n",
      "     11        \u001b[36m0.4529\u001b[0m       0.5926        0.7077  11.0021\n",
      "     12        0.4532       0.5965        0.7038  10.6729\n",
      "     13        \u001b[36m0.4526\u001b[0m       \u001b[32m0.5982\u001b[0m        0.7131  10.7391\n",
      "     14        \u001b[36m0.4517\u001b[0m       0.5935        0.7101  10.7249\n",
      "     15        0.4518       0.5966        0.7035  10.9159\n",
      "     16        0.4524       0.5933        0.7145  11.1131\n",
      "     17        \u001b[36m0.4516\u001b[0m       \u001b[32m0.5984\u001b[0m        0.7179  10.6509\n",
      "     18        \u001b[36m0.4513\u001b[0m       0.5929        0.7216  10.8000\n",
      "     19        \u001b[36m0.4513\u001b[0m       0.5565        0.7246  10.8277\n",
      "     20        0.4517       0.5897        0.7369  10.9445\n",
      "     21        0.4514       0.5946        0.7379  10.6734\n",
      "     22        \u001b[36m0.4506\u001b[0m       \u001b[32m0.5986\u001b[0m        0.7092  10.7994\n",
      "     23        \u001b[36m0.4505\u001b[0m       0.5946        0.7469  10.7481\n",
      "     24        0.4513       0.5966        0.7225  10.8714\n",
      "     25        0.4508       0.5978        0.7312  10.8693\n",
      "     26        0.4511       0.5371        0.7273  10.7015\n",
      "     27        \u001b[36m0.4503\u001b[0m       \u001b[32m0.5993\u001b[0m        0.7521  10.8875\n",
      "     28        0.4506       0.5946        0.7696  10.8166\n",
      "     29        \u001b[36m0.4500\u001b[0m       0.5959        0.7913  10.8117\n",
      "     30        0.4510       0.5929        0.8262  10.7451\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5248\u001b[0m       \u001b[32m0.4596\u001b[0m        \u001b[35m3.2112\u001b[0m  10.9021\n",
      "      2        \u001b[36m0.5099\u001b[0m       0.4529        4.8495  11.0013\n",
      "      3        \u001b[36m0.5063\u001b[0m       0.4475        6.0323  10.9144\n",
      "      4        \u001b[36m0.5037\u001b[0m       0.4542        5.6390  10.7507\n",
      "      5        \u001b[36m0.5013\u001b[0m       0.4429        7.7630  10.7827\n",
      "      6        \u001b[36m0.5000\u001b[0m       0.4467        6.7522  10.9919\n",
      "      7        \u001b[36m0.4984\u001b[0m       0.4483        6.7351  10.8498\n",
      "      8        \u001b[36m0.4965\u001b[0m       0.4449        8.7921  10.8305\n",
      "      9        \u001b[36m0.4955\u001b[0m       0.4415        9.6119  10.7300\n",
      "     10        \u001b[36m0.4938\u001b[0m       0.4467       10.3778  11.0529\n",
      "     11        \u001b[36m0.4930\u001b[0m       0.4454        8.8040  10.8756\n",
      "     12        \u001b[36m0.4917\u001b[0m       0.4433       10.0841  10.8609\n",
      "     13        \u001b[36m0.4905\u001b[0m       0.4451       10.6549  10.8223\n",
      "     14        0.4905       0.4452       10.0122  10.9575\n",
      "     15        \u001b[36m0.4896\u001b[0m       0.4465       12.1106  10.9456\n",
      "     16        \u001b[36m0.4887\u001b[0m       0.4422       11.6833  10.7934\n",
      "     17        0.4888       0.4469       13.3632  10.9436\n",
      "     18        \u001b[36m0.4885\u001b[0m       0.4456       13.2279  11.1770\n",
      "     19        \u001b[36m0.4872\u001b[0m       0.4477       14.2725  10.9924\n",
      "     20        \u001b[36m0.4869\u001b[0m       0.4438       13.5617  10.9400\n",
      "     21        \u001b[36m0.4866\u001b[0m       0.4477       14.3171  10.7846\n",
      "     22        \u001b[36m0.4857\u001b[0m       0.4407       12.5152  10.8408\n",
      "     23        \u001b[36m0.4851\u001b[0m       0.4404       16.7593  10.9902\n",
      "     24        0.4853       0.4426       13.7181  10.7636\n",
      "     25        \u001b[36m0.4844\u001b[0m       0.4434       15.6581  10.7995\n",
      "     26        \u001b[36m0.4839\u001b[0m       0.4430       16.1985  10.7872\n",
      "     27        \u001b[36m0.4834\u001b[0m       0.4425       13.8504  11.0152\n",
      "     28        \u001b[36m0.4826\u001b[0m       0.4398       16.7632  11.0292\n",
      "     29        0.4834       0.4459       13.4507  10.6865\n",
      "     30        \u001b[36m0.4818\u001b[0m       0.4437       14.4785  10.8874\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5393\u001b[0m       \u001b[32m0.4942\u001b[0m        \u001b[35m1.1359\u001b[0m  10.8174\n",
      "      2        \u001b[36m0.5142\u001b[0m       0.4872        1.3923  10.8566\n",
      "      3        \u001b[36m0.5084\u001b[0m       0.4853        1.7711  10.8381\n",
      "      4        \u001b[36m0.5021\u001b[0m       0.4813        2.1003  10.7832\n",
      "      5        \u001b[36m0.4983\u001b[0m       0.4785        2.3711  11.0638\n",
      "      6        \u001b[36m0.4957\u001b[0m       0.4765        2.5709  10.8421\n",
      "      7        \u001b[36m0.4930\u001b[0m       0.4820        2.5130  10.7838\n",
      "      8        \u001b[36m0.4906\u001b[0m       0.4821        2.8285  10.7189\n",
      "      9        \u001b[36m0.4886\u001b[0m       0.4821        2.8904  11.0530\n",
      "     10        \u001b[36m0.4877\u001b[0m       0.4807        3.3909  10.9143\n",
      "     11        \u001b[36m0.4851\u001b[0m       0.4786        2.8570  10.9101\n",
      "     12        \u001b[36m0.4845\u001b[0m       0.4809        3.4580  10.7364\n",
      "     13        \u001b[36m0.4822\u001b[0m       0.4768        2.9427  10.9752\n",
      "     14        \u001b[36m0.4820\u001b[0m       0.4787        3.7528  11.0220\n",
      "     15        \u001b[36m0.4801\u001b[0m       0.4756        4.4198  10.7162\n",
      "     16        0.4804       0.4762        4.3096  10.8368\n",
      "     17        \u001b[36m0.4788\u001b[0m       0.4779        4.3274  10.9137\n",
      "     18        \u001b[36m0.4778\u001b[0m       0.4737        5.4616  10.9967\n",
      "     19        0.4778       0.4767        3.6627  11.0046\n",
      "     20        \u001b[36m0.4765\u001b[0m       0.4756        4.2218  10.7530\n",
      "     21        \u001b[36m0.4758\u001b[0m       0.4778        3.6985  10.9115\n",
      "     22        \u001b[36m0.4751\u001b[0m       0.4734        6.4965  10.9098\n",
      "     23        \u001b[36m0.4740\u001b[0m       0.4735        6.7607  10.8214\n",
      "     24        0.4743       0.4737        6.8019  10.8301\n",
      "     25        \u001b[36m0.4729\u001b[0m       0.4713        7.8990  10.8118\n",
      "     26        0.4735       0.4741        5.5669  11.0078\n",
      "     27        \u001b[36m0.4728\u001b[0m       0.4731        6.7719  10.9697\n",
      "     28        \u001b[36m0.4712\u001b[0m       0.4729        6.0640  10.8648\n",
      "     29        0.4712       0.4728        5.4239  10.7621\n",
      "     30        \u001b[36m0.4705\u001b[0m       0.4751        5.6108  10.9815\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4900\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m0.6834\u001b[0m  10.7999\n",
      "      2        \u001b[36m0.4825\u001b[0m       0.5847        0.6991  10.7516\n",
      "      3        \u001b[36m0.4806\u001b[0m       0.5959        \u001b[35m0.6779\u001b[0m  10.7525\n",
      "      4        \u001b[36m0.4786\u001b[0m       0.5945        0.6789  11.0114\n",
      "      5        \u001b[36m0.4777\u001b[0m       0.5967        0.6789  10.9503\n",
      "      6        \u001b[36m0.4774\u001b[0m       0.5851        0.6905  10.8055\n",
      "      7        \u001b[36m0.4759\u001b[0m       0.5813        0.6961  10.7367\n",
      "      8        \u001b[36m0.4748\u001b[0m       0.5840        0.6933  10.9748\n",
      "      9        \u001b[36m0.4738\u001b[0m       0.5851        0.6969  10.9173\n",
      "     10        \u001b[36m0.4733\u001b[0m       0.5797        0.7003  10.9285\n",
      "     11        \u001b[36m0.4728\u001b[0m       0.5723        0.6940  10.8589\n",
      "     12        \u001b[36m0.4726\u001b[0m       0.5712        0.7134  10.7658\n",
      "     13        \u001b[36m0.4714\u001b[0m       0.5688        0.7426  10.9608\n",
      "     14        \u001b[36m0.4708\u001b[0m       0.5650        0.7287  10.8064\n",
      "     15        \u001b[36m0.4699\u001b[0m       0.5707        0.7445  10.8535\n",
      "     16        \u001b[36m0.4691\u001b[0m       0.5657        0.7632  10.7741\n",
      "     17        0.4692       0.5642        0.7573  11.0836\n",
      "     18        \u001b[36m0.4676\u001b[0m       0.5738        0.7482  10.7692\n",
      "     19        \u001b[36m0.4676\u001b[0m       0.5742        0.7914  10.8521\n",
      "     20        \u001b[36m0.4670\u001b[0m       0.5610        0.9100  10.8133\n",
      "     21        \u001b[36m0.4665\u001b[0m       0.5706        0.8424  11.0519\n",
      "     22        \u001b[36m0.4658\u001b[0m       0.5592        0.9023  10.7408\n",
      "     23        \u001b[36m0.4652\u001b[0m       0.5565        0.8748  10.7585\n",
      "     24        \u001b[36m0.4647\u001b[0m       0.5537        1.1190  10.8582\n",
      "     25        \u001b[36m0.4643\u001b[0m       0.5462        1.1595  10.9591\n",
      "     26        0.4645       0.5449        1.2403  10.9319\n",
      "     27        0.4645       0.5441        1.1450  10.8816\n",
      "     28        \u001b[36m0.4629\u001b[0m       0.5487        1.2113  10.7859\n",
      "     29        0.4631       0.5356        1.4501  10.9883\n",
      "     30        \u001b[36m0.4626\u001b[0m       0.5358        1.5747  10.9619\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5111\u001b[0m       \u001b[32m0.4604\u001b[0m        \u001b[35m3.8216\u001b[0m  11.0047\n",
      "      2        \u001b[36m0.4981\u001b[0m       0.4459        5.1373  10.7707\n",
      "      3        \u001b[36m0.4945\u001b[0m       0.4446        5.7127  10.9960\n",
      "      4        \u001b[36m0.4912\u001b[0m       0.4463        7.2364  10.9026\n",
      "      5        \u001b[36m0.4889\u001b[0m       0.4459        7.1549  10.8146\n",
      "      6        \u001b[36m0.4872\u001b[0m       0.4490        6.9293  10.7831\n",
      "      7        \u001b[36m0.4853\u001b[0m       0.4445        7.6352  11.0970\n",
      "      8        \u001b[36m0.4837\u001b[0m       0.4485        9.8793  10.9920\n",
      "      9        \u001b[36m0.4826\u001b[0m       0.4402        9.7610  10.9982\n",
      "     10        \u001b[36m0.4809\u001b[0m       0.4473       10.3969  10.7685\n",
      "     11        \u001b[36m0.4792\u001b[0m       0.4485        8.1002  11.2185\n",
      "     12        \u001b[36m0.4776\u001b[0m       0.4458       13.4909  11.0758\n",
      "     13        0.4778       0.4446       12.5577  11.0888\n",
      "     14        \u001b[36m0.4760\u001b[0m       0.4492       11.3349  10.7625\n",
      "     15        \u001b[36m0.4746\u001b[0m       0.4434       14.0252  10.7612\n",
      "     16        \u001b[36m0.4744\u001b[0m       0.4448       13.4169  10.9676\n",
      "     17        \u001b[36m0.4736\u001b[0m       0.4388       13.0801  10.8029\n",
      "     18        \u001b[36m0.4728\u001b[0m       0.4447       15.5823  10.8122\n",
      "     19        \u001b[36m0.4710\u001b[0m       0.4418       13.1319  10.7659\n",
      "     20        \u001b[36m0.4710\u001b[0m       0.4442       11.5264  11.0471\n",
      "     21        \u001b[36m0.4700\u001b[0m       0.4455       15.9488  10.9098\n",
      "     22        \u001b[36m0.4700\u001b[0m       0.4421       17.5355  10.7340\n",
      "     23        \u001b[36m0.4693\u001b[0m       0.4394       17.7163  10.7338\n",
      "     24        \u001b[36m0.4685\u001b[0m       0.4398       21.1833  11.0660\n",
      "     25        \u001b[36m0.4679\u001b[0m       0.4401       19.5520  10.8800\n",
      "     26        0.4681       0.4423       16.3630  10.9395\n",
      "     27        \u001b[36m0.4671\u001b[0m       0.4459       18.0089  10.7661\n",
      "     28        \u001b[36m0.4670\u001b[0m       0.4439       19.9575  10.9708\n",
      "     29        \u001b[36m0.4659\u001b[0m       0.4406       20.7613  10.8833\n",
      "     30        0.4662       0.4436       20.1087  10.9870\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5294\u001b[0m       \u001b[32m0.5109\u001b[0m        \u001b[35m1.1129\u001b[0m  10.7796\n",
      "      2        \u001b[36m0.5053\u001b[0m       0.4919        1.5806  10.9076\n",
      "      3        \u001b[36m0.4978\u001b[0m       0.4940        1.3032  11.0308\n",
      "      4        \u001b[36m0.4930\u001b[0m       0.4838        2.3915  10.7553\n",
      "      5        \u001b[36m0.4883\u001b[0m       0.4880        1.8592  10.8675\n",
      "      6        \u001b[36m0.4860\u001b[0m       0.4857        2.2162  10.8996\n",
      "      7        \u001b[36m0.4832\u001b[0m       0.4832        2.4997  10.9361\n",
      "      8        \u001b[36m0.4806\u001b[0m       0.4863        2.9361  10.8533\n",
      "      9        \u001b[36m0.4792\u001b[0m       0.4801        2.9333  10.7547\n",
      "     10        \u001b[36m0.4776\u001b[0m       0.4841        2.9481  10.9092\n",
      "     11        \u001b[36m0.4756\u001b[0m       0.4841        3.2380  11.0194\n",
      "     12        \u001b[36m0.4741\u001b[0m       0.4775        3.9388  10.8658\n",
      "     13        \u001b[36m0.4740\u001b[0m       0.4784        3.6339  10.7066\n",
      "     14        \u001b[36m0.4718\u001b[0m       0.4805        3.7616  10.8686\n",
      "     15        \u001b[36m0.4706\u001b[0m       0.4766        5.2355  10.9090\n",
      "     16        \u001b[36m0.4685\u001b[0m       0.4746        5.1720  10.9275\n",
      "     17        0.4693       0.4760        5.1786  10.7336\n",
      "     18        \u001b[36m0.4685\u001b[0m       0.4799        4.1330  10.9324\n",
      "     19        \u001b[36m0.4667\u001b[0m       0.4751        5.5298  10.8549\n",
      "     20        \u001b[36m0.4666\u001b[0m       0.4742        6.1747  11.0451\n",
      "     21        \u001b[36m0.4650\u001b[0m       0.4791        5.8744  10.7433\n",
      "     22        0.4652       0.4738        7.6951  10.9483\n",
      "     23        \u001b[36m0.4643\u001b[0m       0.4757        5.5892  10.9482\n",
      "     24        \u001b[36m0.4632\u001b[0m       0.4754        5.9585  10.9327\n",
      "     25        \u001b[36m0.4629\u001b[0m       0.4791        5.8276  10.8621\n",
      "     26        \u001b[36m0.4623\u001b[0m       0.4743        8.6260  10.7113\n",
      "     27        \u001b[36m0.4606\u001b[0m       0.4758        7.5972  10.9701\n",
      "     28        \u001b[36m0.4605\u001b[0m       0.4748        6.8793  10.9890\n",
      "     29        \u001b[36m0.4600\u001b[0m       0.4743        8.2606  10.8969\n",
      "     30        \u001b[36m0.4594\u001b[0m       0.4759        8.9252  10.8486\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4640\u001b[0m       \u001b[32m0.5801\u001b[0m        \u001b[35m0.6967\u001b[0m  11.0590\n",
      "      2        \u001b[36m0.4580\u001b[0m       0.5634        0.7109  10.7403\n",
      "      3        \u001b[36m0.4560\u001b[0m       \u001b[32m0.5882\u001b[0m        \u001b[35m0.6877\u001b[0m  10.8487\n",
      "      4        \u001b[36m0.4550\u001b[0m       0.5704        0.7089  10.7519\n",
      "      5        \u001b[36m0.4541\u001b[0m       \u001b[32m0.5936\u001b[0m        0.6919  10.9104\n",
      "      6        \u001b[36m0.4531\u001b[0m       0.5761        0.7180  10.8781\n",
      "      7        \u001b[36m0.4529\u001b[0m       0.5705        0.6974  10.8939\n",
      "      8        \u001b[36m0.4514\u001b[0m       0.5901        0.6976  10.6966\n",
      "      9        \u001b[36m0.4501\u001b[0m       0.5854        0.6967  10.9025\n",
      "     10        \u001b[36m0.4500\u001b[0m       0.5831        0.7022  10.9827\n",
      "     11        \u001b[36m0.4495\u001b[0m       0.5871        0.7085  10.8464\n",
      "     12        \u001b[36m0.4485\u001b[0m       0.5854        0.7016  10.8261\n",
      "     13        0.4485       0.5841        0.7120  10.7027\n",
      "     14        \u001b[36m0.4481\u001b[0m       0.5889        0.7183  10.9373\n",
      "     15        \u001b[36m0.4475\u001b[0m       0.5898        0.7190  10.8875\n",
      "     16        \u001b[36m0.4469\u001b[0m       0.5916        0.7472  10.7128\n",
      "     17        \u001b[36m0.4466\u001b[0m       \u001b[32m0.5937\u001b[0m        0.7241  10.6941\n",
      "     18        \u001b[36m0.4463\u001b[0m       0.5829        0.7332  11.0486\n",
      "     19        \u001b[36m0.4459\u001b[0m       0.5908        0.7916  10.8909\n",
      "     20        \u001b[36m0.4455\u001b[0m       0.5838        0.8326  10.8847\n",
      "     21        0.4456       0.5755        0.8857  10.7600\n",
      "     22        \u001b[36m0.4454\u001b[0m       0.5803        0.8678  10.8139\n",
      "     23        \u001b[36m0.4435\u001b[0m       0.5577        0.8126  11.0196\n",
      "     24        0.4441       0.5836        0.9168  10.9099\n",
      "     25        0.4442       0.5776        0.9505  10.7276\n",
      "     26        \u001b[36m0.4429\u001b[0m       0.5818        0.9856  11.0330\n",
      "     27        0.4434       0.5814        0.9488  10.9538\n",
      "     28        0.4437       0.5806        1.0461  10.8767\n",
      "     29        \u001b[36m0.4428\u001b[0m       0.5818        1.0061  10.7969\n",
      "     30        \u001b[36m0.4422\u001b[0m       0.5802        1.2087  10.8909\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5095\u001b[0m       \u001b[32m0.4515\u001b[0m        \u001b[35m3.6431\u001b[0m  10.8962\n",
      "      2        \u001b[36m0.4957\u001b[0m       0.4462        5.0582  10.9427\n",
      "      3        \u001b[36m0.4919\u001b[0m       0.4492        5.0131  10.8191\n",
      "      4        \u001b[36m0.4889\u001b[0m       0.4423        6.9240  11.0145\n",
      "      5        \u001b[36m0.4867\u001b[0m       0.4457        5.0655  10.9518\n",
      "      6        \u001b[36m0.4848\u001b[0m       0.4466        7.8411  10.9203\n",
      "      7        \u001b[36m0.4827\u001b[0m       0.4437        8.4158  10.7231\n",
      "      8        \u001b[36m0.4822\u001b[0m       0.4431       10.9582  10.9077\n",
      "      9        \u001b[36m0.4803\u001b[0m       0.4412        9.7823  10.8984\n",
      "     10        \u001b[36m0.4789\u001b[0m       0.4509        9.4585  10.9945\n",
      "     11        \u001b[36m0.4778\u001b[0m       0.4403       11.6478  10.6991\n",
      "     12        \u001b[36m0.4771\u001b[0m       0.4407        8.9816  10.8969\n",
      "     13        \u001b[36m0.4755\u001b[0m       0.4394       11.9467  10.9576\n",
      "     14        \u001b[36m0.4739\u001b[0m       0.4397        8.9227  11.0675\n",
      "     15        0.4739       0.4434       15.3058  10.7991\n",
      "     16        \u001b[36m0.4720\u001b[0m       0.4409       13.7664  10.9051\n",
      "     17        0.4720       0.4408       17.3519  10.9833\n",
      "     18        \u001b[36m0.4714\u001b[0m       0.4478       16.5001  10.8140\n",
      "     19        \u001b[36m0.4713\u001b[0m       0.4439       20.4924  10.8538\n",
      "     20        \u001b[36m0.4698\u001b[0m       0.4464       16.3115  10.7164\n",
      "     21        \u001b[36m0.4690\u001b[0m       0.4399       13.4065  10.9570\n",
      "     22        \u001b[36m0.4686\u001b[0m       0.4412       16.5160  10.9352\n",
      "     23        \u001b[36m0.4684\u001b[0m       0.4418       14.5990  10.9399\n",
      "     24        \u001b[36m0.4683\u001b[0m       0.4434       18.4124  10.7820\n",
      "     25        \u001b[36m0.4675\u001b[0m       0.4359       18.2237  10.9718\n",
      "     26        \u001b[36m0.4672\u001b[0m       0.4401       17.1133  10.9394\n",
      "     27        \u001b[36m0.4672\u001b[0m       0.4427       23.4108  11.0096\n",
      "     28        \u001b[36m0.4663\u001b[0m       0.4428       18.4126  10.7490\n",
      "     29        \u001b[36m0.4653\u001b[0m       0.4441       27.8783  10.9167\n",
      "     30        0.4657       0.4461       19.9149  10.8987\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5240\u001b[0m       \u001b[32m0.5036\u001b[0m        \u001b[35m1.2965\u001b[0m  10.7825\n",
      "      2        \u001b[36m0.5028\u001b[0m       0.5031        \u001b[35m1.2857\u001b[0m  10.7477\n",
      "      3        \u001b[36m0.4955\u001b[0m       0.4953        1.6718  10.9178\n",
      "      4        \u001b[36m0.4908\u001b[0m       0.4980        1.5436  10.8701\n",
      "      5        \u001b[36m0.4879\u001b[0m       0.4953        1.8304  10.9092\n",
      "      6        \u001b[36m0.4838\u001b[0m       0.4971        2.3308  10.6798\n",
      "      7        \u001b[36m0.4823\u001b[0m       0.4960        2.4029  10.8304\n",
      "      8        \u001b[36m0.4798\u001b[0m       0.4892        2.8951  10.8508\n",
      "      9        \u001b[36m0.4780\u001b[0m       0.4844        3.5830  10.9794\n",
      "     10        \u001b[36m0.4768\u001b[0m       0.4862        3.0165  10.7753\n",
      "     11        \u001b[36m0.4742\u001b[0m       0.4921        3.6804  10.8708\n",
      "     12        \u001b[36m0.4731\u001b[0m       0.4826        3.6566  10.9183\n",
      "     13        \u001b[36m0.4717\u001b[0m       0.4843        2.8797  10.9856\n",
      "     14        \u001b[36m0.4708\u001b[0m       0.4794        3.7560  10.7395\n",
      "     15        \u001b[36m0.4705\u001b[0m       0.4811        5.1139  10.8858\n",
      "     16        \u001b[36m0.4685\u001b[0m       0.4808        5.2156  10.8603\n",
      "     17        0.4686       0.4823        5.2833  10.9080\n",
      "     18        \u001b[36m0.4676\u001b[0m       0.4787        5.4166  10.7359\n",
      "     19        \u001b[36m0.4671\u001b[0m       0.4781        6.7361  10.8624\n",
      "     20        \u001b[36m0.4661\u001b[0m       0.4821        5.4590  10.7668\n",
      "     21        \u001b[36m0.4659\u001b[0m       0.4767        6.2257  10.9023\n",
      "     22        \u001b[36m0.4648\u001b[0m       0.4765        5.5346  10.8188\n",
      "     23        \u001b[36m0.4637\u001b[0m       0.4778        7.3691  10.8320\n",
      "     24        0.4638       0.4773        7.8595  10.7640\n",
      "     25        \u001b[36m0.4627\u001b[0m       0.4770        7.1315  10.9597\n",
      "     26        0.4627       0.4811        7.4815  10.8705\n",
      "     27        \u001b[36m0.4620\u001b[0m       0.4780        7.0588  10.8678\n",
      "     28        \u001b[36m0.4614\u001b[0m       0.4806        9.7569  10.7298\n",
      "     29        \u001b[36m0.4613\u001b[0m       0.4790        9.1465  11.0980\n",
      "     30        \u001b[36m0.4601\u001b[0m       0.4790       10.5831  10.9134\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4632\u001b[0m       \u001b[32m0.5851\u001b[0m        \u001b[35m0.6856\u001b[0m  10.7966\n",
      "      2        \u001b[36m0.4578\u001b[0m       0.5775        0.6993  10.9912\n",
      "      3        \u001b[36m0.4566\u001b[0m       0.5747        0.7062  10.9916\n",
      "      4        \u001b[36m0.4552\u001b[0m       0.5655        0.6950  10.9668\n",
      "      5        \u001b[36m0.4537\u001b[0m       0.5779        0.6922  10.9191\n",
      "      6        \u001b[36m0.4528\u001b[0m       0.5690        0.7032  10.7387\n",
      "      7        0.4532       0.5645        0.7069  11.0587\n",
      "      8        \u001b[36m0.4526\u001b[0m       0.5818        0.6995  10.9434\n",
      "      9        \u001b[36m0.4512\u001b[0m       0.5842        0.6976  10.8142\n",
      "     10        \u001b[36m0.4509\u001b[0m       0.5784        0.7008  10.7650\n",
      "     11        \u001b[36m0.4500\u001b[0m       \u001b[32m0.5886\u001b[0m        0.6989  11.0629\n",
      "     12        \u001b[36m0.4499\u001b[0m       0.5865        0.7027  10.9199\n",
      "     13        \u001b[36m0.4493\u001b[0m       0.5850        0.7122  10.8666\n",
      "     14        \u001b[36m0.4491\u001b[0m       0.5828        0.7056  10.8403\n",
      "     15        \u001b[36m0.4486\u001b[0m       0.5762        0.7089  10.8074\n",
      "     16        0.4486       0.5493        0.7078  10.9333\n",
      "     17        \u001b[36m0.4483\u001b[0m       0.5786        0.7090  10.7859\n",
      "     18        \u001b[36m0.4476\u001b[0m       0.5840        0.7320  10.9215\n",
      "     19        0.4478       0.5865        0.7571  10.7911\n",
      "     20        0.4479       0.5753        0.7488  11.1334\n",
      "     21        \u001b[36m0.4474\u001b[0m       0.5799        0.7420  10.8472\n",
      "     22        \u001b[36m0.4461\u001b[0m       0.5857        0.7464  10.8543\n",
      "     23        \u001b[36m0.4455\u001b[0m       0.5862        0.7995  10.8030\n",
      "     24        0.4465       0.5871        0.7466  10.8544\n",
      "     25        0.4461       0.5855        0.8737  10.8324\n",
      "     26        \u001b[36m0.4450\u001b[0m       0.5857        0.7931  10.7644\n",
      "     27        0.4456       0.5824        0.9399  10.9022\n",
      "     28        0.4457       0.5818        1.0002  11.0114\n",
      "     29        \u001b[36m0.4446\u001b[0m       0.5769        1.0173  10.8710\n",
      "     30        0.4453       0.5816        0.9912  10.8363\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5089\u001b[0m       \u001b[32m0.4522\u001b[0m        \u001b[35m2.8131\u001b[0m  11.0541\n",
      "      2        \u001b[36m0.4970\u001b[0m       0.4497        3.5369  10.9109\n",
      "      3        \u001b[36m0.4936\u001b[0m       0.4416        3.5841  10.9561\n",
      "      4        \u001b[36m0.4905\u001b[0m       0.4380        4.4526  10.6933\n",
      "      5        \u001b[36m0.4892\u001b[0m       0.4431        5.4854  10.8450\n",
      "      6        \u001b[36m0.4867\u001b[0m       0.4432        5.5742  10.9599\n",
      "      7        \u001b[36m0.4849\u001b[0m       0.4428        9.0912  10.9783\n",
      "      8        \u001b[36m0.4841\u001b[0m       0.4402        7.5306  10.7751\n",
      "      9        \u001b[36m0.4820\u001b[0m       0.4399        9.2725  10.7296\n",
      "     10        \u001b[36m0.4811\u001b[0m       0.4404        8.0794  11.0069\n",
      "     11        \u001b[36m0.4797\u001b[0m       0.4417        7.5859  10.8956\n",
      "     12        \u001b[36m0.4790\u001b[0m       0.4419        7.7745  10.8817\n",
      "     13        \u001b[36m0.4782\u001b[0m       0.4421        9.3726  10.8383\n",
      "     14        \u001b[36m0.4774\u001b[0m       0.4387       10.7315  10.8236\n",
      "     15        \u001b[36m0.4765\u001b[0m       0.4414       10.2245  11.1178\n",
      "     16        \u001b[36m0.4759\u001b[0m       0.4392       14.6351  10.7894\n",
      "     17        \u001b[36m0.4750\u001b[0m       0.4456       12.5148  10.8251\n",
      "     18        \u001b[36m0.4749\u001b[0m       0.4357       14.4804  10.8259\n",
      "     19        \u001b[36m0.4741\u001b[0m       0.4453       14.8036  11.0548\n",
      "     20        \u001b[36m0.4734\u001b[0m       0.4363       14.8756  10.9975\n",
      "     21        \u001b[36m0.4730\u001b[0m       0.4466       13.0909  10.6460\n",
      "     22        \u001b[36m0.4720\u001b[0m       0.4355       17.9739  10.8580\n",
      "     23        \u001b[36m0.4718\u001b[0m       0.4394       18.0005  10.9243\n",
      "     24        \u001b[36m0.4711\u001b[0m       0.4400       15.0162  11.0658\n",
      "     25        \u001b[36m0.4696\u001b[0m       0.4427       20.7373  10.7936\n",
      "     26        0.4708       0.4342       19.6036  10.9471\n",
      "     27        \u001b[36m0.4693\u001b[0m       0.4368       24.1410  11.0078\n",
      "     28        0.4704       0.4405       17.5556  11.0988\n",
      "     29        \u001b[36m0.4686\u001b[0m       0.4394       16.6966  10.8653\n",
      "     30        0.4693       0.4340       28.1705  10.8268\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5245\u001b[0m       \u001b[32m0.5035\u001b[0m        \u001b[35m1.2881\u001b[0m  10.8001\n",
      "      2        \u001b[36m0.5024\u001b[0m       0.4885        1.7814  10.8615\n",
      "      3        \u001b[36m0.4954\u001b[0m       0.4920        1.6132  10.7838\n",
      "      4        \u001b[36m0.4912\u001b[0m       0.4841        2.3607  10.8819\n",
      "      5        \u001b[36m0.4877\u001b[0m       0.4819        2.1523  10.9010\n",
      "      6        \u001b[36m0.4847\u001b[0m       0.4845        2.7260  11.0211\n",
      "      7        \u001b[36m0.4823\u001b[0m       0.4817        3.2200  10.8736\n",
      "      8        \u001b[36m0.4795\u001b[0m       0.4834        3.0586  10.9042\n",
      "      9        \u001b[36m0.4787\u001b[0m       0.4870        2.6569  10.9769\n",
      "     10        \u001b[36m0.4774\u001b[0m       0.4819        3.0116  11.0367\n",
      "     11        \u001b[36m0.4754\u001b[0m       0.4761        4.7812  10.7842\n",
      "     12        \u001b[36m0.4749\u001b[0m       0.4811        4.5089  10.8259\n",
      "     13        \u001b[36m0.4729\u001b[0m       0.4779        4.2962  10.8745\n",
      "     14        \u001b[36m0.4721\u001b[0m       0.4806        4.2412  11.0082\n",
      "     15        \u001b[36m0.4706\u001b[0m       0.4769        4.9233  10.8459\n",
      "     16        \u001b[36m0.4706\u001b[0m       0.4779        5.5781  10.8689\n",
      "     17        \u001b[36m0.4694\u001b[0m       0.4781        5.8251  10.8427\n",
      "     18        \u001b[36m0.4677\u001b[0m       0.4737        5.9299  11.0099\n",
      "     19        \u001b[36m0.4671\u001b[0m       0.4763        6.5574  10.8775\n",
      "     20        \u001b[36m0.4661\u001b[0m       0.4738        6.9617  10.7771\n",
      "     21        0.4664       0.4767        7.9033  10.9131\n",
      "     22        \u001b[36m0.4656\u001b[0m       0.4780        7.0032  10.9859\n",
      "     23        \u001b[36m0.4651\u001b[0m       0.4784        5.6709  11.0074\n",
      "     24        0.4652       0.4746        8.6006  10.7420\n",
      "     25        \u001b[36m0.4641\u001b[0m       0.4742        9.6201  10.9201\n",
      "     26        \u001b[36m0.4638\u001b[0m       0.4781        6.8096  10.9858\n",
      "     27        \u001b[36m0.4628\u001b[0m       0.4747       12.2113  11.0242\n",
      "     28        \u001b[36m0.4625\u001b[0m       0.4796        7.7383  10.7548\n",
      "     29        \u001b[36m0.4620\u001b[0m       0.4802       10.5254  10.8660\n",
      "     30        0.4621       0.4754        6.4644  10.8604\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4621\u001b[0m       \u001b[32m0.5736\u001b[0m        \u001b[35m0.7019\u001b[0m  10.9975\n",
      "      2        \u001b[36m0.4573\u001b[0m       \u001b[32m0.5862\u001b[0m        0.7028  10.6821\n",
      "      3        \u001b[36m0.4557\u001b[0m       \u001b[32m0.5881\u001b[0m        \u001b[35m0.6968\u001b[0m  10.8980\n",
      "      4        \u001b[36m0.4549\u001b[0m       0.5769        0.7054  10.8605\n",
      "      5        \u001b[36m0.4538\u001b[0m       0.5482        0.7181  11.0559\n",
      "      6        0.4539       0.5809        0.6975  10.7374\n",
      "      7        \u001b[36m0.4522\u001b[0m       0.5803        0.6980  10.8418\n",
      "      8        \u001b[36m0.4518\u001b[0m       0.5834        0.7045  10.8742\n",
      "      9        \u001b[36m0.4512\u001b[0m       0.5795        0.7133  10.8777\n",
      "     10        \u001b[36m0.4505\u001b[0m       0.5815        0.7196  10.9045\n",
      "     11        \u001b[36m0.4493\u001b[0m       0.5696        0.7371  10.6785\n",
      "     12        0.4498       0.5475        0.7771  10.8793\n",
      "     13        \u001b[36m0.4485\u001b[0m       0.5548        0.7613  10.8885\n",
      "     14        \u001b[36m0.4472\u001b[0m       0.5420        0.8551  10.8960\n",
      "     15        \u001b[36m0.4470\u001b[0m       0.5386        0.9399  10.6703\n",
      "     16        \u001b[36m0.4464\u001b[0m       0.5443        1.0672  10.8776\n",
      "     17        \u001b[36m0.4461\u001b[0m       0.5381        0.9541  10.9855\n",
      "     18        \u001b[36m0.4451\u001b[0m       0.5382        0.9566  10.7824\n",
      "     19        \u001b[36m0.4450\u001b[0m       0.5369        1.1826  10.7842\n",
      "     20        \u001b[36m0.4442\u001b[0m       0.5298        1.5192  10.7822\n",
      "     21        \u001b[36m0.4441\u001b[0m       0.5108        1.7554  10.9749\n",
      "     22        \u001b[36m0.4439\u001b[0m       0.5242        1.4081  11.4236\n",
      "     23        \u001b[36m0.4432\u001b[0m       0.5186        1.4678  10.8569\n",
      "     24        0.4432       0.5018        2.4039  10.7559\n",
      "     25        \u001b[36m0.4432\u001b[0m       0.5160        1.8435  11.0423\n",
      "     26        \u001b[36m0.4427\u001b[0m       0.5216        2.1098  10.7722\n",
      "     27        0.4430       0.5265        2.2288  10.9034\n",
      "     28        0.4429       0.5216        2.1518  10.8111\n",
      "     29        \u001b[36m0.4423\u001b[0m       0.5073        2.2372  10.9604\n",
      "     30        0.4426       0.5206        2.0950  10.8894\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5321\u001b[0m       \u001b[32m0.4436\u001b[0m        \u001b[35m3.4437\u001b[0m  10.8354\n",
      "      2        \u001b[36m0.5178\u001b[0m       0.4435        \u001b[35m3.0219\u001b[0m  10.8028\n",
      "      3        \u001b[36m0.5116\u001b[0m       0.4402        4.4661  11.0755\n",
      "      4        \u001b[36m0.5078\u001b[0m       \u001b[32m0.4451\u001b[0m        4.0808  10.9667\n",
      "      5        \u001b[36m0.5048\u001b[0m       0.4390        5.2578  10.8554\n",
      "      6        \u001b[36m0.5021\u001b[0m       0.4416        5.9061  10.9294\n",
      "      7        \u001b[36m0.4996\u001b[0m       0.4374        4.8556  11.0063\n",
      "      8        \u001b[36m0.4978\u001b[0m       0.4388        5.3262  10.9493\n",
      "      9        \u001b[36m0.4953\u001b[0m       0.4389        5.2580  10.9308\n",
      "     10        \u001b[36m0.4935\u001b[0m       0.4387        8.0255  10.9165\n",
      "     11        \u001b[36m0.4932\u001b[0m       0.4413        6.4445  10.9280\n",
      "     12        \u001b[36m0.4917\u001b[0m       0.4393        8.3410  10.9865\n",
      "     13        \u001b[36m0.4891\u001b[0m       0.4349        8.2785  11.0646\n",
      "     14        \u001b[36m0.4887\u001b[0m       0.4370        7.7000  10.9160\n",
      "     15        \u001b[36m0.4873\u001b[0m       0.4417        7.7732  10.8926\n",
      "     16        \u001b[36m0.4854\u001b[0m       0.4360       11.4781  10.9554\n",
      "     17        \u001b[36m0.4848\u001b[0m       0.4404        8.7661  11.0383\n",
      "     18        0.4852       0.4376       10.4452  10.8370\n",
      "     19        \u001b[36m0.4839\u001b[0m       0.4372       10.0327  10.8499\n",
      "     20        \u001b[36m0.4833\u001b[0m       0.4366        8.8915  11.0396\n",
      "     21        \u001b[36m0.4812\u001b[0m       0.4359       10.9859  10.9420\n",
      "     22        \u001b[36m0.4809\u001b[0m       0.4337       12.3085  10.7515\n",
      "     23        \u001b[36m0.4803\u001b[0m       0.4361       12.1525  10.9313\n",
      "     24        \u001b[36m0.4793\u001b[0m       0.4394       11.3831  10.9750\n",
      "     25        \u001b[36m0.4781\u001b[0m       0.4382       12.8927  10.9713\n",
      "     26        \u001b[36m0.4776\u001b[0m       0.4422       14.9924  10.8339\n",
      "     27        \u001b[36m0.4770\u001b[0m       0.4352       18.5813  10.8281\n",
      "     28        \u001b[36m0.4761\u001b[0m       0.4352        8.1837  10.8567\n",
      "     29        \u001b[36m0.4748\u001b[0m       0.4312       17.4616  11.0735\n",
      "     30        0.4752       0.4409       13.0702  10.9638\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5530\u001b[0m       \u001b[32m0.4758\u001b[0m        \u001b[35m0.8181\u001b[0m  10.7860\n",
      "      2        \u001b[36m0.5314\u001b[0m       \u001b[32m0.4906\u001b[0m        0.8674  10.8948\n",
      "      3        \u001b[36m0.5239\u001b[0m       0.4795        1.0195  11.0733\n",
      "      4        \u001b[36m0.5183\u001b[0m       0.4712        1.0954  10.8962\n",
      "      5        \u001b[36m0.5138\u001b[0m       0.4720        1.1002  10.7953\n",
      "      6        \u001b[36m0.5114\u001b[0m       0.4619        1.1866  10.9794\n",
      "      7        \u001b[36m0.5089\u001b[0m       0.4800        1.2796  10.9400\n",
      "      8        \u001b[36m0.5059\u001b[0m       0.4655        1.1804  10.8996\n",
      "      9        \u001b[36m0.5027\u001b[0m       0.4691        1.5506  10.8456\n",
      "     10        \u001b[36m0.4996\u001b[0m       0.4641        1.1962  10.9651\n",
      "     11        \u001b[36m0.4973\u001b[0m       0.4690        1.5074  10.9946\n",
      "     12        \u001b[36m0.4971\u001b[0m       0.4655        1.3520  10.7943\n",
      "     13        \u001b[36m0.4954\u001b[0m       0.4683        1.4812  10.8348\n",
      "     14        \u001b[36m0.4944\u001b[0m       0.4676        1.3523  10.9877\n",
      "     15        \u001b[36m0.4925\u001b[0m       0.4659        1.8102  11.0180\n",
      "     16        \u001b[36m0.4901\u001b[0m       0.4697        1.4665  10.9999\n",
      "     17        \u001b[36m0.4884\u001b[0m       0.4701        1.2824  10.7182\n",
      "     18        \u001b[36m0.4877\u001b[0m       0.4682        1.5736  10.8225\n",
      "     19        \u001b[36m0.4867\u001b[0m       0.4669        1.6023  11.0626\n",
      "     20        \u001b[36m0.4862\u001b[0m       0.4671        2.0836  11.0128\n",
      "     21        \u001b[36m0.4849\u001b[0m       0.4678        1.4206  10.8646\n",
      "     22        \u001b[36m0.4843\u001b[0m       0.4654        1.7185  10.8738\n",
      "     23        \u001b[36m0.4824\u001b[0m       0.4672        2.0011  10.8302\n",
      "     24        \u001b[36m0.4816\u001b[0m       0.4689        1.3949  10.8732\n",
      "     25        \u001b[36m0.4808\u001b[0m       0.4664        1.7898  10.8849\n",
      "     26        \u001b[36m0.4807\u001b[0m       0.4656        1.8714  10.8259\n",
      "     27        \u001b[36m0.4789\u001b[0m       0.4664        1.6540  10.8578\n",
      "     28        0.4791       0.4675        2.0860  11.1039\n",
      "     29        \u001b[36m0.4774\u001b[0m       0.4666        2.0328  10.9431\n",
      "     30        0.4782       0.4670        1.9776  10.8605\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4749\u001b[0m       \u001b[32m0.5686\u001b[0m        \u001b[35m0.7167\u001b[0m  10.7188\n",
      "      2        \u001b[36m0.4668\u001b[0m       \u001b[32m0.5719\u001b[0m        \u001b[35m0.6828\u001b[0m  11.1984\n",
      "      3        \u001b[36m0.4647\u001b[0m       \u001b[32m0.5740\u001b[0m        0.6969  10.7629\n",
      "      4        \u001b[36m0.4638\u001b[0m       0.5719        0.6918  10.7684\n",
      "      5        \u001b[36m0.4629\u001b[0m       \u001b[32m0.5752\u001b[0m        \u001b[35m0.6818\u001b[0m  10.9993\n",
      "      6        \u001b[36m0.4616\u001b[0m       \u001b[32m0.5856\u001b[0m        0.7065  10.9669\n",
      "      7        \u001b[36m0.4612\u001b[0m       0.5795        0.6923  10.9320\n",
      "      8        \u001b[36m0.4602\u001b[0m       0.5817        0.6887  10.7285\n",
      "      9        \u001b[36m0.4599\u001b[0m       0.5777        0.6849  10.7762\n",
      "     10        \u001b[36m0.4583\u001b[0m       0.5774        0.6830  10.9945\n",
      "     11        \u001b[36m0.4579\u001b[0m       0.5786        0.7000  10.9541\n",
      "     12        \u001b[36m0.4564\u001b[0m       0.5833        0.6834  10.8016\n",
      "     13        \u001b[36m0.4562\u001b[0m       0.5766        0.6928  10.9867\n",
      "     14        \u001b[36m0.4561\u001b[0m       0.5795        0.7000  10.8538\n",
      "     15        \u001b[36m0.4551\u001b[0m       0.5762        0.6993  10.9320\n",
      "     16        \u001b[36m0.4539\u001b[0m       0.5758        0.7154  10.8446\n",
      "     17        0.4543       0.5827        0.7426  10.8465\n",
      "     18        0.4547       0.5710        0.7163  10.9111\n",
      "     19        0.4540       0.5776        0.7118  11.0718\n",
      "     20        \u001b[36m0.4532\u001b[0m       0.5822        0.7245  11.0050\n",
      "     21        \u001b[36m0.4522\u001b[0m       0.5731        0.7360  11.0848\n",
      "     22        0.4528       0.5724        0.7205  11.0471\n",
      "     23        0.4527       0.5682        0.7269  10.9227\n",
      "     24        0.4523       0.5588        0.7238  10.9776\n",
      "     25        \u001b[36m0.4509\u001b[0m       0.5741        0.7458  10.9207\n",
      "     26        \u001b[36m0.4506\u001b[0m       0.5801        0.7467  11.0195\n",
      "     27        0.4508       0.5825        0.7755  11.0371\n",
      "     28        \u001b[36m0.4499\u001b[0m       0.5741        0.7686  10.8223\n",
      "     29        \u001b[36m0.4498\u001b[0m       0.5781        0.7515  10.9161\n",
      "     30        \u001b[36m0.4486\u001b[0m       0.5612        0.7414  10.9931\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5091\u001b[0m       \u001b[32m0.4514\u001b[0m        \u001b[35m3.3392\u001b[0m  10.7328\n",
      "      2        \u001b[36m0.4947\u001b[0m       0.4485        4.3737  10.9199\n",
      "      3        \u001b[36m0.4898\u001b[0m       0.4433        5.1944  10.7551\n",
      "      4        \u001b[36m0.4861\u001b[0m       0.4444        6.9231  10.8897\n",
      "      5        \u001b[36m0.4836\u001b[0m       0.4441        7.4472  11.0519\n",
      "      6        \u001b[36m0.4820\u001b[0m       0.4444        6.9656  10.8599\n",
      "      7        \u001b[36m0.4793\u001b[0m       0.4389        8.9371  10.8348\n",
      "      8        \u001b[36m0.4766\u001b[0m       0.4432       10.4169  11.0571\n",
      "      9        \u001b[36m0.4748\u001b[0m       0.4410       12.6414  10.8120\n",
      "     10        \u001b[36m0.4741\u001b[0m       0.4414        9.6860  10.9683\n",
      "     11        \u001b[36m0.4728\u001b[0m       0.4428        9.1590  10.7823\n",
      "     12        \u001b[36m0.4710\u001b[0m       0.4401       11.9371  10.8187\n",
      "     13        \u001b[36m0.4700\u001b[0m       0.4423       13.1290  10.9755\n",
      "     14        \u001b[36m0.4683\u001b[0m       0.4414       12.8639  10.9953\n",
      "     15        \u001b[36m0.4676\u001b[0m       0.4383       17.6821  10.7486\n",
      "     16        \u001b[36m0.4651\u001b[0m       0.4399       16.6499  10.8939\n",
      "     17        \u001b[36m0.4649\u001b[0m       0.4395       18.2744  11.0075\n",
      "     18        \u001b[36m0.4636\u001b[0m       0.4410       17.6010  11.1312\n",
      "     19        \u001b[36m0.4633\u001b[0m       0.4365       17.3932  10.7082\n",
      "     20        \u001b[36m0.4622\u001b[0m       0.4418       16.1176  10.7866\n",
      "     21        \u001b[36m0.4605\u001b[0m       0.4394       16.5783  11.0299\n",
      "     22        0.4608       0.4418       23.1822  11.2892\n",
      "     23        \u001b[36m0.4601\u001b[0m       0.4418       21.4810  10.9136\n",
      "     24        0.4607       0.4423       24.1790  10.9602\n",
      "     25        \u001b[36m0.4577\u001b[0m       0.4446       22.5694  10.9906\n",
      "     26        \u001b[36m0.4572\u001b[0m       0.4378       24.5247  10.8394\n",
      "     27        0.4578       0.4430       22.6333  10.8462\n",
      "     28        \u001b[36m0.4565\u001b[0m       0.4396       26.3786  10.8709\n",
      "     29        0.4566       0.4400       27.4099  10.9563\n",
      "     30        \u001b[36m0.4549\u001b[0m       0.4398       25.1544  11.0182\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5226\u001b[0m       \u001b[32m0.5189\u001b[0m        \u001b[35m1.1812\u001b[0m  10.6634\n",
      "      2        \u001b[36m0.5000\u001b[0m       0.4991        1.6801  10.9000\n",
      "      3        \u001b[36m0.4935\u001b[0m       0.4936        2.1341  11.0254\n",
      "      4        \u001b[36m0.4881\u001b[0m       0.4907        2.5164  10.8821\n",
      "      5        \u001b[36m0.4841\u001b[0m       0.4828        2.8054  10.9868\n",
      "      6        \u001b[36m0.4797\u001b[0m       0.4868        2.9181  10.9196\n",
      "      7        \u001b[36m0.4770\u001b[0m       0.4903        2.1350  10.9641\n",
      "      8        \u001b[36m0.4740\u001b[0m       0.4897        2.5700  10.8156\n",
      "      9        \u001b[36m0.4718\u001b[0m       0.4922        2.5257  10.8521\n",
      "     10        \u001b[36m0.4699\u001b[0m       0.4871        3.3113  10.8510\n",
      "     11        \u001b[36m0.4679\u001b[0m       0.4930        3.2081  10.9069\n",
      "     12        \u001b[36m0.4655\u001b[0m       0.4842        3.1173  11.0092\n",
      "     13        0.4655       0.4825        3.5782  10.9617\n",
      "     14        \u001b[36m0.4619\u001b[0m       0.4858        3.1080  10.7180\n",
      "     15        \u001b[36m0.4617\u001b[0m       0.4795        4.0062  10.8731\n",
      "     16        \u001b[36m0.4597\u001b[0m       0.4801        4.0329  10.9587\n",
      "     17        \u001b[36m0.4593\u001b[0m       0.4822        3.8024  11.0028\n",
      "     18        \u001b[36m0.4582\u001b[0m       0.4824        4.6525  10.7210\n",
      "     19        \u001b[36m0.4553\u001b[0m       0.4753        5.8836  10.8271\n",
      "     20        0.4562       0.4819        4.7936  11.0374\n",
      "     21        \u001b[36m0.4537\u001b[0m       0.4791        5.4057  11.0806\n",
      "     22        \u001b[36m0.4529\u001b[0m       0.4795        5.6947  10.9174\n",
      "     23        0.4529       0.4801        5.4990  10.8258\n",
      "     24        \u001b[36m0.4518\u001b[0m       0.4772        7.5287  10.8622\n",
      "     25        0.4533       0.4790        8.0397  10.9335\n",
      "     26        \u001b[36m0.4514\u001b[0m       0.4790        6.3238  10.8661\n",
      "     27        \u001b[36m0.4507\u001b[0m       0.4754        8.0676  10.9187\n",
      "     28        \u001b[36m0.4494\u001b[0m       0.4770        8.4383  10.8100\n",
      "     29        \u001b[36m0.4494\u001b[0m       0.4719        8.9853  10.9885\n",
      "     30        \u001b[36m0.4482\u001b[0m       0.4768        7.5106  10.9511\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4652\u001b[0m       \u001b[32m0.5907\u001b[0m        \u001b[35m0.6754\u001b[0m  10.7822\n",
      "      2        \u001b[36m0.4572\u001b[0m       0.5362        0.7039  10.8032\n",
      "      3        \u001b[36m0.4557\u001b[0m       0.5877        0.6885  10.9410\n",
      "      4        \u001b[36m0.4540\u001b[0m       0.5819        0.7017  10.8587\n",
      "      5        \u001b[36m0.4527\u001b[0m       0.5856        0.6879  10.7744\n",
      "      6        \u001b[36m0.4516\u001b[0m       \u001b[32m0.5966\u001b[0m        0.7018  10.9649\n",
      "      7        \u001b[36m0.4504\u001b[0m       0.5799        0.7000  11.0760\n",
      "      8        \u001b[36m0.4497\u001b[0m       0.5711        0.6976  10.7019\n",
      "      9        \u001b[36m0.4488\u001b[0m       0.5631        0.7337  10.7287\n",
      "     10        \u001b[36m0.4473\u001b[0m       0.5606        0.7599  10.9065\n",
      "     11        \u001b[36m0.4461\u001b[0m       0.5745        0.7917  10.9517\n",
      "     12        \u001b[36m0.4459\u001b[0m       0.5477        0.8112  10.8707\n",
      "     13        \u001b[36m0.4445\u001b[0m       0.5422        0.8916  10.8174\n",
      "     14        \u001b[36m0.4437\u001b[0m       0.5424        0.9828  10.9328\n",
      "     15        \u001b[36m0.4424\u001b[0m       0.5445        1.2065  11.0044\n",
      "     16        0.4426       0.5290        1.1758  10.7806\n",
      "     17        \u001b[36m0.4414\u001b[0m       0.5349        1.0655  10.7695\n",
      "     18        \u001b[36m0.4405\u001b[0m       0.5322        1.1106  10.9182\n",
      "     19        \u001b[36m0.4402\u001b[0m       0.5328        1.3624  11.0124\n",
      "     20        \u001b[36m0.4400\u001b[0m       0.5368        1.1501  11.0154\n",
      "     21        \u001b[36m0.4390\u001b[0m       0.5169        1.4255  10.9179\n",
      "     22        \u001b[36m0.4389\u001b[0m       0.5411        1.2868  10.7282\n",
      "     23        \u001b[36m0.4383\u001b[0m       0.5267        1.4870  10.9929\n",
      "     24        0.4385       0.5191        1.2965  11.0732\n",
      "     25        \u001b[36m0.4370\u001b[0m       0.5072        1.5668  11.0550\n",
      "     26        0.4372       0.5223        1.9837  10.7420\n",
      "     27        \u001b[36m0.4364\u001b[0m       0.5179        1.6765  10.7708\n",
      "     28        \u001b[36m0.4362\u001b[0m       0.5118        2.4785  10.9618\n",
      "     29        \u001b[36m0.4356\u001b[0m       0.5197        1.7596  10.9384\n",
      "     30        0.4358       0.5183        1.8973  10.8454\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5063\u001b[0m       \u001b[32m0.4504\u001b[0m        \u001b[35m3.1903\u001b[0m  10.8233\n",
      "      2        \u001b[36m0.4925\u001b[0m       0.4451        5.6961  11.0212\n",
      "      3        \u001b[36m0.4883\u001b[0m       0.4388        5.4432  10.9631\n",
      "      4        \u001b[36m0.4851\u001b[0m       0.4461        7.4132  10.8383\n",
      "      5        \u001b[36m0.4826\u001b[0m       0.4412        8.2370  10.8537\n",
      "      6        \u001b[36m0.4796\u001b[0m       0.4420        7.6288  11.0458\n",
      "      7        \u001b[36m0.4784\u001b[0m       0.4414        9.4932  10.9986\n",
      "      8        \u001b[36m0.4761\u001b[0m       0.4422        7.5744  10.7722\n",
      "      9        \u001b[36m0.4752\u001b[0m       0.4389       10.1537  10.9372\n",
      "     10        \u001b[36m0.4725\u001b[0m       0.4407       14.7203  10.8230\n",
      "     11        \u001b[36m0.4712\u001b[0m       0.4413       13.6229  10.9312\n",
      "     12        \u001b[36m0.4698\u001b[0m       0.4398       13.0230  10.8995\n",
      "     13        \u001b[36m0.4677\u001b[0m       0.4460       14.1665  10.8760\n",
      "     14        \u001b[36m0.4664\u001b[0m       0.4450       13.3835  10.9311\n",
      "     15        \u001b[36m0.4663\u001b[0m       0.4424       15.5893  10.9132\n",
      "     16        \u001b[36m0.4643\u001b[0m       0.4411       17.4615  10.8699\n",
      "     17        \u001b[36m0.4636\u001b[0m       0.4448       19.8694  10.9177\n",
      "     18        \u001b[36m0.4632\u001b[0m       0.4421       15.7538  10.9137\n",
      "     19        \u001b[36m0.4613\u001b[0m       0.4440       13.2696  11.0815\n",
      "     20        \u001b[36m0.4607\u001b[0m       0.4393       20.3789  10.8473\n",
      "     21        \u001b[36m0.4601\u001b[0m       0.4442       18.0279  10.8037\n",
      "     22        \u001b[36m0.4588\u001b[0m       0.4447       23.4125  10.9899\n",
      "     23        \u001b[36m0.4583\u001b[0m       0.4406       16.9387  11.1215\n",
      "     24        \u001b[36m0.4582\u001b[0m       0.4406       25.3877  10.7648\n",
      "     25        \u001b[36m0.4562\u001b[0m       0.4345       23.9140  10.8229\n",
      "     26        0.4569       0.4430       31.2550  11.0068\n",
      "     27        \u001b[36m0.4555\u001b[0m       0.4444       18.3425  11.0783\n",
      "     28        0.4561       0.4390       21.8352  11.0476\n",
      "     29        \u001b[36m0.4540\u001b[0m       0.4279       30.5744  10.9116\n",
      "     30        0.4545       0.4282       29.3593  10.9095\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5217\u001b[0m       \u001b[32m0.5016\u001b[0m        \u001b[35m1.4123\u001b[0m  10.9500\n",
      "      2        \u001b[36m0.4984\u001b[0m       0.4968        1.4437  11.0632\n",
      "      3        \u001b[36m0.4916\u001b[0m       0.4950        1.8255  10.8098\n",
      "      4        \u001b[36m0.4864\u001b[0m       0.4785        2.7435  10.9644\n",
      "      5        \u001b[36m0.4819\u001b[0m       0.4784        2.4560  10.9246\n",
      "      6        \u001b[36m0.4796\u001b[0m       0.4828        3.2896  11.0318\n",
      "      7        \u001b[36m0.4764\u001b[0m       0.4779        3.7869  10.7436\n",
      "      8        \u001b[36m0.4736\u001b[0m       0.4831        2.8521  10.9901\n",
      "      9        \u001b[36m0.4708\u001b[0m       0.4818        4.2328  10.8189\n",
      "     10        \u001b[36m0.4687\u001b[0m       0.4846        3.7976  10.8386\n",
      "     11        \u001b[36m0.4674\u001b[0m       0.4856        3.2756  10.8749\n",
      "     12        \u001b[36m0.4654\u001b[0m       0.4767        4.0523  10.8548\n",
      "     13        \u001b[36m0.4641\u001b[0m       0.4853        4.4479  11.0514\n",
      "     14        \u001b[36m0.4619\u001b[0m       0.4740        6.6874  10.8498\n",
      "     15        \u001b[36m0.4608\u001b[0m       0.4776        5.5516  10.7406\n",
      "     16        \u001b[36m0.4596\u001b[0m       0.4775        6.3471  10.8782\n",
      "     17        \u001b[36m0.4588\u001b[0m       0.4761        7.9122  11.0182\n",
      "     18        \u001b[36m0.4563\u001b[0m       0.4796        5.8782  11.1760\n",
      "     19        \u001b[36m0.4561\u001b[0m       0.4789        7.4936  10.7886\n",
      "     20        \u001b[36m0.4554\u001b[0m       0.4725        8.9668  10.8618\n",
      "     21        \u001b[36m0.4536\u001b[0m       0.4812        5.8460  10.9184\n",
      "     22        \u001b[36m0.4535\u001b[0m       0.4782        7.9816  11.1595\n",
      "     23        \u001b[36m0.4518\u001b[0m       0.4766        9.2676  10.9010\n",
      "     24        \u001b[36m0.4516\u001b[0m       0.4731       10.2124  10.8686\n",
      "     25        \u001b[36m0.4506\u001b[0m       0.4750        9.5704  10.7588\n",
      "     26        \u001b[36m0.4505\u001b[0m       0.4693       13.8233  11.1055\n",
      "     27        \u001b[36m0.4497\u001b[0m       0.4745        9.6186  10.9792\n",
      "     28        \u001b[36m0.4487\u001b[0m       0.4740       10.1072  10.8691\n",
      "     29        \u001b[36m0.4478\u001b[0m       0.4729       14.2141  10.9011\n",
      "     30        \u001b[36m0.4472\u001b[0m       0.4739       11.7641  11.0294\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4620\u001b[0m       \u001b[32m0.5458\u001b[0m        \u001b[35m0.7275\u001b[0m  10.9666\n",
      "      2        \u001b[36m0.4564\u001b[0m       \u001b[32m0.5720\u001b[0m        \u001b[35m0.7051\u001b[0m  10.8648\n",
      "      3        \u001b[36m0.4550\u001b[0m       \u001b[32m0.5850\u001b[0m        \u001b[35m0.6836\u001b[0m  11.0768\n",
      "      4        \u001b[36m0.4534\u001b[0m       0.5719        0.7122  10.9176\n",
      "      5        \u001b[36m0.4523\u001b[0m       0.5693        0.7008  10.8292\n",
      "      6        \u001b[36m0.4514\u001b[0m       \u001b[32m0.5879\u001b[0m        0.6920  10.8168\n",
      "      7        \u001b[36m0.4505\u001b[0m       0.5529        0.7059  10.8929\n",
      "      8        \u001b[36m0.4496\u001b[0m       0.5832        0.7141  11.0629\n",
      "      9        \u001b[36m0.4486\u001b[0m       0.5872        0.7133  10.8910\n",
      "     10        \u001b[36m0.4478\u001b[0m       0.5722        0.7030  10.8722\n",
      "     11        \u001b[36m0.4471\u001b[0m       0.5801        0.7142  10.7415\n",
      "     12        \u001b[36m0.4470\u001b[0m       0.5868        0.7117  10.9117\n",
      "     13        \u001b[36m0.4459\u001b[0m       0.5795        0.7553  10.9842\n",
      "     14        0.4459       0.5804        0.7505  10.8273\n",
      "     15        \u001b[36m0.4445\u001b[0m       0.5776        0.7375  10.8200\n",
      "     16        \u001b[36m0.4441\u001b[0m       0.5622        0.7642  10.8974\n",
      "     17        \u001b[36m0.4438\u001b[0m       0.5707        0.8328  11.1095\n",
      "     18        \u001b[36m0.4423\u001b[0m       0.5530        0.8769  10.7963\n",
      "     19        \u001b[36m0.4413\u001b[0m       0.5534        0.8470  10.8747\n",
      "     20        \u001b[36m0.4409\u001b[0m       0.5507        0.8685  10.7867\n",
      "     21        \u001b[36m0.4406\u001b[0m       0.5414        1.1070  10.9497\n",
      "     22        \u001b[36m0.4397\u001b[0m       0.5306        1.1687  10.9145\n",
      "     23        \u001b[36m0.4391\u001b[0m       0.5315        1.1418  10.8849\n",
      "     24        \u001b[36m0.4382\u001b[0m       0.5279        1.2864  10.8851\n",
      "     25        \u001b[36m0.4376\u001b[0m       0.5257        1.3336  11.0816\n",
      "     26        \u001b[36m0.4376\u001b[0m       0.5243        1.4777  10.8041\n",
      "     27        \u001b[36m0.4374\u001b[0m       0.5174        1.8068  10.8533\n",
      "     28        \u001b[36m0.4369\u001b[0m       0.5191        2.2718  10.9643\n",
      "     29        \u001b[36m0.4361\u001b[0m       0.5154        2.0608  11.1674\n",
      "     30        0.4366       0.5198        2.5697  10.7799\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5054\u001b[0m       \u001b[32m0.4528\u001b[0m        \u001b[35m4.6480\u001b[0m  10.9890\n",
      "      2        \u001b[36m0.4922\u001b[0m       \u001b[32m0.4534\u001b[0m        4.8410  11.0427\n",
      "      3        \u001b[36m0.4884\u001b[0m       0.4424        \u001b[35m4.2834\u001b[0m  10.9355\n",
      "      4        \u001b[36m0.4856\u001b[0m       0.4419        5.2201  10.9153\n",
      "      5        \u001b[36m0.4821\u001b[0m       0.4446        5.6679  10.8597\n",
      "      6        \u001b[36m0.4809\u001b[0m       0.4446        6.8039  10.9457\n",
      "      7        \u001b[36m0.4788\u001b[0m       0.4443        8.1601  10.9902\n",
      "      8        \u001b[36m0.4767\u001b[0m       0.4404       10.0910  11.0471\n",
      "      9        \u001b[36m0.4750\u001b[0m       0.4407       11.2121  10.8115\n",
      "     10        \u001b[36m0.4736\u001b[0m       0.4369       11.2574  10.8559\n",
      "     11        \u001b[36m0.4719\u001b[0m       0.4401       12.9537  11.1124\n",
      "     12        \u001b[36m0.4704\u001b[0m       0.4448       10.6352  10.9831\n",
      "     13        \u001b[36m0.4700\u001b[0m       0.4400       15.3454  10.8938\n",
      "     14        \u001b[36m0.4688\u001b[0m       0.4407       14.5263  10.8119\n",
      "     15        \u001b[36m0.4673\u001b[0m       0.4430       16.0143  10.9614\n",
      "     16        \u001b[36m0.4660\u001b[0m       0.4455       15.2861  10.8665\n",
      "     17        \u001b[36m0.4658\u001b[0m       0.4426       19.2350  10.8544\n",
      "     18        \u001b[36m0.4647\u001b[0m       0.4452       18.6060  10.8326\n",
      "     19        \u001b[36m0.4638\u001b[0m       0.4407       19.0705  11.0033\n",
      "     20        \u001b[36m0.4636\u001b[0m       0.4416       20.1883  11.0950\n",
      "     21        \u001b[36m0.4623\u001b[0m       0.4368       20.6576  10.7358\n",
      "     22        \u001b[36m0.4608\u001b[0m       0.4379       22.9863  10.8345\n",
      "     23        0.4609       0.4388       24.4141  10.8051\n",
      "     24        \u001b[36m0.4594\u001b[0m       0.4393       22.5130  11.0854\n",
      "     25        0.4600       0.4404       16.6282  10.8866\n",
      "     26        \u001b[36m0.4589\u001b[0m       0.4375       27.5314  10.8712\n",
      "     27        \u001b[36m0.4577\u001b[0m       0.4405       40.6010  10.8025\n",
      "     28        0.4578       0.4388       20.1209  10.9567\n",
      "     29        \u001b[36m0.4577\u001b[0m       0.4404       33.7787  11.0180\n",
      "     30        \u001b[36m0.4565\u001b[0m       0.4395       27.0656  10.8405\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5200\u001b[0m       \u001b[32m0.5003\u001b[0m        \u001b[35m1.5941\u001b[0m  10.7531\n",
      "      2        \u001b[36m0.4995\u001b[0m       \u001b[32m0.5057\u001b[0m        \u001b[35m1.3477\u001b[0m  10.9284\n",
      "      3        \u001b[36m0.4917\u001b[0m       0.4871        1.8597  11.0218\n",
      "      4        \u001b[36m0.4865\u001b[0m       0.4881        1.9626  10.7402\n",
      "      5        \u001b[36m0.4833\u001b[0m       0.4920        1.7072  10.8207\n",
      "      6        \u001b[36m0.4788\u001b[0m       0.4941        1.9807  10.9969\n",
      "      7        \u001b[36m0.4771\u001b[0m       0.4832        2.4527  10.9775\n",
      "      8        \u001b[36m0.4739\u001b[0m       0.4865        3.0554  11.0733\n",
      "      9        \u001b[36m0.4712\u001b[0m       0.4724        3.4392  10.7690\n",
      "     10        \u001b[36m0.4686\u001b[0m       0.4856        3.5876  10.9322\n",
      "     11        0.4690       0.4786        2.9943  11.0814\n",
      "     12        \u001b[36m0.4650\u001b[0m       0.4829        3.1705  10.7744\n",
      "     13        \u001b[36m0.4641\u001b[0m       0.4779        5.2509  10.8424\n",
      "     14        \u001b[36m0.4610\u001b[0m       0.4752        5.6412  10.9908\n",
      "     15        \u001b[36m0.4602\u001b[0m       0.4764        5.9484  11.0961\n",
      "     16        \u001b[36m0.4592\u001b[0m       0.4767        6.2333  10.7981\n",
      "     17        \u001b[36m0.4582\u001b[0m       0.4708        7.9473  10.8503\n",
      "     18        \u001b[36m0.4561\u001b[0m       0.4753        6.2258  11.0372\n",
      "     19        0.4562       0.4745        7.9714  11.1967\n",
      "     20        \u001b[36m0.4555\u001b[0m       0.4749        9.4625  10.7800\n",
      "     21        \u001b[36m0.4533\u001b[0m       0.4768        7.4567  10.8062\n",
      "     22        \u001b[36m0.4531\u001b[0m       0.4737       11.7908  11.0678\n",
      "     23        \u001b[36m0.4508\u001b[0m       0.4788        7.2266  11.0673\n",
      "     24        0.4516       0.4756       10.8103  10.9021\n",
      "     25        0.4510       0.4714       13.7291  10.8243\n",
      "     26        \u001b[36m0.4499\u001b[0m       0.4764       10.6586  11.0489\n",
      "     27        \u001b[36m0.4488\u001b[0m       0.4764       10.5393  11.1213\n",
      "     28        0.4497       0.4725       15.3442  10.9885\n",
      "     29        \u001b[36m0.4471\u001b[0m       0.4746       14.1109  10.8813\n",
      "     30        0.4471       0.4723       16.6913  10.8581\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4618\u001b[0m       \u001b[32m0.5608\u001b[0m        \u001b[35m0.7092\u001b[0m  10.9861\n",
      "      2        \u001b[36m0.4560\u001b[0m       \u001b[32m0.5827\u001b[0m        \u001b[35m0.6848\u001b[0m  10.7592\n",
      "      3        \u001b[36m0.4546\u001b[0m       \u001b[32m0.5859\u001b[0m        0.6869  10.7394\n",
      "      4        \u001b[36m0.4535\u001b[0m       0.5769        0.6993  10.8901\n",
      "      5        \u001b[36m0.4521\u001b[0m       \u001b[32m0.5898\u001b[0m        0.6934  10.9966\n",
      "      6        \u001b[36m0.4510\u001b[0m       0.5864        0.6904  10.9052\n",
      "      7        \u001b[36m0.4501\u001b[0m       0.5827        0.7012  10.8172\n",
      "      8        \u001b[36m0.4485\u001b[0m       0.5890        0.7124  10.8919\n",
      "      9        0.4486       0.5700        0.7079  10.8715\n",
      "     10        \u001b[36m0.4477\u001b[0m       0.5822        0.7282  10.8266\n",
      "     11        \u001b[36m0.4473\u001b[0m       0.5812        0.6958  10.8420\n",
      "     12        \u001b[36m0.4456\u001b[0m       0.5842        0.7431  10.8370\n",
      "     13        0.4457       0.5776        0.7337  10.9102\n",
      "     14        \u001b[36m0.4449\u001b[0m       0.5691        0.7088  11.0759\n",
      "     15        \u001b[36m0.4439\u001b[0m       0.5726        0.7424  10.8778\n",
      "     16        \u001b[36m0.4437\u001b[0m       0.5647        0.7396  10.7393\n",
      "     17        \u001b[36m0.4427\u001b[0m       0.5760        0.7488  10.8610\n",
      "     18        \u001b[36m0.4422\u001b[0m       0.5716        0.7863  11.0056\n",
      "     19        \u001b[36m0.4413\u001b[0m       0.5664        0.7916  10.9082\n",
      "     20        \u001b[36m0.4411\u001b[0m       0.5658        0.7826  10.9942\n",
      "     21        \u001b[36m0.4408\u001b[0m       0.5568        0.8139  10.8136\n",
      "     22        \u001b[36m0.4408\u001b[0m       0.5698        0.8490  10.9447\n",
      "     23        \u001b[36m0.4392\u001b[0m       0.5686        0.8942  11.0217\n",
      "     24        0.4400       0.5591        0.9115  10.8502\n",
      "     25        0.4393       0.5522        0.9579  10.9282\n",
      "     26        \u001b[36m0.4392\u001b[0m       0.5647        0.9094  11.0547\n",
      "     27        \u001b[36m0.4387\u001b[0m       0.5565        1.0409  10.8780\n",
      "     28        \u001b[36m0.4386\u001b[0m       0.5531        1.2505  10.9174\n",
      "     29        \u001b[36m0.4373\u001b[0m       0.5500        1.2798  10.8497\n",
      "     30        \u001b[36m0.4372\u001b[0m       0.5543        1.4160  11.0439\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5489\u001b[0m       \u001b[32m0.6143\u001b[0m        \u001b[35m3.6739\u001b[0m  10.7367\n",
      "      2        \u001b[36m0.5440\u001b[0m       0.6122        6.4117  10.5665\n",
      "      3        \u001b[36m0.5409\u001b[0m       0.6087        7.4368  10.7220\n",
      "      4        0.5412       0.5965       10.9599  10.8584\n",
      "      5        0.5429       0.6042       17.2614  10.6121\n",
      "      6        \u001b[36m0.5391\u001b[0m       0.5295       10.3879  10.6265\n",
      "      7        \u001b[36m0.5389\u001b[0m       0.5122       15.1436  10.6815\n",
      "      8        0.5398       0.6057       13.6698  10.7345\n",
      "      9        \u001b[36m0.5385\u001b[0m       0.5145       18.0218  10.7708\n",
      "     10        0.5414       0.6053       17.5432  10.6054\n",
      "     11        0.5405       0.6027       28.6988  10.5907\n",
      "     12        0.5417       0.4920       36.5807  10.8277\n",
      "     13        \u001b[36m0.5382\u001b[0m       0.4870       50.8458  10.6871\n",
      "     14        0.5390       0.4960       32.6197  10.6319\n",
      "     15        0.5417       0.4834       42.6524  10.6231\n",
      "     16        0.5429       0.5982       93.4645  10.6498\n",
      "     17        0.5428       0.6007       29.3716  10.8027\n",
      "     18        0.5450       0.6071       56.0643  10.7172\n",
      "     19        0.5416       0.6015       50.8050  10.6256\n",
      "     20        0.5443       0.5979       48.1841  10.6282\n",
      "     21        0.5407       0.5935      124.4512  10.8262\n",
      "     22        0.5523       0.5985       55.5975  10.7685\n",
      "     23        0.5412       0.4924       52.2945  10.6442\n",
      "     24        0.5445       0.4827       90.3213  10.6727\n",
      "     25        0.5427       0.5952       93.1469  10.7750\n",
      "     26        0.5450       0.4749      113.3044  10.7302\n",
      "     27        0.5440       0.5979       86.4720  10.6886\n",
      "     28        0.5429       0.6044       56.1353  10.6566\n",
      "     29        0.5429       0.4989       92.7540  10.6002\n",
      "     30        0.5419       0.4853       75.0949  10.6171\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5899\u001b[0m       \u001b[32m0.5339\u001b[0m        \u001b[35m0.7217\u001b[0m  10.6129\n",
      "      2        \u001b[36m0.5748\u001b[0m       0.5230        0.7519  10.5600\n",
      "      3        \u001b[36m0.5736\u001b[0m       0.4851        1.0757  10.5651\n",
      "      4        0.5736       0.5073        1.1277  10.7709\n",
      "      5        \u001b[36m0.5722\u001b[0m       0.4867        1.6169  10.6300\n",
      "      6        0.5727       0.5005        1.1169  10.5304\n",
      "      7        0.5736       0.4873        1.7186  10.7462\n",
      "      8        0.5725       0.4978        1.4611  10.7652\n",
      "      9        \u001b[36m0.5701\u001b[0m       0.4987        1.3429  10.6625\n",
      "     10        0.5757       0.4962        1.2925  10.4057\n",
      "     11        0.5745       0.4921        1.7328  10.6411\n",
      "     12        0.5736       0.4852        2.3363  10.8117\n",
      "     13        0.5723       0.4793        1.9955  10.7580\n",
      "     14        0.5787       0.5006        0.9613  10.6117\n",
      "     15        0.5761       0.4981        7.8085  10.5444\n",
      "     16        0.5798       0.4967        6.3114  10.7936\n",
      "     17        0.5762       0.5164        1.3156  10.8429\n",
      "     18        0.5746       0.4909        2.3358  10.5704\n",
      "     19        0.5770       0.4894        6.7818  10.6652\n",
      "     20        0.5775       0.5119        2.8413  10.6639\n",
      "     21        0.5791       0.5085        2.3787  10.6869\n",
      "     22        0.5749       0.4900        3.3252  10.6986\n",
      "     23        0.5786       0.5135        1.5171  10.6769\n",
      "     24        0.5773       0.4908        9.8803  10.5222\n",
      "     25        0.5758       0.4922        6.4448  10.7455\n",
      "     26        0.5752       0.4984       13.4681  10.8157\n",
      "     27        0.5755       0.4895       13.0358  10.6068\n",
      "     28        0.5768       0.5034        4.1848  10.7007\n",
      "     29        0.5776       0.4987        9.0216  10.8197\n",
      "     30        0.5848       0.5129        8.7019  10.7384\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5002\u001b[0m       \u001b[32m0.4594\u001b[0m        \u001b[35m0.7386\u001b[0m  10.5239\n",
      "      2        0.5002       0.4441        \u001b[35m0.7229\u001b[0m  10.7039\n",
      "      3        \u001b[36m0.5001\u001b[0m       0.4440        \u001b[35m0.7120\u001b[0m  10.7880\n",
      "      4        \u001b[36m0.4992\u001b[0m       0.4446        0.7284  10.7743\n",
      "      5        0.4993       0.4445        0.7321  10.5838\n",
      "      6        \u001b[36m0.4988\u001b[0m       0.4446        0.7343  10.6412\n",
      "      7        0.4997       0.4445        0.7437  10.7443\n",
      "      8        0.5005       0.4445        0.7429  10.7983\n",
      "      9        0.4993       0.4444        0.7315  10.7509\n",
      "     10        0.4989       0.4446        0.7405  10.6279\n",
      "     11        \u001b[36m0.4988\u001b[0m       0.4447        0.7198  10.6825\n",
      "     12        0.4989       0.4447        0.7192  10.8764\n",
      "     13        0.4994       0.4446        0.7190  10.5547\n",
      "     14        0.5002       0.4442        0.7477  10.6279\n",
      "     15        0.4992       0.4442        0.7244  10.5297\n",
      "     16        0.4993       0.4439        0.7470  10.7420\n",
      "     17        0.4994       0.4440        0.7241  10.8072\n",
      "     18        0.4988       0.4443        0.7251  10.6654\n",
      "     19        \u001b[36m0.4984\u001b[0m       0.4439        0.7149  10.6411\n",
      "     20        0.4992       0.4439        0.7248  10.7568\n",
      "     21        0.4987       0.4438        0.7138  10.6343\n",
      "     22        \u001b[36m0.4980\u001b[0m       0.4437        0.7294  10.4767\n",
      "     23        0.4984       0.4437        0.7313  10.6238\n",
      "     24        0.4989       0.4437        \u001b[35m0.7116\u001b[0m  10.7184\n",
      "     25        0.4992       0.4440        0.7247  10.7693\n",
      "     26        0.5000       0.4440        0.7524  10.7451\n",
      "     27        0.5003       \u001b[32m0.5378\u001b[0m        0.7202  10.7094\n",
      "     28        0.4998       0.4440        0.7380  10.6236\n",
      "     29        0.4986       0.4448        0.7320  10.7623\n",
      "     30        0.4988       0.4451        0.7156  10.7891\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5463\u001b[0m       \u001b[32m0.5279\u001b[0m        \u001b[35m5.9465\u001b[0m  10.7050\n",
      "      2        \u001b[36m0.5379\u001b[0m       \u001b[32m0.5449\u001b[0m        9.2371  10.6201\n",
      "      3        \u001b[36m0.5359\u001b[0m       0.4959        6.7935  10.8223\n",
      "      4        \u001b[36m0.5341\u001b[0m       0.5434       11.8698  10.6557\n",
      "      5        0.5353       0.5156       13.8146  10.5725\n",
      "      6        0.5343       0.5002       23.9507  10.6386\n",
      "      7        \u001b[36m0.5335\u001b[0m       0.4783       11.9684  10.7226\n",
      "      8        \u001b[36m0.5316\u001b[0m       0.4848       26.6825  10.6930\n",
      "      9        0.5355       0.4731       13.7310  10.6954\n",
      "     10        0.5375       0.4659       25.7986  10.6041\n",
      "     11        0.5370       0.4700       23.3114  10.7317\n",
      "     12        0.5378       0.4938       24.8663  10.7945\n",
      "     13        0.5390       0.4598       27.9045  10.5688\n",
      "     14        0.5388       0.4937       24.6031  10.6046\n",
      "     15        0.5384       0.4861       18.7914  10.7510\n",
      "     16        0.5380       0.4698       29.4075  10.6679\n",
      "     17        0.5359       0.4707       43.8410  10.7169\n",
      "     18        0.5380       0.4745       20.4544  10.6042\n",
      "     19        0.5377       0.4786       30.9632  10.6704\n",
      "     20        0.5371       0.4991       35.7357  10.6738\n",
      "     21        0.5368       0.4626       52.1557  10.7844\n",
      "     22        0.5365       0.4847       28.6158  10.6961\n",
      "     23        0.5367       0.4555       35.3352  10.7368\n",
      "     24        0.5357       0.4778       58.7746  10.7815\n",
      "     25        0.5365       0.4797       36.4446  10.7821\n",
      "     26        0.5358       0.4736       73.8646  10.7039\n",
      "     27        0.5356       0.4758       50.1507  10.6260\n",
      "     28        0.5363       0.4671       36.2089  10.7087\n",
      "     29        0.5359       0.4729       60.7939  10.8313\n",
      "     30        0.5361       0.4560       36.4685  10.6712\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5870\u001b[0m       \u001b[32m0.5053\u001b[0m        \u001b[35m0.7100\u001b[0m  10.6384\n",
      "      2        \u001b[36m0.5742\u001b[0m       0.4733        0.7638  10.7401\n",
      "      3        0.5752       \u001b[32m0.5556\u001b[0m        \u001b[35m0.6909\u001b[0m  10.7599\n",
      "      4        0.5744       \u001b[32m0.5815\u001b[0m        \u001b[35m0.6842\u001b[0m  10.8197\n",
      "      5        0.5782       0.5561        0.6911  10.5727\n",
      "      6        0.5820       0.4603        0.7071  10.7337\n",
      "      7        0.5788       0.4659        0.7092  10.6877\n",
      "      8        0.5761       0.4533        0.7188  10.7589\n",
      "      9        0.5766       0.4488        0.6976  10.5509\n",
      "     10        0.5754       0.4652        0.7055  10.6313\n",
      "     11        0.5747       0.4468        0.6998  10.7461\n",
      "     12        0.5745       0.4713        0.7019  10.7023\n",
      "     13        0.5744       0.4514        0.7171  10.4197\n",
      "     14        0.5751       0.4755        0.7024  10.6170\n",
      "     15        \u001b[36m0.5714\u001b[0m       0.4948        0.6938  10.7189\n",
      "     16        0.5735       0.4751        0.7029  10.7397\n",
      "     17        0.5732       0.4552        0.7059  10.6967\n",
      "     18        0.5736       0.4536        0.7055  10.6436\n",
      "     19        \u001b[36m0.5704\u001b[0m       0.4688        0.6910  10.7456\n",
      "     20        0.5715       0.4716        0.7028  10.7818\n",
      "     21        0.5709       0.4812        0.6947  10.6806\n",
      "     22        0.5742       0.4780        0.6910  10.6727\n",
      "     23        0.5743       0.5075        \u001b[35m0.6834\u001b[0m  10.8034\n",
      "     24        0.5758       0.4598        0.7009  10.7932\n",
      "     25        0.5781       0.4578        0.7002  10.8002\n",
      "     26        0.5779       0.4639        0.6911  10.7723\n",
      "     27        0.5741       0.4660        0.6981  10.7217\n",
      "     28        0.5730       0.4595        0.7026  10.6654\n",
      "     29        0.5755       0.4490        0.6940  10.8004\n",
      "     30        0.5735       0.4525        0.6988  10.6040\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4916\u001b[0m       \u001b[32m0.4438\u001b[0m        \u001b[35m0.7291\u001b[0m  10.6478\n",
      "      2        0.4935       \u001b[32m0.4438\u001b[0m        \u001b[35m0.7122\u001b[0m  10.8995\n",
      "      3        0.4931       \u001b[32m0.4439\u001b[0m        0.7471  10.7367\n",
      "      4        0.4944       0.4438        0.7199  10.5711\n",
      "      5        0.4936       \u001b[32m0.4445\u001b[0m        0.7491  10.6046\n",
      "      6        0.4947       0.4441        0.7272  10.7509\n",
      "      7        0.4951       0.4439        \u001b[35m0.6938\u001b[0m  10.7013\n",
      "      8        0.4925       0.4439        0.7213  10.5992\n",
      "      9        0.4923       0.4439        0.7000  10.5691\n",
      "     10        0.4926       0.4439        0.7206  10.7126\n",
      "     11        0.4937       0.4437        0.7128  10.7991\n",
      "     12        0.4920       0.4438        0.7110  10.6834\n",
      "     13        0.4940       \u001b[32m0.4445\u001b[0m        0.6986  10.5697\n",
      "     14        0.4923       0.4438        0.7163  10.7108\n",
      "     15        0.4925       0.4438        0.7336  10.8374\n",
      "     16        0.4951       \u001b[32m0.4466\u001b[0m        0.9367  10.8469\n",
      "     17        0.4921       \u001b[32m0.4467\u001b[0m        1.0206  10.7437\n",
      "     18        0.4928       0.4465        1.0318  11.8157\n",
      "     19        0.4932       0.4442        0.7198  12.3971\n",
      "     20        0.4925       0.4441        0.7076  12.4104\n",
      "     21        0.4931       0.4438        0.7272  12.4420\n",
      "     22        0.4926       0.4439        0.7623  12.4533\n",
      "     23        \u001b[36m0.4915\u001b[0m       0.4455        0.8678  12.4527\n",
      "     24        \u001b[36m0.4915\u001b[0m       0.4453        0.8865  12.4725\n",
      "     25        0.4925       0.4453        0.8776  12.4306\n",
      "     26        0.4947       0.4451        0.8655  12.3953\n",
      "     27        0.4934       0.4449        0.8393  12.4060\n",
      "     28        0.4921       0.4450        0.9223  12.4837\n",
      "     29        0.4919       0.4455        0.9494  12.4122\n",
      "     30        0.4933       0.4455        0.9961  12.4933\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5457\u001b[0m       \u001b[32m0.5498\u001b[0m        \u001b[35m5.4312\u001b[0m  12.6423\n",
      "      2        \u001b[36m0.5386\u001b[0m       \u001b[32m0.6047\u001b[0m        \u001b[35m3.5839\u001b[0m  12.4756\n",
      "      3        \u001b[36m0.5365\u001b[0m       0.5040       11.8095  12.4855\n",
      "      4        0.5369       0.5127       16.3490  12.4616\n",
      "      5        \u001b[36m0.5343\u001b[0m       0.5198       14.5619  12.4683\n",
      "      6        0.5366       \u001b[32m0.6072\u001b[0m       15.7335  12.4563\n",
      "      7        0.5345       0.4764       23.4916  12.4784\n",
      "      8        0.5355       0.5053       33.3599  12.4821\n",
      "      9        \u001b[36m0.5337\u001b[0m       0.4681       26.3656  12.5029\n",
      "     10        0.5367       0.4847       61.9322  12.4400\n",
      "     11        0.5399       0.5991       40.3331  12.4666\n",
      "     12        0.5368       0.4599       72.2489  12.5236\n",
      "     13        0.5354       0.4830       83.1394  12.5043\n",
      "     14        0.5353       0.4648       66.0684  12.4857\n",
      "     15        \u001b[36m0.5336\u001b[0m       0.4745       53.6053  12.4397\n",
      "     16        0.5347       0.4671       42.9679  12.4802\n",
      "     17        0.5348       0.4752       67.1208  12.4957\n",
      "     18        0.5368       0.5974       84.0891  12.5009\n",
      "     19        0.5377       0.4818      105.0718  12.4676\n",
      "     20        0.5363       0.6003       91.9843  12.4570\n",
      "     21        0.5352       0.4755      123.0246  12.4756\n",
      "     22        0.5369       0.5015      112.4168  12.4606\n",
      "     23        0.5363       0.4823      132.3765  12.5447\n",
      "     24        0.5347       0.4760      126.5214  12.4942\n",
      "     25        0.5355       0.4957      165.9457  12.4980\n",
      "     26        0.5342       0.4499       94.5878  12.5176\n",
      "     27        0.5365       0.4711      156.8368  12.4466\n",
      "     28        0.5338       0.4818      173.8198  12.4589\n",
      "     29        0.5353       0.4757      154.6961  12.4944\n",
      "     30        0.5345       0.4485      107.9524  12.4778\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5915\u001b[0m       \u001b[32m0.5617\u001b[0m        \u001b[35m0.6836\u001b[0m  12.5777\n",
      "      2        \u001b[36m0.5761\u001b[0m       0.4979        0.6950  12.4755\n",
      "      3        0.5762       0.4637        0.7076  12.4484\n",
      "      4        0.5776       0.5526        0.6880  12.5079\n",
      "      5        0.5782       0.5543        0.6880  12.4828\n",
      "      6        \u001b[36m0.5759\u001b[0m       0.5590        \u001b[35m0.6720\u001b[0m  12.4661\n",
      "      7        \u001b[36m0.5748\u001b[0m       0.5580        0.6732  12.4807\n",
      "      8        0.5774       0.5551        0.6779  12.4314\n",
      "      9        0.5766       0.5549        0.6743  12.4708\n",
      "     10        0.5757       0.5369        0.6881  12.4222\n",
      "     11        0.5761       0.5186        0.6837  12.4437\n",
      "     12        0.5754       0.5395        0.6747  12.4108\n",
      "     13        0.5785       \u001b[32m0.5645\u001b[0m        \u001b[35m0.6697\u001b[0m  12.4385\n",
      "     14        0.5806       0.5225        0.6870  12.4391\n",
      "     15        0.5780       0.5561        \u001b[35m0.6697\u001b[0m  12.4489\n",
      "     16        0.5766       0.5507        \u001b[35m0.6690\u001b[0m  12.4597\n",
      "     17        0.5766       0.5171        0.6818  12.4217\n",
      "     18        0.5776       0.5563        0.6716  12.4610\n",
      "     19        0.5767       \u001b[32m0.5877\u001b[0m        \u001b[35m0.6651\u001b[0m  12.4198\n",
      "     20        0.5776       0.5273        0.6750  12.4362\n",
      "     21        0.5772       0.5537        0.6777  12.4389\n",
      "     22        0.5798       0.5563        0.6696  12.3718\n",
      "     23        0.5811       0.5566        0.6678  12.3953\n",
      "     24        0.5801       0.5414        0.6716  12.4494\n",
      "     25        0.5821       0.5512        0.6705  12.4469\n",
      "     26        0.5861       0.5316        0.6844  12.4917\n",
      "     27        0.5860       0.5563        0.6711  12.4268\n",
      "     28        0.5846       0.5697        0.6701  12.4461\n",
      "     29        0.5849       0.5624        0.6670  12.4424\n",
      "     30        0.5844       0.5707        0.6679  11.4249\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4935\u001b[0m       \u001b[32m0.4445\u001b[0m        \u001b[35m0.7353\u001b[0m  10.6610\n",
      "      2        0.4979       0.4441        0.7636  10.5026\n",
      "      3        0.4986       \u001b[32m0.4448\u001b[0m        0.7364  10.6629\n",
      "      4        0.4991       0.4439        \u001b[35m0.7292\u001b[0m  10.6752\n",
      "      5        0.4985       0.4437        \u001b[35m0.7257\u001b[0m  10.4498\n",
      "      6        0.4983       \u001b[32m0.4451\u001b[0m        \u001b[35m0.7240\u001b[0m  10.3850\n",
      "      7        0.5001       0.4441        0.7780  10.6119\n",
      "      8        0.4981       0.4444        0.7448  10.6669\n",
      "      9        0.4980       0.4448        0.7485  10.5725\n",
      "     10        0.4981       \u001b[32m0.4456\u001b[0m        0.7657  10.5771\n",
      "     11        0.4980       \u001b[32m0.4457\u001b[0m        0.7831  10.4973\n",
      "     12        0.4980       0.4445        0.7506  10.6386\n",
      "     13        0.4980       0.4446        0.7465  10.4619\n",
      "     14        0.4979       0.4446        0.7604  10.6569\n",
      "     15        0.4989       0.4448        0.7687  10.6268\n",
      "     16        0.4980       0.4453        0.7946  10.4550\n",
      "     17        0.4986       0.4451        0.8000  10.5892\n",
      "     18        0.4981       0.4452        0.8086  10.6898\n",
      "     19        0.4981       0.4451        0.7917  10.6733\n",
      "     20        0.4980       0.4451        0.7840  10.5813\n",
      "     21        0.4980       \u001b[32m0.4459\u001b[0m        0.8927  10.5250\n",
      "     22        0.4992       \u001b[32m0.4460\u001b[0m        0.8627  10.6015\n",
      "     23        0.4987       0.4455        0.7569  10.5438\n",
      "     24        0.4982       0.4457        0.7768  10.5822\n",
      "     25        0.4980       0.4456        0.7845  10.6641\n",
      "     26        0.4978       0.4451        0.7778  10.6584\n",
      "     27        0.4980       0.4451        0.8086  10.6613\n",
      "     28        0.4979       0.4451        0.7958  10.6788\n",
      "     29        0.4980       0.4451        0.7840  10.6200\n",
      "     30        0.4980       0.4451        0.8009  10.5120\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5563\u001b[0m       \u001b[32m0.5157\u001b[0m        \u001b[35m4.1454\u001b[0m  10.7304\n",
      "      2        \u001b[36m0.5477\u001b[0m       \u001b[32m0.5305\u001b[0m        5.5319  10.6749\n",
      "      3        \u001b[36m0.5444\u001b[0m       0.4807        6.4580  10.4618\n",
      "      4        \u001b[36m0.5430\u001b[0m       0.4800        8.3767  10.5851\n",
      "      5        \u001b[36m0.5424\u001b[0m       \u001b[32m0.5626\u001b[0m        8.5073  10.6357\n",
      "      6        0.5432       0.5095       10.8818  10.6094\n",
      "      7        0.5434       0.5171       12.2040  10.5889\n",
      "      8        0.5458       0.4691        8.2846  10.6313\n",
      "      9        0.5435       0.4785       23.1672  10.5785\n",
      "     10        0.5468       0.4485       14.1023  10.4943\n",
      "     11        0.5447       0.4731       13.4967  10.5477\n",
      "     12        0.5443       0.4720       13.3852  10.6582\n",
      "     13        0.5434       0.4672       16.1914  10.5689\n",
      "     14        0.5453       0.5155       14.2593  10.6355\n",
      "     15        0.5494       0.4821       21.8964  10.5759\n",
      "     16        0.5514       0.4474       22.4026  10.4085\n",
      "     17        0.5493       0.4879       14.0429  10.5690\n",
      "     18        0.5478       0.4684       20.8454  10.6166\n",
      "     19        0.5473       0.5118       24.2881  10.6094\n",
      "     20        0.5482       0.4807       25.1864  10.6386\n",
      "     21        0.5494       0.4733       15.1999  10.4746\n",
      "     22        0.5505       0.4760       11.2951  10.6744\n",
      "     23        0.5504       0.4702       11.6657  10.5126\n",
      "     24        0.5499       0.4728       16.8764  10.4014\n",
      "     25        0.5494       0.4757       15.1893  10.5958\n",
      "     26        0.5502       0.4765       18.1169  10.7353\n",
      "     27        0.5509       0.4765       20.5056  10.6039\n",
      "     28        0.5518       0.4848       14.0341  10.7098\n",
      "     29        0.5490       0.4797       14.5669  10.4482\n",
      "     30        0.5505       0.4893       16.1354  10.5726\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5908\u001b[0m       \u001b[32m0.5824\u001b[0m        \u001b[35m0.6694\u001b[0m  10.5861\n",
      "      2        \u001b[36m0.5815\u001b[0m       \u001b[32m0.5834\u001b[0m        0.6810  10.5296\n",
      "      3        \u001b[36m0.5811\u001b[0m       \u001b[32m0.5897\u001b[0m        0.6975  10.4580\n",
      "      4        0.5825       0.5631        0.6752  10.6317\n",
      "      5        0.5821       0.5437        0.7020  10.6988\n",
      "      6        \u001b[36m0.5798\u001b[0m       0.5599        0.6772  10.6369\n",
      "      7        0.5827       0.5347        0.6790  10.6225\n",
      "      8        0.5800       0.5876        \u001b[35m0.6669\u001b[0m  10.6401\n",
      "      9        \u001b[36m0.5782\u001b[0m       0.5517        0.6805  10.5433\n",
      "     10        \u001b[36m0.5777\u001b[0m       0.5503        0.7235  10.6351\n",
      "     11        \u001b[36m0.5767\u001b[0m       0.5562        0.6677  10.4152\n",
      "     12        0.5770       0.5826        0.6762  10.5279\n",
      "     13        \u001b[36m0.5763\u001b[0m       0.5486        0.6694  10.5817\n",
      "     14        0.5764       0.5582        0.6719  10.5773\n",
      "     15        0.5777       0.5689        0.6690  10.7163\n",
      "     16        0.5795       0.5862        \u001b[35m0.6607\u001b[0m  10.6123\n",
      "     17        0.5776       0.5483        0.6818  10.5336\n",
      "     18        0.5823       0.5563        0.6688  10.5887\n",
      "     19        0.5846       0.5674        0.6675  10.5917\n",
      "     20        0.5907       0.5720        0.6649  10.5984\n",
      "     21        0.5880       0.5563        0.6660  10.6228\n",
      "     22        0.5857       0.5620        0.6695  10.6192\n",
      "     23        0.5863       0.5563        0.6738  10.5612\n",
      "     24        0.5880       0.5689        0.6696  10.6509\n",
      "     25        0.5857       0.5563        0.6693  10.6444\n",
      "     26        0.5864       0.5719        0.6685  10.6098\n",
      "     27        0.5854       0.5521        0.6770  10.6293\n",
      "     28        0.5876       0.5566        0.6698  10.6116\n",
      "     29        0.5928       0.5563        0.6667  10.5644\n",
      "     30        0.5935       0.5563        0.6716  10.6464\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4921\u001b[0m       \u001b[32m0.4441\u001b[0m        \u001b[35m0.7242\u001b[0m  10.7587\n",
      "      2        \u001b[36m0.4897\u001b[0m       \u001b[32m0.5936\u001b[0m        \u001b[35m0.7108\u001b[0m  10.5764\n",
      "      3        0.4924       0.5913        \u001b[35m0.7087\u001b[0m  10.4703\n",
      "      4        0.4909       0.4437        0.7468  10.4271\n",
      "      5        0.4924       0.4437        0.7260  10.5922\n",
      "      6        0.4960       0.4437        0.7310  10.6081\n",
      "      7        0.4959       0.4437        0.7349  10.6278\n",
      "      8        0.5012       0.4439        0.7326  10.5107\n",
      "      9        0.5016       0.4438        0.7314  10.5932\n",
      "     10        0.5012       0.4448        0.7447  10.5883\n",
      "     11        0.5020       0.4447        0.7417  10.5928\n",
      "     12        0.5012       0.4448        0.7245  10.6660\n",
      "     13        0.5013       0.4460        0.7634  10.5425\n",
      "     14        0.5009       0.4460        0.7700  10.6793\n",
      "     15        0.5011       0.4460        0.7857  10.5012\n",
      "     16        0.5013       0.4460        0.7819  10.5864\n",
      "     17        0.5013       0.4459        0.7917  10.4172\n",
      "     18        0.5011       0.4460        0.7629  10.5865\n",
      "     19        0.5028       0.4454        0.7656  10.5232\n",
      "     20        0.5025       0.4439        0.7290  10.5694\n",
      "     21        0.5013       0.4439        0.7306  10.5637\n",
      "     22        0.5012       0.4439        0.7394  10.5770\n",
      "     23        0.5010       0.4448        0.7485  10.6531\n",
      "     24        0.5015       0.4448        0.7427  10.4873\n",
      "     25        0.5013       0.4448        0.7593  10.6758\n",
      "     26        0.5013       0.4448        0.7424  10.3953\n",
      "     27        0.5012       0.4448        0.7464  10.5241\n",
      "     28        0.5020       0.4450        0.8273  10.5273\n",
      "     29        0.5016       0.4442        0.7456  10.5954\n",
      "     30        0.5014       0.4442        0.7419  10.7237\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5360\u001b[0m       \u001b[32m0.4521\u001b[0m        \u001b[35m5.9497\u001b[0m  10.7791\n",
      "      2        \u001b[36m0.5256\u001b[0m       \u001b[32m0.4631\u001b[0m       10.3593  10.8212\n",
      "      3        \u001b[36m0.5217\u001b[0m       0.4545       10.6871  10.7272\n",
      "      4        0.5270       0.4546       19.6125  10.7207\n",
      "      5        0.5245       0.4612       21.0644  10.7172\n",
      "      6        0.5231       0.4626       20.0805  10.7760\n",
      "      7        0.5231       0.4612       32.8594  10.8361\n",
      "      8        0.5244       0.4506       28.5424  10.7693\n",
      "      9        0.5230       \u001b[32m0.4646\u001b[0m       20.2123  10.8021\n",
      "     10        0.5253       \u001b[32m0.4716\u001b[0m       29.6205  10.7244\n",
      "     11        0.5233       0.4531       31.4882  10.8059\n",
      "     12        0.5223       0.4522       45.7017  10.6803\n",
      "     13        \u001b[36m0.5216\u001b[0m       0.4430       51.1301  10.7920\n",
      "     14        0.5231       0.4649       40.0478  10.6093\n",
      "     15        0.5381       \u001b[32m0.4739\u001b[0m      146.3382  10.7515\n",
      "     16        0.5335       0.4146       50.4980  10.7808\n",
      "     17        0.5287       0.4610       68.4269  10.7821\n",
      "     18        0.5297       0.4571       53.7486  10.7675\n",
      "     19        0.5262       0.4640       89.1459  10.7095\n",
      "     20        0.5274       0.4070       57.4677  10.8259\n",
      "     21        0.5278       0.4593       71.8501  10.7227\n",
      "     22        0.5291       \u001b[32m0.4805\u001b[0m       82.5313  10.7205\n",
      "     23        0.5266       0.4768      222.8804  10.7676\n",
      "     24        0.5307       0.4728      121.8280  10.7146\n",
      "     25        0.5258       0.4159      142.6115  10.8940\n",
      "     26        0.5317       0.4532      168.0959  10.6581\n",
      "     27        0.5308       0.4583      118.7956  10.7453\n",
      "     28        0.5289       0.4564      193.3954  10.8253\n",
      "     29        0.5296       0.4651      148.1137  10.7389\n",
      "     30        0.5271       0.4250      106.5762  10.8149\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5656\u001b[0m       \u001b[32m0.5455\u001b[0m        \u001b[35m1.0055\u001b[0m  10.7157\n",
      "      2        \u001b[36m0.5519\u001b[0m       0.5100        2.2168  10.7729\n",
      "      3        \u001b[36m0.5465\u001b[0m       0.5027        4.0232  10.7791\n",
      "      4        \u001b[36m0.5451\u001b[0m       0.5322        2.5999  10.7018\n",
      "      5        \u001b[36m0.5445\u001b[0m       0.5305        2.2444  10.7723\n",
      "      6        0.5471       0.5239        2.6582  10.7237\n",
      "      7        0.5511       \u001b[32m0.5512\u001b[0m        4.2565  10.6898\n",
      "      8        0.5493       0.5392        3.7803  10.8823\n",
      "      9        0.5504       0.5249        6.7292  10.7697\n",
      "     10        0.5503       0.5325        4.4890  10.8418\n",
      "     11        0.5500       0.5154        7.9951  10.7031\n",
      "     12        0.5517       0.5226       10.1653  10.7697\n",
      "     13        0.5480       0.5292        8.7614  10.7511\n",
      "     14        0.5470       0.5353        6.1171  10.6810\n",
      "     15        0.5494       0.5189       17.1416  10.8420\n",
      "     16        0.5579       0.5247        7.2466  10.7246\n",
      "     17        0.5536       0.5021        8.5327  10.7445\n",
      "     18        0.5563       0.5299        2.5233  10.7197\n",
      "     19        0.5553       0.5173        9.8825  10.7563\n",
      "     20        0.5533       0.5087       10.0509  10.8450\n",
      "     21        0.5580       0.5308        7.5460  10.7263\n",
      "     22        0.5550       0.5373       10.1179  10.8287\n",
      "     23        0.5523       0.5410       15.8252  10.6655\n",
      "     24        0.5509       0.5253       24.3360  10.7533\n",
      "     25        0.5626       0.5281       36.0613  10.6765\n",
      "     26        0.5660       0.5121       22.4409  10.8112\n",
      "     27        0.5672       0.5269       26.4283  10.7342\n",
      "     28        0.5662       0.5488       33.1481  10.7254\n",
      "     29        0.5651       0.5451       40.5951  10.8138\n",
      "     30        0.5632       0.5343       70.2091  10.6494\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4912\u001b[0m       \u001b[32m0.4443\u001b[0m        \u001b[35m0.7242\u001b[0m  10.6430\n",
      "      2        \u001b[36m0.4894\u001b[0m       0.4438        0.7405  10.7265\n",
      "      3        \u001b[36m0.4877\u001b[0m       \u001b[32m0.4444\u001b[0m        \u001b[35m0.7168\u001b[0m  10.9789\n",
      "      4        0.4885       \u001b[32m0.4447\u001b[0m        \u001b[35m0.7149\u001b[0m  10.6063\n",
      "      5        0.4882       0.4444        \u001b[35m0.7148\u001b[0m  10.7545\n",
      "      6        0.4878       0.4439        0.7218  10.6310\n",
      "      7        0.4881       \u001b[32m0.5691\u001b[0m        \u001b[35m0.7109\u001b[0m  10.7879\n",
      "      8        0.4882       0.4448        0.7294  10.7149\n",
      "      9        \u001b[36m0.4875\u001b[0m       0.5428        0.7240  10.8372\n",
      "     10        0.4888       0.5298        0.7229  10.7024\n",
      "     11        \u001b[36m0.4863\u001b[0m       0.4458        0.7815  10.6805\n",
      "     12        0.4909       0.5618        \u001b[35m0.7093\u001b[0m  10.8223\n",
      "     13        0.4878       0.4441        0.7342  10.7166\n",
      "     14        0.4867       0.5450        0.7122  10.8278\n",
      "     15        \u001b[36m0.4858\u001b[0m       \u001b[32m0.5791\u001b[0m        0.7116  10.6606\n",
      "     16        0.4883       0.4436        0.7770  10.7728\n",
      "     17        0.4874       0.5036        0.7293  10.6808\n",
      "     18        0.4891       0.4438        0.7715  10.7936\n",
      "     19        0.4896       0.4684        0.7328  10.6761\n",
      "     20        0.4894       0.4448        0.7702  10.7425\n",
      "     21        0.4863       0.5385        0.7335  10.7000\n",
      "     22        0.4865       0.4440        0.7234  10.8068\n",
      "     23        0.4859       0.5677        0.8235  10.7576\n",
      "     24        0.4863       0.5477        0.8416  10.8974\n",
      "     25        0.4895       0.5291        0.8348  10.8739\n",
      "     26        0.4897       0.4442        0.8190  10.7014\n",
      "     27        0.4868       0.5573        0.8001  10.7161\n",
      "     28        0.4875       0.4451        0.9582  10.7429\n",
      "     29        0.4871       0.4449        0.8390  10.7286\n",
      "     30        0.4879       0.4452        0.9048  10.8307\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5363\u001b[0m       \u001b[32m0.5426\u001b[0m        \u001b[35m4.6577\u001b[0m  10.7215\n",
      "      2        \u001b[36m0.5252\u001b[0m       0.4921       13.3312  10.8189\n",
      "      3        \u001b[36m0.5251\u001b[0m       0.4569       16.2390  10.8256\n",
      "      4        \u001b[36m0.5244\u001b[0m       0.4684       25.5308  10.6721\n",
      "      5        \u001b[36m0.5214\u001b[0m       0.4662       45.3197  10.7776\n",
      "      6        0.5233       0.4644       54.7912  10.7013\n",
      "      7        0.5232       0.4838       78.6322  10.6678\n",
      "      8        0.5248       0.4680       62.7278  10.8834\n",
      "      9        0.5227       0.4793       69.6271  10.5809\n",
      "     10        0.5237       0.4661       69.6372  10.7806\n",
      "     11        0.5218       0.4647       70.2233  10.7305\n",
      "     12        0.5253       0.4571       49.3362  10.7047\n",
      "     13        0.5238       0.4660       53.1750  10.9485\n",
      "     14        \u001b[36m0.5207\u001b[0m       0.4584       83.8182  10.7387\n",
      "     15        0.5233       0.4496       67.9361  10.8565\n",
      "     16        0.5215       0.4956      125.0928  10.6964\n",
      "     17        0.5230       0.4717      185.4803  10.7773\n",
      "     18        0.5233       0.4660      169.9270  10.7170\n",
      "     19        0.5248       0.4713      135.5329  10.8132\n",
      "     20        \u001b[36m0.5205\u001b[0m       0.4801      173.8150  10.6169\n",
      "     21        0.5216       0.4508      167.9874  10.7004\n",
      "     22        0.5220       0.4322      156.7825  10.6284\n",
      "     23        0.5221       0.4474      235.6673  10.7481\n",
      "     24        \u001b[36m0.5205\u001b[0m       0.4555      153.0759  10.7907\n",
      "     25        0.5224       0.4432      289.1484  10.7103\n",
      "     26        0.5211       0.4640      209.6060  10.8819\n",
      "     27        0.5216       0.4448      234.7508  10.7478\n",
      "     28        0.5259       0.4573      149.4483  10.8551\n",
      "     29        0.5238       0.4661      264.0967  10.7393\n",
      "     30        0.5223       0.4543      281.8646  10.7829\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5693\u001b[0m       \u001b[32m0.5917\u001b[0m        \u001b[35m0.6731\u001b[0m  10.8020\n",
      "      2        \u001b[36m0.5591\u001b[0m       0.5725        0.6760  10.6506\n",
      "      3        0.5606       0.5577        0.6921  10.7277\n",
      "      4        0.5601       0.5873        0.6812  10.6782\n",
      "      5        \u001b[36m0.5577\u001b[0m       0.5683        0.6892  10.7452\n",
      "      6        0.5581       0.5394        0.6789  10.8431\n",
      "      7        0.5580       0.5574        0.6763  10.7316\n",
      "      8        0.5578       0.5802        \u001b[35m0.6663\u001b[0m  10.8459\n",
      "      9        0.5583       0.5740        0.6680  10.6551\n",
      "     10        0.5590       0.5651        0.6745  10.7858\n",
      "     11        0.5606       0.5850        \u001b[35m0.6649\u001b[0m  10.7199\n",
      "     12        0.5597       0.5458        0.7409  10.6792\n",
      "     13        \u001b[36m0.5541\u001b[0m       0.5508        0.7089  10.7943\n",
      "     14        0.5561       0.5594        0.6828  10.6676\n",
      "     15        0.5541       0.5490        0.6707  10.7638\n",
      "     16        0.5636       0.5466        0.6715  10.9238\n",
      "     17        0.5654       0.5214        0.7012  10.8948\n",
      "     18        0.5615       0.5717        0.6678  10.6994\n",
      "     19        0.5599       0.5457        0.6764  10.7332\n",
      "     20        0.5637       0.5597        0.6731  10.8263\n",
      "     21        0.5612       0.5563        0.6708  10.5352\n",
      "     22        0.5629       0.5562        0.6733  10.8391\n",
      "     23        0.5626       0.5433        0.6740  10.6678\n",
      "     24        0.5630       0.5654        0.6776  10.8148\n",
      "     25        0.5621       0.5749        0.6673  10.6950\n",
      "     26        0.5630       0.5613        0.6880  10.7531\n",
      "     27        0.5638       0.5654        0.6790  10.6701\n",
      "     28        0.5615       0.5826        0.6673  10.6375\n",
      "     29        0.5639       0.5841        0.6770  10.8570\n",
      "     30        0.5674       0.5860        \u001b[35m0.6648\u001b[0m  10.7566\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4792\u001b[0m       \u001b[32m0.5773\u001b[0m        \u001b[35m0.7024\u001b[0m  10.6346\n",
      "      2        \u001b[36m0.4775\u001b[0m       0.5416        0.7340  10.8389\n",
      "      3        0.4802       0.4443        0.7365  10.9559\n",
      "      4        0.4807       0.4443        0.7300  10.7294\n",
      "      5        0.4825       0.5460        0.7665  10.8319\n",
      "      6        0.4815       0.5760        0.7042  10.7125\n",
      "      7        0.4812       0.5430        0.7311  10.7440\n",
      "      8        0.4799       0.5652        0.7365  10.7812\n",
      "      9        0.4817       0.4437        0.7432  10.6666\n",
      "     10        0.4864       0.4442        0.7948  10.8122\n",
      "     11        0.4854       0.4439        0.7584  10.6556\n",
      "     12        0.4858       0.4438        0.7463  10.8348\n",
      "     13        0.4855       0.4438        0.7539  10.6797\n",
      "     14        0.4878       0.4440        0.7722  10.7343\n",
      "     15        0.4856       0.5626        0.7501  10.6154\n",
      "     16        0.4868       0.5499        0.7463  10.8272\n",
      "     17        0.4862       0.5447        0.7378  10.6567\n",
      "     18        0.4863       0.4449        0.7610  10.7218\n",
      "     19        0.4850       0.4447        0.7347  10.6757\n",
      "     20        0.4853       0.5524        0.8729  10.6268\n",
      "     21        0.4867       0.5462        0.8497  10.8128\n",
      "     22        0.4847       0.5595        0.8773  10.6878\n",
      "     23        0.4845       0.4464        0.8840  10.7346\n",
      "     24        0.4852       0.4462        0.9780  10.6623\n",
      "     25        0.4854       0.5753        0.9079  10.7616\n",
      "     26        0.4878       0.4451        0.7294  10.6416\n",
      "     27        0.4850       0.4450        0.8113  10.7218\n",
      "     28        0.4845       0.4449        0.7417  10.6012\n",
      "     29        0.4860       0.4459        0.8632  10.7369\n",
      "     30        0.4886       0.5609        0.7701  10.6856\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5361\u001b[0m       \u001b[32m0.4836\u001b[0m        \u001b[35m9.2756\u001b[0m  10.6561\n",
      "      2        \u001b[36m0.5253\u001b[0m       0.4778       15.8467  10.7383\n",
      "      3        0.5259       0.4578       31.1360  10.8786\n",
      "      4        \u001b[36m0.5237\u001b[0m       0.4624       37.0496  10.6996\n",
      "      5        0.5264       0.4654       41.5096  10.8842\n",
      "      6        \u001b[36m0.5211\u001b[0m       0.4570       77.1319  10.6687\n",
      "      7        0.5234       0.4827       84.2152  10.7803\n",
      "      8        0.5228       0.4719       62.3670  10.6271\n",
      "      9        0.5228       0.4643       80.4073  10.6853\n",
      "     10        0.5245       0.4525      102.2408  10.7235\n",
      "     11        0.5220       0.4596      117.5575  10.6922\n",
      "     12        0.5218       0.4744      136.1816  10.8529\n",
      "     13        \u001b[36m0.5197\u001b[0m       0.4683      217.0153  10.7398\n",
      "     14        0.5229       0.4565      144.4954  10.7896\n",
      "     15        0.5216       0.4553       93.7322  10.6906\n",
      "     16        0.5197       0.4528       88.8501  10.7237\n",
      "     17        0.5198       0.4597      169.6047  10.7264\n",
      "     18        0.5201       0.4694      207.1490  10.7864\n",
      "     19        0.5217       0.4618      187.1576  10.6718\n",
      "     20        0.5211       0.4580      260.3897  10.6211\n",
      "     21        0.5329       0.4672      278.6679  10.8547\n",
      "     22        0.5258       0.4450      198.5690  10.5807\n",
      "     23        0.5198       0.4559      209.3692  10.7589\n",
      "     24        0.5219       0.4729      311.1895  10.6906\n",
      "     25        0.5218       0.4659      243.1976  10.7698\n",
      "     26        0.5244       0.4548      242.3849  10.6623\n",
      "     27        0.5203       0.4691      313.6127  10.7874\n",
      "     28        0.5255       0.4523      360.9035  10.7236\n",
      "     29        0.5207       0.4675      351.3309  10.7806\n",
      "     30        \u001b[36m0.5186\u001b[0m       0.4582      347.0106  10.6277\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5666\u001b[0m       \u001b[32m0.5217\u001b[0m        \u001b[35m1.4387\u001b[0m  10.6554\n",
      "      2        \u001b[36m0.5533\u001b[0m       \u001b[32m0.5470\u001b[0m        \u001b[35m1.3277\u001b[0m  10.7720\n",
      "      3        \u001b[36m0.5524\u001b[0m       0.5392        1.7949  10.7632\n",
      "      4        \u001b[36m0.5500\u001b[0m       \u001b[32m0.5593\u001b[0m        1.4818  10.7301\n",
      "      5        0.5523       \u001b[32m0.5808\u001b[0m        1.8388  10.6683\n",
      "      6        0.5570       0.5364        4.7886  10.7401\n",
      "      7        0.5522       0.5423        4.8205  10.9117\n",
      "      8        0.5532       0.5323        2.3177  10.6335\n",
      "      9        \u001b[36m0.5495\u001b[0m       0.5354        5.7488  10.7153\n",
      "     10        \u001b[36m0.5492\u001b[0m       0.5542        3.0833  10.6522\n",
      "     11        0.5501       0.5384        3.0510  10.7755\n",
      "     12        0.5566       0.5279        5.4299  10.8263\n",
      "     13        0.5544       0.5418        5.1647  10.7191\n",
      "     14        0.5585       0.5223        6.4164  10.7571\n",
      "     15        0.5573       0.5100       13.2110  10.6875\n",
      "     16        0.5631       0.5132       21.0988  10.8624\n",
      "     17        0.5605       0.5227       30.3570  10.6694\n",
      "     18        0.5666       0.5261       15.7582  10.7991\n",
      "     19        0.5673       0.5166       17.7883  10.6693\n",
      "     20        0.5689       0.5459       12.8771  10.8181\n",
      "     21        0.5632       0.5655        7.6993  10.6085\n",
      "     22        0.5593       0.5547        7.2875  10.7749\n",
      "     23        0.5609       0.5657        8.2789  10.7263\n",
      "     24        0.5588       0.5570       12.2425  10.7585\n",
      "     25        0.5604       0.5360       39.4293  10.7413\n",
      "     26        0.5593       0.5436       21.2005  10.7671\n",
      "     27        0.5632       0.5252       38.8273  10.7052\n",
      "     28        0.5632       0.5221       26.3465  10.6542\n",
      "     29        0.5586       0.5558       27.1078  10.8779\n",
      "     30        0.5635       0.5604       12.9673  10.7195\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4830\u001b[0m       \u001b[32m0.4617\u001b[0m        \u001b[35m0.7292\u001b[0m  10.7710\n",
      "      2        \u001b[36m0.4804\u001b[0m       \u001b[32m0.5442\u001b[0m        0.7314  10.8735\n",
      "      3        0.4823       \u001b[32m0.5811\u001b[0m        \u001b[35m0.7128\u001b[0m  10.6982\n",
      "      4        0.4807       0.4458        0.7515  10.7840\n",
      "      5        0.4815       0.4454        0.7532  10.6571\n",
      "      6        \u001b[36m0.4796\u001b[0m       0.5626        0.7432  10.7598\n",
      "      7        \u001b[36m0.4790\u001b[0m       \u001b[32m0.5900\u001b[0m        0.7387  10.6480\n",
      "      8        0.4883       0.4450        0.7226  10.7961\n",
      "      9        0.4861       0.4453        0.7470  10.6124\n",
      "     10        0.4861       0.4455        0.7236  10.7757\n",
      "     11        0.4871       0.5712        0.7206  10.6897\n",
      "     12        0.4891       0.4456        0.7815  10.7575\n",
      "     13        0.4861       0.4463        0.7962  10.6298\n",
      "     14        0.4862       0.4445        0.7586  10.7468\n",
      "     15        0.4865       0.4444        0.7517  10.6077\n",
      "     16        0.4886       0.4465        0.8155  10.6314\n",
      "     17        0.4880       0.4452        0.7239  10.7357\n",
      "     18        0.4859       0.4454        0.7378  10.7188\n",
      "     19        0.4864       0.5778        0.8524  10.8281\n",
      "     20        0.4857       0.4460        0.8319  10.6830\n",
      "     21        0.4862       0.5801        0.9963  10.7910\n",
      "     22        0.4869       0.5713        0.7747  10.6533\n",
      "     23        0.4859       0.4463        0.8107  10.7726\n",
      "     24        0.4877       0.5108        0.7652  10.7028\n",
      "     25        0.4868       0.4447        0.8116  10.6763\n",
      "     26        0.4896       0.5388        0.7903  10.8694\n",
      "     27        0.4892       0.4459        0.8342  10.6885\n",
      "     28        0.4868       0.5034        0.7500  10.8157\n",
      "     29        0.4859       0.5693        1.4535  10.6951\n",
      "     30        0.4855       0.4460        1.1134  10.8383\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5365\u001b[0m       \u001b[32m0.5097\u001b[0m        \u001b[35m5.0469\u001b[0m  10.7390\n",
      "      2        \u001b[36m0.5264\u001b[0m       0.4764       12.3172  12.8427\n",
      "      3        \u001b[36m0.5256\u001b[0m       0.4724       15.5916  10.8322\n",
      "      4        0.5258       0.4694       32.5476  10.7240\n",
      "      5        0.5264       0.4490       46.2577  10.7223\n",
      "      6        \u001b[36m0.5247\u001b[0m       0.4539       77.4574  10.6114\n",
      "      7        \u001b[36m0.5211\u001b[0m       0.4726       62.2281  10.7787\n",
      "      8        0.5233       0.4643       39.6684  10.6560\n",
      "      9        0.5254       0.4552       65.9429  10.7732\n",
      "     10        0.5277       0.4544      112.8944  10.6344\n",
      "     11        0.5257       0.4709      104.7440  10.7495\n",
      "     12        0.5270       0.4703       91.2911  10.6447\n",
      "     13        0.5249       0.4565      169.4367  10.7444\n",
      "     14        0.5231       0.4574      273.6150  10.6594\n",
      "     15        0.5252       0.4618      129.5308  10.7798\n",
      "     16        0.5218       0.4659      161.0371  10.7539\n",
      "     17        0.5267       0.4780      184.0917  10.6696\n",
      "     18        0.5221       0.4736      143.8019  10.7654\n",
      "     19        0.5232       0.4665      175.2585  10.6943\n",
      "     20        0.5224       0.4589      190.0393  10.7596\n",
      "     21        0.5233       0.4725      192.1671  10.7083\n",
      "     22        0.5213       0.4582      255.8107  10.7688\n",
      "     23        0.5236       0.4584      277.2544  10.7976\n",
      "     24        \u001b[36m0.5201\u001b[0m       0.4603      321.6949  10.6840\n",
      "     25        \u001b[36m0.5191\u001b[0m       0.4526      363.0754  10.8156\n",
      "     26        0.5198       0.4576      313.9572  10.7095\n",
      "     27        0.5254       0.4484      234.6076  10.7948\n",
      "     28        0.5203       0.4565      392.2329  10.6826\n",
      "     29        0.5208       0.4382      370.5525  10.8106\n",
      "     30        0.5260       0.4552      472.4816  10.6862\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5718\u001b[0m       \u001b[32m0.5442\u001b[0m        \u001b[35m0.8956\u001b[0m  10.6583\n",
      "      2        \u001b[36m0.5591\u001b[0m       0.5111        2.2932  10.7692\n",
      "      3        \u001b[36m0.5541\u001b[0m       0.5049        2.0925  10.6995\n",
      "      4        0.5543       0.5051        7.4587  10.8001\n",
      "      5        \u001b[36m0.5534\u001b[0m       0.5084        4.0185  10.7786\n",
      "      6        \u001b[36m0.5509\u001b[0m       0.5041        6.6206  10.6749\n",
      "      7        0.5528       0.5083        6.4658  10.8183\n",
      "      8        0.5570       0.5229        8.2978  10.6314\n",
      "      9        0.5666       0.5352       12.6216  10.6748\n",
      "     10        0.5665       0.5362       13.3730  10.8070\n",
      "     11        0.5580       0.5196       19.7393  10.7569\n",
      "     12        0.5576       \u001b[32m0.5451\u001b[0m        8.0314  10.8148\n",
      "     13        0.5588       0.5338       23.2234  10.6464\n",
      "     14        0.5550       0.5358       13.6199  10.7221\n",
      "     15        0.5522       0.5080       45.4734  10.6264\n",
      "     16        \u001b[36m0.5508\u001b[0m       0.5156       38.2079  10.7429\n",
      "     17        \u001b[36m0.5502\u001b[0m       0.5040       23.8371  10.7078\n",
      "     18        \u001b[36m0.5499\u001b[0m       0.5159       31.3643  10.6695\n",
      "     19        0.5511       0.5158       22.8068  10.8332\n",
      "     20        0.5515       0.5180       35.2864  10.7042\n",
      "     21        0.5517       0.5192       28.8805  10.7875\n",
      "     22        0.5515       0.5239       48.0242  10.7126\n",
      "     23        \u001b[36m0.5474\u001b[0m       0.5302       47.2997  10.8817\n",
      "     24        0.5606       0.5114       38.6207  10.6910\n",
      "     25        0.5557       0.5099       59.5246  10.8116\n",
      "     26        0.5533       0.5127       55.1614  10.6187\n",
      "     27        0.5595       0.5178       67.3448  10.7362\n",
      "     28        0.5579       0.5154      105.7424  10.6056\n",
      "     29        0.5574       0.5316       80.1922  10.7187\n",
      "     30        0.5570       0.5225      107.4065  10.6291\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4826\u001b[0m       \u001b[32m0.5709\u001b[0m        \u001b[35m0.6992\u001b[0m  10.6216\n",
      "      2        \u001b[36m0.4794\u001b[0m       0.5621        0.7360  10.7374\n",
      "      3        \u001b[36m0.4777\u001b[0m       0.5705        0.7267  10.7395\n",
      "      4        0.4804       0.4440        0.7433  10.7675\n",
      "      5        0.4815       0.4441        0.7548  10.5951\n",
      "      6        0.4817       \u001b[32m0.5742\u001b[0m        0.7372  10.6020\n",
      "      7        0.4818       \u001b[32m0.5843\u001b[0m        0.7427  10.7836\n",
      "      8        0.4811       0.5688        0.8581  10.6712\n",
      "      9        0.4818       0.4459        0.8728  10.6449\n",
      "     10        0.4802       0.5292        0.8332  10.8148\n",
      "     11        0.4810       0.5760        0.8887  10.7619\n",
      "     12        0.4808       0.5574        0.7798  10.7161\n",
      "     13        0.4804       0.5701        0.8521  10.6072\n",
      "     14        0.4810       0.4458        0.8601  10.7655\n",
      "     15        0.4836       0.4456        0.9353  10.6370\n",
      "     16        0.4810       \u001b[32m0.5902\u001b[0m        0.7107  10.7454\n",
      "     17        0.4803       \u001b[32m0.5956\u001b[0m        0.7006  10.5493\n",
      "     18        0.4810       0.5637        0.7451  10.7657\n",
      "     19        0.4801       0.5665        0.7738  10.6265\n",
      "     20        0.4866       0.4457        1.1335  10.7065\n",
      "     21        0.4848       0.4456        1.1799  10.6406\n",
      "     22        0.4862       0.5300        1.1814  10.7509\n",
      "     23        0.4869       0.4451        1.1332  10.6137\n",
      "     24        0.4876       0.4860        1.0648  10.7217\n",
      "     25        0.4887       0.4874        1.2559  10.6574\n",
      "     26        0.4873       0.4931        1.3278  10.7189\n",
      "     27        0.4881       0.4458        1.3963  10.6854\n",
      "     28        0.4853       0.5655        1.2820  10.6649\n",
      "     29        0.4857       0.4457        1.3277  10.7919\n",
      "     30        0.4858       0.4462        1.6007  10.7057\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5477\u001b[0m       \u001b[32m0.4890\u001b[0m        \u001b[35m5.8515\u001b[0m  10.9358\n",
      "      2        \u001b[36m0.5352\u001b[0m       0.4620        9.4526  10.7061\n",
      "      3        \u001b[36m0.5331\u001b[0m       0.4183        8.0647  10.8573\n",
      "      4        \u001b[36m0.5330\u001b[0m       0.4473       12.9871  10.8018\n",
      "      5        \u001b[36m0.5304\u001b[0m       0.4609       10.6641  10.8603\n",
      "      6        0.5319       0.4628       10.0701  10.7848\n",
      "      7        0.5313       0.4549       20.3030  10.9337\n",
      "      8        \u001b[36m0.5294\u001b[0m       0.4584       23.2614  10.8839\n",
      "      9        0.5303       0.4558       19.6346  10.6041\n",
      "     10        \u001b[36m0.5281\u001b[0m       0.4624       18.9009  10.8324\n",
      "     11        0.5289       0.4610       24.9696  10.7097\n",
      "     12        \u001b[36m0.5280\u001b[0m       0.4606       20.7291  10.8302\n",
      "     13        0.5287       0.4584       35.6592  10.7453\n",
      "     14        0.5284       0.4519       44.0030  10.7829\n",
      "     15        0.5306       0.4502       26.7897  10.7701\n",
      "     16        0.5282       0.4594       22.4377  10.7323\n",
      "     17        \u001b[36m0.5276\u001b[0m       0.4558       33.3075  10.7830\n",
      "     18        0.5309       0.4485       21.8633  10.8532\n",
      "     19        0.5297       0.4463       55.1491  10.7556\n",
      "     20        0.5308       0.4527       61.8643  10.9033\n",
      "     21        0.5291       0.4496       26.4412  10.7143\n",
      "     22        \u001b[36m0.5273\u001b[0m       0.4645       47.5184  10.8686\n",
      "     23        0.5291       0.4468       33.8928  10.7477\n",
      "     24        0.5291       0.4587       78.9569  10.8036\n",
      "     25        0.5356       0.4502       34.0826  10.7805\n",
      "     26        0.5286       0.4603       67.1169  10.8241\n",
      "     27        0.5299       0.4581       81.2055  10.8192\n",
      "     28        0.5321       0.4560       30.2647  10.6950\n",
      "     29        0.5278       0.4638       37.6543  10.8912\n",
      "     30        0.5290       0.4777       83.0477  10.7975\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5805\u001b[0m       \u001b[32m0.4523\u001b[0m        \u001b[35m0.7092\u001b[0m  10.8192\n",
      "      2        \u001b[36m0.5666\u001b[0m       0.4457        0.7533  10.8244\n",
      "      3        \u001b[36m0.5646\u001b[0m       0.4461        0.7323  10.8664\n",
      "      4        \u001b[36m0.5641\u001b[0m       \u001b[32m0.4554\u001b[0m        0.7192  10.7911\n",
      "      5        \u001b[36m0.5636\u001b[0m       0.4527        0.7211  10.6737\n",
      "      6        \u001b[36m0.5630\u001b[0m       0.4458        0.7380  10.8828\n",
      "      7        \u001b[36m0.5614\u001b[0m       0.4466        0.7310  10.7010\n",
      "      8        0.5640       0.4471        0.7267  10.7847\n",
      "      9        \u001b[36m0.5614\u001b[0m       0.4491        0.7209  11.0826\n",
      "     10        \u001b[36m0.5598\u001b[0m       0.4479        0.7154  10.9689\n",
      "     11        0.5611       0.4515        0.7356  10.9270\n",
      "     12        \u001b[36m0.5586\u001b[0m       0.4476        0.7442  10.7986\n",
      "     13        0.5599       0.4513        0.7279  10.7000\n",
      "     14        0.5600       0.4497        \u001b[35m0.7076\u001b[0m  10.8315\n",
      "     15        0.5626       0.4486        0.7224  10.8029\n",
      "     16        0.5612       0.4468        0.7116  10.7055\n",
      "     17        0.5627       \u001b[32m0.4555\u001b[0m        0.7257  10.8160\n",
      "     18        0.5609       \u001b[32m0.4754\u001b[0m        0.7157  10.7134\n",
      "     19        0.5610       0.4488        0.7181  10.8444\n",
      "     20        0.5613       0.4482        0.7249  10.7779\n",
      "     21        0.5591       0.4477        0.7127  10.9584\n",
      "     22        0.5589       0.4475        0.7189  10.6452\n",
      "     23        \u001b[36m0.5581\u001b[0m       0.4498        0.7364  10.8309\n",
      "     24        0.5595       0.4732        0.7281  10.8275\n",
      "     25        \u001b[36m0.5576\u001b[0m       0.4593        \u001b[35m0.7029\u001b[0m  10.7322\n",
      "     26        0.5585       0.4498        0.7358  10.8855\n",
      "     27        0.5582       0.4517        0.7120  10.7662\n",
      "     28        0.5598       0.4472        0.7163  10.8702\n",
      "     29        0.5638       0.4522        0.7118  10.7732\n",
      "     30        0.5650       0.4520        0.7058  10.7712\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4850\u001b[0m       \u001b[32m0.5561\u001b[0m        \u001b[35m0.6995\u001b[0m  10.6797\n",
      "      2        \u001b[36m0.4782\u001b[0m       \u001b[32m0.5568\u001b[0m        0.7143  10.8958\n",
      "      3        0.4816       0.5529        0.7151  10.8490\n",
      "      4        0.4825       0.5005        0.7540  10.7296\n",
      "      5        0.4807       0.5501        0.7015  10.8371\n",
      "      6        0.4800       \u001b[32m0.5627\u001b[0m        0.7019  10.7317\n",
      "      7        0.4807       0.4439        0.7399  10.8517\n",
      "      8        0.4829       0.5545        0.7324  10.7689\n",
      "      9        0.4827       0.5205        0.7216  10.8265\n",
      "     10        0.4809       0.4445        0.7502  10.8173\n",
      "     11        0.4859       0.4440        0.7212  10.8610\n",
      "     12        0.4873       0.4445        0.7417  10.7968\n",
      "     13        0.4858       0.4440        0.7327  10.6828\n",
      "     14        0.4885       0.5469        0.7155  10.7564\n",
      "     15        0.4887       0.4439        0.7413  10.7092\n",
      "     16        0.4874       0.4439        0.7380  10.8138\n",
      "     17        0.4864       0.5458        0.7228  10.7209\n",
      "     18        0.4861       0.5477        0.7238  10.7954\n",
      "     19        0.4879       0.5459        0.7188  10.7476\n",
      "     20        0.4887       0.5552        0.7051  10.6531\n",
      "     21        0.4857       0.5533        0.7169  10.7551\n",
      "     22        0.4865       0.4450        0.7335  10.8100\n",
      "     23        0.4860       \u001b[32m0.5685\u001b[0m        0.7160  10.8380\n",
      "     24        0.4866       0.4441        0.7333  10.9262\n",
      "     25        0.4851       0.4450        0.7297  10.7294\n",
      "     26        0.4863       0.5496        0.7321  10.7634\n",
      "     27        0.4858       0.4446        0.7497  10.8225\n",
      "     28        0.4870       0.4446        0.7526  10.8526\n",
      "     29        0.4869       0.5352        0.7176  10.7119\n",
      "     30        0.4867       \u001b[32m0.5796\u001b[0m        0.7131  10.8339\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5369\u001b[0m       \u001b[32m0.4719\u001b[0m        \u001b[35m9.2688\u001b[0m  10.7794\n",
      "      2        \u001b[36m0.5265\u001b[0m       0.4574       16.9008  10.9137\n",
      "      3        0.5272       \u001b[32m0.4811\u001b[0m       18.4142  10.7705\n",
      "      4        \u001b[36m0.5244\u001b[0m       0.4535       22.1565  10.8595\n",
      "      5        \u001b[36m0.5239\u001b[0m       0.4637       34.3868  10.7397\n",
      "      6        \u001b[36m0.5199\u001b[0m       0.4521       45.8897  10.8111\n",
      "      7        0.5225       0.4589       83.2452  10.7281\n",
      "      8        0.5266       0.4675       57.6238  10.9019\n",
      "      9        0.5246       0.4572       90.0870  10.7830\n",
      "     10        0.5218       0.4517       87.2335  10.7456\n",
      "     11        0.5236       0.4541       87.8880  10.8840\n",
      "     12        0.5244       0.4643       76.3836  10.7290\n",
      "     13        0.5220       0.4364       53.0564  10.8294\n",
      "     14        0.5239       0.4630      105.3240  10.7885\n",
      "     15        0.5240       0.4217      138.9920  11.0509\n",
      "     16        0.5251       0.4433      118.1515  10.9531\n",
      "     17        0.5256       0.4619      156.6360  10.8057\n",
      "     18        0.5220       0.4644       99.9862  10.7736\n",
      "     19        \u001b[36m0.5198\u001b[0m       0.4557      164.7698  10.7523\n",
      "     20        0.5203       0.4639      131.2045  10.8560\n",
      "     21        0.5200       0.4711      247.0862  10.7893\n",
      "     22        0.5218       0.4594      244.1197  10.7505\n",
      "     23        0.5221       0.4766      181.8213  10.8971\n",
      "     24        0.5247       0.4631      341.1897  10.7142\n",
      "     25        0.5224       0.4545      305.6465  10.7997\n",
      "     26        0.5238       0.4512      291.5212  10.8351\n",
      "     27        0.5294       0.4503      456.2118  10.7601\n",
      "     28        0.5278       0.4683      453.5501  10.8697\n",
      "     29        0.5373       0.4729      283.3656  10.7537\n",
      "     30        0.5234       0.4587      303.6581  10.6718\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5727\u001b[0m       \u001b[32m0.5784\u001b[0m        \u001b[35m0.7765\u001b[0m  10.6688\n",
      "      2        \u001b[36m0.5572\u001b[0m       0.5459        1.1757  10.9020\n",
      "      3        \u001b[36m0.5542\u001b[0m       0.5273        3.7817  10.8023\n",
      "      4        0.5542       0.5694        1.3205  10.8165\n",
      "      5        \u001b[36m0.5513\u001b[0m       0.5512        8.2850  10.9029\n",
      "      6        0.5517       0.5369        3.8050  10.8919\n",
      "      7        0.5526       0.5420        6.9022  10.7035\n",
      "      8        0.5535       0.5094       21.6698  10.8796\n",
      "      9        0.5552       0.5317        9.0937  10.8004\n",
      "     10        0.5564       0.5111       10.2717  10.7761\n",
      "     11        0.5548       0.5193       16.8538  10.7429\n",
      "     12        0.5533       0.5564        9.6937  10.7106\n",
      "     13        0.5516       0.5316        8.4908  10.7787\n",
      "     14        \u001b[36m0.5509\u001b[0m       0.5611       14.5764  10.7040\n",
      "     15        0.5538       0.5248       15.0690  10.8144\n",
      "     16        0.5555       0.5265       15.7072  10.7015\n",
      "     17        0.5587       0.5334       22.6862  10.8178\n",
      "     18        0.5566       0.5315       18.0773  10.7783\n",
      "     19        0.5555       0.5234       17.1277  10.8028\n",
      "     20        0.5549       0.5290       26.3647  10.8252\n",
      "     21        0.5558       0.5331       20.5943  10.8682\n",
      "     22        0.5543       0.5259       27.0017  10.6963\n",
      "     23        0.5568       0.5228       24.1281  10.8309\n",
      "     24        0.5547       0.5483       20.6160  10.7543\n",
      "     25        0.5589       0.5343       16.2255  10.8542\n",
      "     26        0.5539       0.5250       21.4702  10.8470\n",
      "     27        0.5565       0.5502       22.3203  10.7718\n",
      "     28        0.5527       0.5422       25.1258  10.8553\n",
      "     29        0.5585       0.5158       41.7776  10.8118\n",
      "     30        0.5560       0.5566       24.0195  10.7879\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4807\u001b[0m       \u001b[32m0.5062\u001b[0m        \u001b[35m0.7467\u001b[0m  10.6770\n",
      "      2        \u001b[36m0.4773\u001b[0m       \u001b[32m0.5693\u001b[0m        \u001b[35m0.7261\u001b[0m  10.8567\n",
      "      3        0.4791       \u001b[32m0.5726\u001b[0m        \u001b[35m0.7260\u001b[0m  10.8012\n",
      "      4        0.4796       0.4457        0.7794  10.8432\n",
      "      5        0.4801       0.4454        0.7593  10.8093\n",
      "      6        0.4817       0.4437        0.7396  10.7431\n",
      "      7        0.4862       0.4443        \u001b[35m0.7258\u001b[0m  10.8716\n",
      "      8        0.4855       0.4444        0.7294  10.7668\n",
      "      9        0.4851       0.5454        \u001b[35m0.7241\u001b[0m  10.8011\n",
      "     10        0.4859       0.5021        0.7464  10.8202\n",
      "     11        0.4869       0.4449        0.7249  10.6713\n",
      "     12        0.4867       0.5430        0.7568  10.7877\n",
      "     13        0.4853       0.4457        0.7780  10.7024\n",
      "     14        0.4857       0.5597        0.7508  10.8155\n",
      "     15        0.4870       0.5252        0.9463  10.7075\n",
      "     16        0.4866       0.4460        1.0105  10.7457\n",
      "     17        0.4850       0.4460        0.8380  10.8137\n",
      "     18        0.4858       \u001b[32m0.5758\u001b[0m        0.8437  10.6503\n",
      "     19        0.4850       0.4465        0.8367  10.8209\n",
      "     20        0.4855       0.5542        0.7988  10.7874\n",
      "     21        0.4889       0.5582        0.7450  10.8538\n",
      "     22        0.4881       0.4450        0.7644  10.8878\n",
      "     23        0.4892       0.5285        0.7601  10.6437\n",
      "     24        0.4869       0.4456        0.8029  10.7651\n",
      "     25        0.4891       0.5572        0.7285  10.7714\n",
      "     26        0.4858       0.5461        0.7413  10.8791\n",
      "     27        0.4868       0.4450        0.7550  10.7276\n",
      "     28        0.4869       0.5727        \u001b[35m0.7208\u001b[0m  10.7225\n",
      "     29        0.4857       0.4450        0.7683  10.7129\n",
      "     30        0.4852       0.4447        0.7424  10.7605\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5335\u001b[0m       \u001b[32m0.4596\u001b[0m        \u001b[35m6.2097\u001b[0m  10.8591\n",
      "      2        \u001b[36m0.5261\u001b[0m       \u001b[32m0.5007\u001b[0m       15.1462  10.7423\n",
      "      3        \u001b[36m0.5251\u001b[0m       0.4709       26.2784  10.7031\n",
      "      4        \u001b[36m0.5231\u001b[0m       0.4632       27.0720  10.8176\n",
      "      5        0.5235       0.4663       31.6077  10.7258\n",
      "      6        0.5237       0.4500       70.6009  10.8440\n",
      "      7        \u001b[36m0.5205\u001b[0m       0.4444       63.2044  10.8178\n",
      "      8        0.5239       0.4638       73.6615  10.7074\n",
      "      9        0.5212       0.4584      100.0551  10.7844\n",
      "     10        0.5235       0.4525       94.0634  10.7472\n",
      "     11        0.5237       0.4558       69.4231  10.8074\n",
      "     12        0.5216       0.4654      100.6463  10.7770\n",
      "     13        0.5216       0.4666      136.7626  10.7148\n",
      "     14        0.5272       0.4497      184.9573  10.8660\n",
      "     15        \u001b[36m0.5197\u001b[0m       0.4366      168.2708  10.7141\n",
      "     16        0.5200       0.4549      140.7696  10.7150\n",
      "     17        0.5217       0.4680      208.6886  10.7412\n",
      "     18        0.5205       0.4360      124.0104  10.7578\n",
      "     19        0.5224       0.4511      229.4509  10.9051\n",
      "     20        0.5239       0.4492      152.4840  10.7524\n",
      "     21        0.5274       0.4497      200.5519  10.8371\n",
      "     22        0.5274       0.4475      192.1538  10.7623\n",
      "     23        0.5220       0.4362      224.8018  10.9180\n",
      "     24        0.5241       0.4657      238.3160  10.8010\n",
      "     25        0.5238       0.4504      207.4194  10.6847\n",
      "     26        0.5222       0.4535      253.3487  10.7967\n",
      "     27        0.5208       0.4536      232.8622  10.7245\n",
      "     28        0.5252       0.4573      264.9757  10.8872\n",
      "     29        0.5218       0.4519      262.8214  10.7680\n",
      "     30        0.5214       0.4471      293.7312  10.6409\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5692\u001b[0m       \u001b[32m0.5286\u001b[0m        \u001b[35m1.0387\u001b[0m  10.6118\n",
      "      2        \u001b[36m0.5545\u001b[0m       0.5098        1.7023  10.8415\n",
      "      3        \u001b[36m0.5510\u001b[0m       0.5191        1.9278  10.7915\n",
      "      4        \u001b[36m0.5507\u001b[0m       0.5134        2.9808  10.8571\n",
      "      5        0.5515       0.5187        4.3705  10.7522\n",
      "      6        \u001b[36m0.5486\u001b[0m       \u001b[32m0.5297\u001b[0m        8.0674  10.8825\n",
      "      7        0.5585       0.5131        6.8565  10.8036\n",
      "      8        0.5684       0.5155        9.4069  10.7611\n",
      "      9        0.5673       \u001b[32m0.5424\u001b[0m        3.8214  10.7604\n",
      "     10        0.5689       0.5282       13.7147  10.8245\n",
      "     11        0.5574       0.5354       17.7264  10.6809\n",
      "     12        0.5559       \u001b[32m0.5429\u001b[0m        9.0984  10.8296\n",
      "     13        0.5583       0.5244       29.1554  10.7654\n",
      "     14        0.5576       \u001b[32m0.5552\u001b[0m       14.8020  10.6551\n",
      "     15        0.5573       0.5375       13.4096  10.7595\n",
      "     16        0.5572       0.5542       11.9228  10.6868\n",
      "     17        0.5588       0.4913       25.8889  10.8353\n",
      "     18        0.5556       0.5224       15.4194  10.7107\n",
      "     19        0.5608       0.4974       27.3925  10.9151\n",
      "     20        0.5584       0.5170       32.1437  10.7739\n",
      "     21        0.5583       0.5274       20.7321  10.6760\n",
      "     22        0.5543       0.5165       60.6996  10.8320\n",
      "     23        0.5546       0.5115       51.2278  10.7439\n",
      "     24        0.5587       0.5382       19.9565  10.9096\n",
      "     25        0.5602       0.5223       57.3374  10.8871\n",
      "     26        0.5557       0.5337       51.7179  10.8650\n",
      "     27        0.5540       0.5147       86.9978  10.9016\n",
      "     28        0.5558       0.5336       36.3432  10.9282\n",
      "     29        0.5515       0.5274       49.5382  10.7629\n",
      "     30        0.5552       0.5060       40.7560  10.8120\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4792\u001b[0m       \u001b[32m0.5335\u001b[0m        \u001b[35m0.7491\u001b[0m  10.7216\n",
      "      2        \u001b[36m0.4750\u001b[0m       \u001b[32m0.5751\u001b[0m        \u001b[35m0.7322\u001b[0m  10.8255\n",
      "      3        \u001b[36m0.4747\u001b[0m       0.5039        0.7832  10.7153\n",
      "      4        0.4786       0.5426        0.7537  10.7934\n",
      "      5        0.4789       0.5502        0.7556  10.6930\n",
      "      6        0.4793       0.4437        0.7498  10.8601\n",
      "      7        0.4784       0.4438        0.7417  10.8794\n",
      "      8        0.4768       0.5198        0.7588  10.9208\n",
      "      9        0.4801       0.4452        0.7897  10.8463\n",
      "     10        0.4847       0.5570        0.9853  10.6678\n",
      "     11        0.4809       0.5442        0.8501  10.7837\n",
      "     12        0.4817       0.5702        0.7772  10.7100\n",
      "     13        0.4805       0.4452        0.7896  10.7877\n",
      "     14        0.4829       0.5303        \u001b[35m0.7298\u001b[0m  10.7512\n",
      "     15        0.4827       0.5250        \u001b[35m0.7246\u001b[0m  10.7698\n",
      "     16        0.4820       0.5071        0.7754  10.8142\n",
      "     17        0.4816       0.5326        0.7542  10.6633\n",
      "     18        0.4902       0.4442        0.7888  10.7763\n",
      "     19        0.4883       0.4457        0.8748  10.8193\n",
      "     20        0.4853       0.4465        1.0327  10.9971\n",
      "     21        0.4850       0.5645        1.0416  10.6817\n",
      "     22        0.4875       0.5278        0.8437  10.7327\n",
      "     23        0.4933       0.4461        0.8703  10.6559\n",
      "     24        0.4900       0.4452        0.9483  10.8032\n",
      "     25        0.4857       0.5411        1.0153  10.6672\n",
      "     26        0.4851       0.4452        0.8902  10.8372\n",
      "     27        0.4864       0.5462        0.8176  10.8081\n",
      "     28        0.4869       0.4452        0.9741  10.6401\n",
      "     29        0.4849       0.4459        1.1137  10.8235\n",
      "     30        0.4867       0.4456        0.8832  10.7707\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5352\u001b[0m       \u001b[32m0.4516\u001b[0m        \u001b[35m5.2958\u001b[0m  10.8007\n",
      "      2        \u001b[36m0.5287\u001b[0m       \u001b[32m0.5062\u001b[0m       20.5167  10.9087\n",
      "      3        \u001b[36m0.5247\u001b[0m       0.4712       24.0635  10.7860\n",
      "      4        \u001b[36m0.5239\u001b[0m       0.4717       27.0053  10.9153\n",
      "      5        0.5268       0.4554       36.0416  10.8172\n",
      "      6        0.5263       0.4683       53.3686  10.7159\n",
      "      7        0.5250       0.4911       52.9858  10.7121\n",
      "      8        \u001b[36m0.5229\u001b[0m       0.4726       60.5927  10.6896\n",
      "      9        0.5239       0.4882       80.8307  10.8251\n",
      "     10        0.5256       0.4567       98.0631  10.7760\n",
      "     11        0.5240       0.4822      105.4718  10.6693\n",
      "     12        0.5257       0.4992      105.9550  10.7759\n",
      "     13        0.5268       0.4862       65.2600  10.6974\n",
      "     14        0.5247       0.4712      132.7098  10.7791\n",
      "     15        \u001b[36m0.5219\u001b[0m       0.4660      152.3720  10.7855\n",
      "     16        \u001b[36m0.5203\u001b[0m       0.4554      152.9651  10.6911\n",
      "     17        0.5261       0.4631      217.8706  10.8469\n",
      "     18        0.5243       0.4613      184.8936  10.7626\n",
      "     19        0.5236       0.4753      334.7806  10.6345\n",
      "     20        0.5263       0.4717      332.7664  10.8103\n",
      "     21        0.5269       0.4432      243.5963  10.8071\n",
      "     22        0.5236       0.4563      320.0221  10.7059\n",
      "     23        0.5283       0.4723      401.7839  10.7749\n",
      "     24        0.5231       0.4579      321.1062  10.7645\n",
      "     25        0.5219       0.4787      400.3078  10.8350\n",
      "     26        0.5242       0.4628      373.6870  10.7287\n",
      "     27        0.5210       0.4590      411.9160  10.8528\n",
      "     28        0.5281       0.4664      401.3742  10.7782\n",
      "     29        0.5244       0.4528      284.4293  10.7451\n",
      "     30        0.5273       0.4681      327.2754  10.8648\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5622\u001b[0m       \u001b[32m0.5662\u001b[0m        \u001b[35m0.9359\u001b[0m  10.7661\n",
      "      2        \u001b[36m0.5570\u001b[0m       \u001b[32m0.5782\u001b[0m        \u001b[35m0.8020\u001b[0m  10.8357\n",
      "      3        \u001b[36m0.5499\u001b[0m       0.5479        2.4744  10.7902\n",
      "      4        0.5504       0.5336        1.8917  10.7805\n",
      "      5        0.5499       0.5313        1.9221  10.8215\n",
      "      6        \u001b[36m0.5497\u001b[0m       0.5363        1.9407  10.7644\n",
      "      7        0.5514       0.5536        6.9162  10.8616\n",
      "      8        0.5546       0.5444        5.3441  10.7555\n",
      "      9        0.5515       0.5149        9.0221  10.7532\n",
      "     10        0.5515       0.5256        8.0917  10.8030\n",
      "     11        0.5538       0.5568        8.0275  10.8129\n",
      "     12        0.5529       0.5671        3.0982  10.6698\n",
      "     13        0.5606       0.5483        7.2142  10.7841\n",
      "     14        0.5549       0.5382        6.6510  10.7096\n",
      "     15        0.5567       0.5375        8.1882  10.8134\n",
      "     16        0.5587       0.5575        1.7050  10.8264\n",
      "     17        0.5564       0.5353       13.1466  10.6804\n",
      "     18        0.5544       0.5213       10.2136  10.8129\n",
      "     19        \u001b[36m0.5496\u001b[0m       0.5060        8.0999  10.7201\n",
      "     20        0.5517       0.5266       11.1338  10.8229\n",
      "     21        \u001b[36m0.5481\u001b[0m       0.5538        7.5315  10.8356\n",
      "     22        0.5520       0.5284        6.3045  10.6866\n",
      "     23        0.5521       0.5493        3.8117  10.9005\n",
      "     24        0.5512       0.5469        6.7693  10.7452\n",
      "     25        0.5543       0.5355       11.5399  10.8494\n",
      "     26        0.5514       0.5384       18.6272  10.7822\n",
      "     27        0.5535       0.5440       10.0171  10.7343\n",
      "     28        0.5503       0.5456       15.9245  10.8082\n",
      "     29        0.5489       0.5444        6.6711  10.8137\n",
      "     30        0.5494       0.5354        5.6015  10.7527\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4841\u001b[0m       \u001b[32m0.4748\u001b[0m        \u001b[35m0.7223\u001b[0m  10.9892\n",
      "      2        \u001b[36m0.4830\u001b[0m       \u001b[32m0.5876\u001b[0m        \u001b[35m0.6971\u001b[0m  10.7717\n",
      "      3        \u001b[36m0.4804\u001b[0m       0.4518        0.7227  10.7342\n",
      "      4        0.4807       0.4631        0.7310  10.7924\n",
      "      5        0.4807       0.4459        0.7271  10.7155\n",
      "      6        0.4820       0.5714        0.7278  10.8006\n",
      "      7        \u001b[36m0.4794\u001b[0m       0.5708        0.7535  10.7171\n",
      "      8        \u001b[36m0.4787\u001b[0m       0.5713        0.7173  10.7391\n",
      "      9        0.4814       0.5734        1.1169  10.7879\n",
      "     10        0.4843       0.5693        0.8509  10.6703\n",
      "     11        0.4831       0.4446        0.7729  10.7689\n",
      "     12        0.4825       0.5867        0.7308  10.7705\n",
      "     13        0.4826       0.5120        0.7335  10.8124\n",
      "     14        0.4826       0.5365        0.8962  10.7873\n",
      "     15        0.4840       0.5453        1.0392  10.8135\n",
      "     16        0.4832       0.4457        0.8180  10.7431\n",
      "     17        0.4819       0.5311        0.7272  10.7766\n",
      "     18        0.4822       0.5331        0.7517  10.7765\n",
      "     19        0.4816       0.5476        0.8573  10.7536\n",
      "     20        0.4835       0.5122        0.8596  10.8279\n",
      "     21        0.4807       0.5527        0.9638  10.6749\n",
      "     22        0.4830       0.4455        1.0113  10.8055\n",
      "     23        0.4815       0.5509        0.8959  10.7734\n",
      "     24        0.4815       0.4450        0.9278  10.7447\n",
      "     25        0.4810       0.5721        0.8849  10.8429\n",
      "     26        0.4852       0.5595        1.4320  10.7199\n",
      "     27        0.4809       0.5627        1.3392  10.7871\n",
      "     28        0.4818       0.4463        1.5173  10.8056\n",
      "     29        0.4811       0.5745        1.2450  10.7514\n",
      "     30        0.4856       0.5767        0.9771  10.8149\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5474\u001b[0m       \u001b[32m0.4738\u001b[0m        \u001b[35m6.6484\u001b[0m  10.7925\n",
      "      2        \u001b[36m0.5315\u001b[0m       0.4489       15.3332  10.7933\n",
      "      3        \u001b[36m0.5270\u001b[0m       0.4584       10.6800  10.8548\n",
      "      4        \u001b[36m0.5248\u001b[0m       0.4598       16.5983  10.5860\n",
      "      5        \u001b[36m0.5246\u001b[0m       0.4512       20.3726  10.9108\n",
      "      6        0.5250       0.4701       41.0954  10.7542\n",
      "      7        0.5282       0.4732       31.9862  10.8384\n",
      "      8        0.5267       0.4565       30.8568  10.8516\n",
      "      9        0.5254       0.4516       35.9355  10.8653\n",
      "     10        0.5246       0.4598       39.2293  10.8588\n",
      "     11        \u001b[36m0.5225\u001b[0m       0.4666       51.6991  10.7486\n",
      "     12        0.5234       0.4508       97.2971  10.7507\n",
      "     13        \u001b[36m0.5224\u001b[0m       0.4584       78.1265  10.8257\n",
      "     14        \u001b[36m0.5198\u001b[0m       0.4538       76.8066  10.7152\n",
      "     15        \u001b[36m0.5180\u001b[0m       0.4563      122.1859  10.7738\n",
      "     16        0.5217       0.4622      143.8251  10.7424\n",
      "     17        0.5193       0.4330       78.2215  10.8079\n",
      "     18        0.5210       0.4513       97.4323  10.7851\n",
      "     19        0.5186       0.4461      118.1704  10.7416\n",
      "     20        0.5221       0.4510      164.0380  10.8370\n",
      "     21        0.5210       0.4573      100.4925  10.7181\n",
      "     22        0.5200       0.4539      183.0976  10.7269\n",
      "     23        0.5202       0.4618      153.4025  10.7759\n",
      "     24        0.5212       0.4623      192.5899  10.8385\n",
      "     25        0.5213       0.4621      163.1352  10.7850\n",
      "     26        0.5193       0.4645      211.0918  10.7934\n",
      "     27        0.5193       0.4589      209.4881  10.8536\n",
      "     28        0.5186       0.4548      211.2676  10.7357\n",
      "     29        \u001b[36m0.5177\u001b[0m       0.4214      238.5090  10.7763\n",
      "     30        0.5209       0.4643      338.9156  10.8380\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5613\u001b[0m       \u001b[32m0.5529\u001b[0m        \u001b[35m0.9968\u001b[0m  10.9932\n",
      "      2        \u001b[36m0.5475\u001b[0m       0.5369        1.5653  10.7494\n",
      "      3        \u001b[36m0.5460\u001b[0m       0.5266        2.2183  10.7637\n",
      "      4        \u001b[36m0.5456\u001b[0m       0.5120        2.5251  10.8606\n",
      "      5        \u001b[36m0.5449\u001b[0m       0.5101        6.0285  10.8367\n",
      "      6        \u001b[36m0.5447\u001b[0m       0.5300        8.1760  10.8182\n",
      "      7        0.5503       0.5327        1.6505  10.8232\n",
      "      8        0.5498       0.5112        5.2790  10.9004\n",
      "      9        0.5560       0.5278        3.6709  10.8247\n",
      "     10        0.5519       0.5226        5.1963  10.8658\n",
      "     11        0.5573       0.4995        4.9119  10.9002\n",
      "     12        0.5502       0.5256        4.5203  10.8212\n",
      "     13        0.5510       0.5263       11.2832  10.7764\n",
      "     14        0.5523       0.5169       16.7353  10.8814\n",
      "     15        0.5517       0.5263       11.7447  10.8354\n",
      "     16        0.5521       0.5344        7.8146  10.7864\n",
      "     17        0.5503       0.5384        5.1198  10.6781\n",
      "     18        0.5506       0.5345       25.7523  10.7249\n",
      "     19        0.5479       0.5388       26.6962  10.7965\n",
      "     20        0.5508       0.5462       26.6846  10.8289\n",
      "     21        0.5506       0.5203       26.3186  10.7288\n",
      "     22        0.5485       0.5526       15.3515  10.8656\n",
      "     23        0.5517       0.5310       31.5460  10.8143\n",
      "     24        0.5491       0.5393       26.8305  10.8799\n",
      "     25        0.5484       0.5497       34.7782  10.7861\n",
      "     26        0.5498       0.5524       39.6804  10.8226\n",
      "     27        0.5601       \u001b[32m0.5645\u001b[0m       19.4202  10.8631\n",
      "     28        0.5497       0.5430       52.1306  10.8059\n",
      "     29        0.5471       0.5379       63.6063  10.8812\n",
      "     30        0.5485       0.5312       39.0004  10.7160\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4817\u001b[0m       \u001b[32m0.5634\u001b[0m        \u001b[35m0.7161\u001b[0m  10.7477\n",
      "      2        \u001b[36m0.4780\u001b[0m       \u001b[32m0.5771\u001b[0m        0.7168  10.8964\n",
      "      3        0.4798       0.5347        0.7331  10.8274\n",
      "      4        0.4840       0.4450        0.7515  10.8598\n",
      "      5        0.4812       0.4444        0.7365  10.8726\n",
      "      6        0.4834       0.5398        \u001b[35m0.7150\u001b[0m  11.0157\n",
      "      7        0.4824       0.5348        0.7280  10.6400\n",
      "      8        0.4814       \u001b[32m0.5865\u001b[0m        0.7290  10.7812\n",
      "      9        0.4813       0.4441        0.7739  10.7715\n",
      "     10        0.4809       \u001b[32m0.5885\u001b[0m        \u001b[35m0.7011\u001b[0m  10.8663\n",
      "     11        0.4817       0.5778        0.7021  10.7064\n",
      "     12        0.4819       0.4438        0.7463  10.8417\n",
      "     13        0.4809       0.5787        0.7229  10.7208\n",
      "     14        0.4810       0.4450        0.7616  10.8453\n",
      "     15        0.4826       0.4447        0.7317  10.8055\n",
      "     16        0.4820       0.5755        0.7217  10.8242\n",
      "     17        0.4805       0.5722        0.7269  10.8011\n",
      "     18        0.4806       0.4464        0.7319  10.7955\n",
      "     19        0.4808       0.4449        0.7527  10.7888\n",
      "     20        0.4812       \u001b[32m0.5910\u001b[0m        0.7219  10.8516\n",
      "     21        0.4804       0.4468        0.7500  10.7748\n",
      "     22        0.4803       0.4445        0.7453  10.7832\n",
      "     23        0.4820       0.5646        0.7347  10.8233\n",
      "     24        0.4830       0.5800        0.7507  10.7740\n",
      "     25        0.4805       0.5750        1.0838  10.8378\n",
      "     26        0.4808       0.5719        1.0475  10.8236\n",
      "     27        0.4806       0.5734        0.9107  10.8077\n",
      "     28        0.4802       0.5747        0.9275  10.8198\n",
      "     29        0.4824       0.5188        0.9525  10.7005\n",
      "     30        0.4827       0.4456        1.0299  10.7184\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5319\u001b[0m       \u001b[32m0.4476\u001b[0m        \u001b[35m9.8269\u001b[0m  10.8720\n",
      "      2        \u001b[36m0.5257\u001b[0m       \u001b[32m0.4648\u001b[0m       18.1676  10.8714\n",
      "      3        \u001b[36m0.5243\u001b[0m       \u001b[32m0.4742\u001b[0m       29.0969  10.6113\n",
      "      4        0.5248       \u001b[32m0.4882\u001b[0m       37.6757  10.8124\n",
      "      5        0.5256       \u001b[32m0.5110\u001b[0m       23.6250  10.8161\n",
      "      6        0.5245       0.4575       44.6295  10.8204\n",
      "      7        \u001b[36m0.5217\u001b[0m       0.4775       55.9464  10.7381\n",
      "      8        0.5283       0.4642       57.9047  10.7473\n",
      "      9        0.5251       0.4703       68.5611  10.8271\n",
      "     10        0.5258       0.4634       85.1248  10.8197\n",
      "     11        0.5219       0.4737       92.6783  10.7340\n",
      "     12        0.5228       0.4500       94.5788  10.8091\n",
      "     13        \u001b[36m0.5212\u001b[0m       0.4570      101.6045  10.8141\n",
      "     14        0.5231       0.4720      194.9156  10.7487\n",
      "     15        0.5236       0.4525      171.8979  10.8939\n",
      "     16        0.5227       0.4679      123.3168  10.8543\n",
      "     17        0.5216       0.4454      200.9907  10.7341\n",
      "     18        \u001b[36m0.5196\u001b[0m       0.4568      171.5760  10.7771\n",
      "     19        0.5220       0.4659      159.9081  10.8319\n",
      "     20        0.5226       0.4566      164.7808  10.7781\n",
      "     21        \u001b[36m0.5192\u001b[0m       0.4518      187.4954  10.8403\n",
      "     22        0.5249       0.4817      178.3365  10.8437\n",
      "     23        0.5198       0.4624      201.9866  10.6954\n",
      "     24        0.5213       0.4572      224.9539  10.7245\n",
      "     25        0.5229       0.4619      214.3624  10.8201\n",
      "     26        0.5236       0.4541      321.5198  10.8834\n",
      "     27        0.5239       0.4724      121.3519  10.7443\n",
      "     28        0.5205       0.4551      199.4838  10.7705\n",
      "     29        0.5200       0.4605      202.4892  10.8331\n",
      "     30        \u001b[36m0.5187\u001b[0m       0.4501      310.3782  10.7425\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5620\u001b[0m       \u001b[32m0.5476\u001b[0m        \u001b[35m1.4526\u001b[0m  10.8627\n",
      "      2        \u001b[36m0.5464\u001b[0m       0.5313        1.7487  10.8466\n",
      "      3        0.5470       0.5382        1.7335  10.9596\n",
      "      4        0.5468       0.5315        3.2029  10.6915\n",
      "      5        0.5470       \u001b[32m0.5563\u001b[0m        3.3316  10.7754\n",
      "      6        0.5493       0.5322       19.0248  10.8344\n",
      "      7        0.5506       0.5498       12.2929  10.7976\n",
      "      8        0.5494       \u001b[32m0.5651\u001b[0m        4.7911  10.7515\n",
      "      9        0.5517       0.5544        7.5382  10.7333\n",
      "     10        0.5529       0.5651        7.0205  10.9365\n",
      "     11        0.5627       0.5228       16.9066  10.7058\n",
      "     12        0.5625       0.5131       42.5840  10.8829\n",
      "     13        0.5630       0.5389       15.5831  10.7730\n",
      "     14        0.5646       0.5279        6.9854  10.7140\n",
      "     15        0.5597       0.5308       33.3555  10.7879\n",
      "     16        0.5623       0.5369       23.9088  10.7866\n",
      "     17        0.5641       0.5326       20.9307  10.7401\n",
      "     18        0.5681       0.5404       18.0640  10.8049\n",
      "     19        0.5628       0.5233       18.7090  10.8526\n",
      "     20        0.5610       0.5182       49.6915  10.7568\n",
      "     21        0.5643       0.5162       33.3247  10.7960\n",
      "     22        0.5661       0.5178       50.6225  10.7591\n",
      "     23        0.5693       0.5300       47.1129  10.8261\n",
      "     24        0.5647       0.5282       33.7436  10.9666\n",
      "     25        0.5596       0.5434       42.2408  10.7221\n",
      "     26        0.5605       0.5217       54.9088  10.7301\n",
      "     27        0.5622       0.5094       56.0327  10.8068\n",
      "     28        0.5715       0.5381       38.7290  10.8598\n",
      "     29        0.5576       0.5244       54.3989  10.8207\n",
      "     30        0.5574       0.5212       27.3153  10.8680\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4882\u001b[0m       \u001b[32m0.4793\u001b[0m        \u001b[35m0.7313\u001b[0m  10.9071\n",
      "      2        \u001b[36m0.4871\u001b[0m       0.4532        0.7390  10.8003\n",
      "      3        \u001b[36m0.4865\u001b[0m       0.4617        0.7410  10.8112\n",
      "      4        \u001b[36m0.4857\u001b[0m       \u001b[32m0.5781\u001b[0m        \u001b[35m0.7125\u001b[0m  10.8727\n",
      "      5        0.4862       0.4455        0.7650  10.8999\n",
      "      6        0.4861       0.4453        0.7337  10.8406\n",
      "      7        \u001b[36m0.4847\u001b[0m       0.4454        0.7334  10.8291\n",
      "      8        0.4850       0.4454        0.7609  10.8049\n",
      "      9        0.4909       0.4453        0.7701  10.7466\n",
      "     10        0.4860       0.4441        0.7517  10.8056\n",
      "     11        0.4855       0.4447        0.7631  10.8002\n",
      "     12        0.4868       0.4447        0.7495  10.8414\n",
      "     13        0.4872       0.4447        0.7734  10.7706\n",
      "     14        0.4849       0.4440        0.7403  10.8744\n",
      "     15        0.4876       0.4629        0.7481  10.6704\n",
      "     16        0.4887       0.4992        0.7379  10.7879\n",
      "     17        0.4905       0.4448        0.7670  10.8108\n",
      "     18        0.4867       0.4454        0.8241  10.8066\n",
      "     19        0.4858       0.4453        0.8297  10.7358\n",
      "     20        0.4859       0.4460        0.8619  10.7565\n",
      "     21        0.4855       0.4449        0.7521  10.9445\n",
      "     22        0.4859       0.4446        0.7557  11.0285\n",
      "     23        0.4856       0.4439        0.7431  10.9839\n",
      "     24        0.4865       0.4920        0.7250  10.8280\n",
      "     25        0.4869       0.5366        1.0616  10.7726\n",
      "     26        0.4860       0.5256        1.1802  10.9067\n",
      "     27        0.4881       0.5414        0.8079  10.8028\n",
      "     28        0.4877       0.4450        0.7846  10.7952\n",
      "     29        0.4876       0.4452        1.0989  10.7793\n",
      "     30        0.4872       0.4462        1.4650  10.7710\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5328\u001b[0m       \u001b[32m0.4734\u001b[0m        \u001b[35m8.3933\u001b[0m  10.7082\n",
      "      2        \u001b[36m0.5240\u001b[0m       \u001b[32m0.4916\u001b[0m       14.2129  10.8096\n",
      "      3        0.5262       0.4731       21.8026  10.8306\n",
      "      4        \u001b[36m0.5215\u001b[0m       0.4627       16.3899  10.8582\n",
      "      5        0.5222       0.4509       39.8476  10.8168\n",
      "      6        0.5215       0.4620       50.7919  10.8395\n",
      "      7        0.5233       0.4433       61.7557  10.6700\n",
      "      8        0.5231       0.4685       87.0449  10.7139\n",
      "      9        0.5217       0.4575       65.9432  10.8219\n",
      "     10        0.5228       0.4747       85.1572  10.7980\n",
      "     11        0.5220       0.4554      131.1366  10.8018\n",
      "     12        0.5222       0.4609       45.4132  10.7089\n",
      "     13        0.5232       0.4640       70.7591  10.8159\n",
      "     14        \u001b[36m0.5188\u001b[0m       0.4610      149.0433  10.6767\n",
      "     15        0.5232       0.4641      153.2759  10.7393\n",
      "     16        0.5221       0.4533      122.9878  10.8197\n",
      "     17        0.5236       0.4577      113.2563  10.8331\n",
      "     18        0.5232       0.4657      171.7613  10.8269\n",
      "     19        0.5282       0.4625       91.6982  10.8149\n",
      "     20        0.5264       0.4509      241.6030  10.8496\n",
      "     21        0.5221       0.4708      208.8746  10.7969\n",
      "     22        0.5217       0.4534      321.4975  10.7432\n",
      "     23        0.5213       0.4602      276.9664  10.8523\n",
      "     24        0.5223       0.4598      293.5386  10.8700\n",
      "     25        0.5227       0.4644      394.7110  10.7642\n",
      "     26        0.5262       0.4543      326.6479  10.8062\n",
      "     27        0.5231       0.4481      408.5932  10.7153\n",
      "     28        0.5267       0.4544      364.6251  10.7378\n",
      "     29        0.5220       0.4495      429.4733  10.8567\n",
      "     30        0.5208       0.4504      446.4371  10.8482\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5624\u001b[0m       \u001b[32m0.5568\u001b[0m        \u001b[35m1.5123\u001b[0m  10.9782\n",
      "      2        \u001b[36m0.5448\u001b[0m       0.5153        2.8429  10.7299\n",
      "      3        0.5450       0.5304        2.7398  10.8021\n",
      "      4        \u001b[36m0.5434\u001b[0m       0.5210        4.1498  10.8634\n",
      "      5        0.5460       0.5227        5.2196  10.9000\n",
      "      6        0.5491       0.5525        4.5422  10.8069\n",
      "      7        0.5477       0.5251       10.5568  10.8703\n",
      "      8        0.5568       0.5471       12.3223  10.7129\n",
      "      9        0.5489       0.5255       13.2169  10.8279\n",
      "     10        0.5502       0.5196        9.9445  10.7601\n",
      "     11        0.5494       0.5300       13.9941  10.8879\n",
      "     12        0.5482       0.5239        9.7324  10.8902\n",
      "     13        0.5521       0.5201       37.6089  10.7658\n",
      "     14        0.5557       0.5455       14.3476  10.7623\n",
      "     15        0.5509       0.5240       36.1135  10.8604\n",
      "     16        0.5525       0.5253       17.5561  10.7016\n",
      "     17        0.5511       0.5279       19.8656  10.6638\n",
      "     18        0.5530       0.5359       39.2421  10.8346\n",
      "     19        0.5467       0.5103       61.4359  11.0400\n",
      "     20        0.5448       0.5306       23.3842  10.6854\n",
      "     21        0.5463       0.5339       44.6291  10.7094\n",
      "     22        0.5492       0.5105       55.3984  10.8411\n",
      "     23        0.5475       0.5272       42.5598  10.9231\n",
      "     24        0.5452       0.5269       37.2533  10.6491\n",
      "     25        0.5464       0.5185       99.5679  10.8761\n",
      "     26        0.5480       0.5171       42.8134  10.9210\n",
      "     27        0.5512       0.5250       69.7327  11.2173\n",
      "     28        0.5542       0.5220       39.1956  10.8292\n",
      "     29        0.5550       0.5309       42.6577  10.8837\n",
      "     30        0.5536       0.5346       68.0161  10.8833\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4826\u001b[0m       \u001b[32m0.5469\u001b[0m        \u001b[35m0.7132\u001b[0m  10.8671\n",
      "      2        \u001b[36m0.4804\u001b[0m       0.4437        0.7366  10.8279\n",
      "      3        0.4827       0.4438        0.7395  10.8595\n",
      "      4        0.4817       \u001b[32m0.5579\u001b[0m        0.7268  10.8690\n",
      "      5        \u001b[36m0.4801\u001b[0m       0.5420        0.7326  10.7895\n",
      "      6        \u001b[36m0.4786\u001b[0m       \u001b[32m0.5698\u001b[0m        0.7890  10.8278\n",
      "      7        0.4817       0.4457        0.8404  10.7787\n",
      "      8        0.4829       0.4458        0.8352  10.8168\n",
      "      9        0.4798       0.5348        0.7821  10.7504\n",
      "     10        0.4811       0.5683        0.9025  10.7023\n",
      "     11        0.4869       0.4442        0.7485  10.8277\n",
      "     12        0.4867       0.5481        0.9145  10.6752\n",
      "     13        0.4850       0.4462        0.9232  10.7520\n",
      "     14        0.4859       0.4461        0.8984  10.7912\n",
      "     15        0.4856       \u001b[32m0.5712\u001b[0m        0.9501  10.7943\n",
      "     16        0.4861       0.4457        0.9559  10.7322\n",
      "     17        0.4850       0.5584        0.8645  10.7891\n",
      "     18        0.4913       0.4461        1.1379  10.9048\n",
      "     19        0.4861       0.4461        1.1507  10.7164\n",
      "     20        0.4872       0.4465        1.3833  10.8660\n",
      "     21        0.4865       0.5668        1.6385  10.8517\n",
      "     22        0.4865       0.5573        0.9521  10.7591\n",
      "     23        0.4851       0.5693        1.0596  10.8080\n",
      "     24        0.4850       0.4454        0.8513  10.7517\n",
      "     25        0.4869       0.4453        0.9033  10.8163\n",
      "     26        0.4874       0.5115        0.8653  10.8346\n",
      "     27        0.4870       0.4455        0.8164  10.6540\n",
      "     28        0.4859       0.4455        0.9808  10.6990\n",
      "     29        0.4852       0.4454        0.8651  10.7417\n",
      "     30        0.4846       0.4456        0.8999  10.7993\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5371\u001b[0m       \u001b[32m0.4658\u001b[0m        \u001b[35m9.2208\u001b[0m  10.8036\n",
      "      2        \u001b[36m0.5257\u001b[0m       0.4287       14.3557  10.7837\n",
      "      3        \u001b[36m0.5247\u001b[0m       \u001b[32m0.4733\u001b[0m       43.9103  10.8336\n",
      "      4        \u001b[36m0.5230\u001b[0m       0.4699       30.6146  10.7567\n",
      "      5        \u001b[36m0.5230\u001b[0m       0.4550       34.7282  10.7682\n",
      "      6        \u001b[36m0.5216\u001b[0m       0.4721       45.7636  10.8633\n",
      "      7        0.5230       0.4599       47.0016  10.6641\n",
      "      8        0.5222       0.4633       63.2310  10.7663\n",
      "      9        0.5267       0.4297       74.0828  10.7968\n",
      "     10        0.5235       0.4492       84.4737  10.7930\n",
      "     11        \u001b[36m0.5205\u001b[0m       0.4423      122.6458  10.7994\n",
      "     12        0.5225       0.4671      143.3518  10.7050\n",
      "     13        0.5248       0.4640       98.2242  10.8414\n",
      "     14        0.5216       0.4726      137.3118  10.7006\n",
      "     15        0.5213       0.4638      220.3345  10.7906\n",
      "     16        0.5233       \u001b[32m0.4833\u001b[0m      159.5126  10.7169\n",
      "     17        0.5220       0.4671       98.7800  10.7644\n",
      "     18        0.5235       0.4639      189.3840  10.7964\n",
      "     19        \u001b[36m0.5202\u001b[0m       0.4244      216.2864  10.7105\n",
      "     20        0.5203       0.4679      154.9098  10.7631\n",
      "     21        \u001b[36m0.5198\u001b[0m       0.4676      228.2104  10.8785\n",
      "     22        0.5209       0.4571      220.6153  10.7183\n",
      "     23        0.5232       0.4656      229.7097  10.8261\n",
      "     24        \u001b[36m0.5192\u001b[0m       0.4745      301.6812  10.7034\n",
      "     25        \u001b[36m0.5186\u001b[0m       0.4779      225.8431  10.7951\n",
      "     26        0.5215       0.4689      334.5776  10.8215\n",
      "     27        0.5189       0.4643      359.9368  10.7528\n",
      "     28        0.5206       0.4481      630.1215  10.8676\n",
      "     29        0.5188       0.4632      442.8384  10.6955\n",
      "     30        0.5210       0.4737      400.8214  10.6832\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5620\u001b[0m       \u001b[32m0.5585\u001b[0m        \u001b[35m0.9598\u001b[0m  10.8783\n",
      "      2        \u001b[36m0.5487\u001b[0m       0.5342        3.7290  10.9153\n",
      "      3        \u001b[36m0.5482\u001b[0m       0.5351        3.3103  10.8880\n",
      "      4        \u001b[36m0.5478\u001b[0m       0.5394        4.7385  10.7518\n",
      "      5        \u001b[36m0.5462\u001b[0m       0.5319        5.3900  10.7407\n",
      "      6        0.5506       0.5411        7.4729  10.7974\n",
      "      7        0.5512       0.5334        5.6594  10.7479\n",
      "      8        0.5479       0.5041       15.1547  10.7112\n",
      "      9        0.5526       0.5076       27.9738  10.7507\n",
      "     10        0.5518       0.5141       11.2258  10.7715\n",
      "     11        0.5505       0.5543       13.4720  10.7968\n",
      "     12        0.5500       0.5389       23.0225  10.7992\n",
      "     13        0.5485       0.5468       13.3962  10.8236\n",
      "     14        0.5486       0.5325       24.6789  10.7965\n",
      "     15        \u001b[36m0.5440\u001b[0m       0.5554       25.4749  10.8228\n",
      "     16        0.5576       \u001b[32m0.5705\u001b[0m        9.9329  10.8052\n",
      "     17        0.5521       0.5433       26.3496  10.8176\n",
      "     18        0.5502       0.5186       39.9657  10.9132\n",
      "     19        0.5519       0.5298       38.2742  10.6862\n",
      "     20        0.5594       0.5157       31.7262  10.8057\n",
      "     21        0.5534       0.5529       25.5241  10.7281\n",
      "     22        0.5537       0.5276       49.2494  10.8634\n",
      "     23        0.5514       0.5437       58.8155  10.8941\n",
      "     24        0.5520       0.5177       82.4818  10.7746\n",
      "     25        0.5583       0.5111       39.4868  10.7842\n",
      "     26        0.5555       0.5293       27.6735  10.9127\n",
      "     27        0.5507       0.5323       58.7054  10.6932\n",
      "     28        0.5518       0.5017       52.0068  10.6773\n",
      "     29        0.5515       0.5144       62.6039  10.7468\n",
      "     30        0.5583       0.5152       46.0545  10.8691\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4803\u001b[0m       \u001b[32m0.4446\u001b[0m        \u001b[35m0.7359\u001b[0m  10.8250\n",
      "      2        \u001b[36m0.4780\u001b[0m       \u001b[32m0.4701\u001b[0m        0.7708  10.7796\n",
      "      3        0.4887       0.4442        0.7475  10.7872\n",
      "      4        0.4814       \u001b[32m0.5430\u001b[0m        0.7617  10.9033\n",
      "      5        0.4831       \u001b[32m0.5494\u001b[0m        0.7411  10.8582\n",
      "      6        0.4807       0.5391        \u001b[35m0.7225\u001b[0m  10.7951\n",
      "      7        0.4853       \u001b[32m0.5607\u001b[0m        \u001b[35m0.7028\u001b[0m  10.8498\n",
      "      8        0.4862       0.4443        0.8139  10.8820\n",
      "      9        0.4860       0.4453        0.7540  10.6575\n",
      "     10        0.4883       0.5353        0.7199  10.7239\n",
      "     11        0.4882       0.4448        0.8417  10.8218\n",
      "     12        0.4904       0.5413        0.8345  10.8461\n",
      "     13        0.4855       0.4455        0.8282  10.6963\n",
      "     14        0.4871       0.4461        0.8966  10.7621\n",
      "     15        0.4883       0.4462        0.8654  10.7754\n",
      "     16        0.4856       0.5362        0.8727  10.7836\n",
      "     17        0.4875       0.4467        0.9247  10.7288\n",
      "     18        0.4874       0.5498        0.7857  10.7870\n",
      "     19        0.4896       0.4443        0.7256  10.8757\n",
      "     20        0.4906       0.4933        0.7681  10.8594\n",
      "     21        0.4876       0.4450        0.8040  10.6274\n",
      "     22        0.4886       0.4451        0.7706  10.7222\n",
      "     23        0.4874       0.4459        1.2939  10.7686\n",
      "     24        0.4920       0.4454        0.7654  10.8865\n",
      "     25        0.4858       0.4451        0.7681  10.7140\n",
      "     26        0.4851       0.4454        0.7707  10.7325\n",
      "     27        0.4860       \u001b[32m0.5690\u001b[0m        0.7263  10.9062\n",
      "     28        0.4857       \u001b[32m0.5907\u001b[0m        0.7029  10.6977\n",
      "     29        0.4883       0.4462        0.9834  10.7134\n",
      "     30        0.4857       0.4462        0.9688  10.7875\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6066\u001b[0m       \u001b[32m0.6242\u001b[0m        \u001b[35m2.8155\u001b[0m  10.5529\n",
      "      2        \u001b[36m0.5996\u001b[0m       \u001b[32m0.6294\u001b[0m        4.8099  10.6209\n",
      "      3        0.6063       0.6157       12.1642  10.6025\n",
      "      4        0.6061       \u001b[32m0.6353\u001b[0m        4.4663  10.7235\n",
      "      5        0.6013       0.6302       10.1111  10.5748\n",
      "      6        \u001b[36m0.5969\u001b[0m       0.6120        4.2676  10.6780\n",
      "      7        0.6102       0.6110       11.1784  10.6226\n",
      "      8        0.5978       \u001b[32m0.6393\u001b[0m       10.8066  10.5912\n",
      "      9        0.6069       \u001b[32m0.6528\u001b[0m       19.5907  10.4034\n",
      "     10        0.6175       0.6475       13.9028  10.5479\n",
      "     11        0.6142       0.6027       17.3749  10.5821\n",
      "     12        0.6013       0.6217       23.1144  10.6124\n",
      "     13        0.6017       0.5789        \u001b[35m1.8221\u001b[0m  10.6026\n",
      "     14        0.6062       0.5488       72.9243  10.6915\n",
      "     15        0.6191       0.5764       19.3980  10.6623\n",
      "     16        0.5994       0.6204       22.1260  10.5987\n",
      "     17        \u001b[36m0.5938\u001b[0m       0.5865       11.9537  10.4957\n",
      "     18        \u001b[36m0.5918\u001b[0m       0.5891        9.6915  10.6331\n",
      "     19        0.5956       0.6131       35.4478  10.6011\n",
      "     20        0.5968       0.6249       17.5882  10.5861\n",
      "     21        0.5967       0.6252       55.0861  10.6112\n",
      "     22        0.6052       0.5301       28.4190  10.5818\n",
      "     23        0.6139       0.6191       17.8042  10.5659\n",
      "     24        0.6272       0.5755       12.9113  10.6800\n",
      "     25        0.6307       0.6383       16.7904  10.7062\n",
      "     26        0.6265       0.6201       13.8426  10.5783\n",
      "     27        0.6249       0.6246        7.8557  10.4495\n",
      "     28        0.6260       0.6242       19.1487  10.5732\n",
      "     29        0.6276       0.6239        9.2118  10.5695\n",
      "     30        0.6262       0.5962        4.7179  10.6173\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6429\u001b[0m       \u001b[32m0.5558\u001b[0m        \u001b[35m0.6849\u001b[0m  10.5128\n",
      "      2        \u001b[36m0.6387\u001b[0m       0.5554        0.7064  10.6172\n",
      "      3        0.6487       \u001b[32m0.5563\u001b[0m        0.6868  10.7683\n",
      "      4        0.6517       0.5563        0.6872  10.6751\n",
      "      5        0.6523       0.5563        0.6869  10.5938\n",
      "      6        0.6506       0.5563        0.6869  10.5986\n",
      "      7        0.6524       0.5563        0.6893  10.6025\n",
      "      8        0.6554       0.5563        0.6871  10.5666\n",
      "      9        0.6525       0.5563        0.6913  10.6678\n",
      "     10        0.6529       0.5563        0.6911  10.5232\n",
      "     11        0.6502       0.5563        0.6870  10.5882\n",
      "     12        0.6495       0.5563        0.6871  10.4087\n",
      "     13        0.6505       0.5563        0.6952  10.5977\n",
      "     14        0.6486       0.5563        0.6883  10.7412\n",
      "     15        0.6498       0.5563        0.6875  10.4761\n",
      "     16        0.6502       0.5563        0.6878  10.6940\n",
      "     17        0.6487       0.5563        0.6868  10.4902\n",
      "     18        0.6490       0.5563        0.6898  10.4881\n",
      "     19        0.6475       0.5563        0.6886  10.6525\n",
      "     20        0.6470       0.5563        0.6931  10.4613\n",
      "     21        0.6496       0.5563        0.6891  10.6667\n",
      "     22        0.6474       0.5563        0.6891  10.7070\n",
      "     23        0.6473       0.5563        0.6869  10.5845\n",
      "     24        0.6486       0.5563        0.6868  10.4192\n",
      "     25        0.6508       0.5563        0.6886  10.7415\n",
      "     26        0.6475       0.5563        0.6937  10.5244\n",
      "     27        0.6491       0.5563        0.6869  10.6508\n",
      "     28        0.6498       0.5563        0.6984  10.4491\n",
      "     29        0.6463       0.5563        0.6868  10.6045\n",
      "     30        0.6482       0.5563        0.6905  10.6833\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5161\u001b[0m       \u001b[32m0.4437\u001b[0m        \u001b[35m0.7205\u001b[0m  10.5950\n",
      "      2        \u001b[36m0.5091\u001b[0m       \u001b[32m0.4437\u001b[0m        0.7256  10.6588\n",
      "      3        \u001b[36m0.5087\u001b[0m       0.4437        0.7369  10.6245\n",
      "      4        0.5128       \u001b[32m0.4443\u001b[0m        0.7255  10.6694\n",
      "      5        0.5117       \u001b[32m0.4447\u001b[0m        0.9512  10.7216\n",
      "      6        0.5094       \u001b[32m0.4465\u001b[0m        1.0170  10.6760\n",
      "      7        \u001b[36m0.5084\u001b[0m       0.4465        1.0216  10.6712\n",
      "      8        0.5091       0.4463        0.9574  10.6743\n",
      "      9        \u001b[36m0.5080\u001b[0m       0.4463        0.9907  10.5793\n",
      "     10        \u001b[36m0.5076\u001b[0m       0.4463        0.9941  10.4272\n",
      "     11        0.5089       0.4453        0.9877  10.4280\n",
      "     12        0.5094       0.4435        0.7657  10.5508\n",
      "     13        0.5086       0.4435        0.7755  10.5976\n",
      "     14        \u001b[36m0.5075\u001b[0m       0.4437        0.7362  10.6623\n",
      "     15        \u001b[36m0.5074\u001b[0m       0.4437        0.7404  10.6276\n",
      "     16        0.5092       0.4441        0.7871  10.5521\n",
      "     17        \u001b[36m0.5074\u001b[0m       0.4459        0.9424  10.7278\n",
      "     18        \u001b[36m0.5071\u001b[0m       0.4447        0.8640  10.5752\n",
      "     19        0.5073       0.4448        0.8170  10.6503\n",
      "     20        \u001b[36m0.5068\u001b[0m       0.4450        0.7826  10.5905\n",
      "     21        0.5071       0.4450        0.7901  10.5038\n",
      "     22        0.5069       0.4449        0.8056  10.6319\n",
      "     23        0.5070       0.4450        0.7914  10.7527\n",
      "     24        0.5072       0.4447        0.8124  10.5513\n",
      "     25        0.5072       0.4448        0.7608  10.4258\n",
      "     26        0.5070       0.4448        0.7718  10.5304\n",
      "     27        0.5071       0.4448        0.7557  10.6574\n",
      "     28        0.5072       0.4447        0.7953  10.6077\n",
      "     29        0.5073       0.4447        0.7990  10.6079\n",
      "     30        0.5073       0.4447        0.8042  10.5262\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5718\u001b[0m       \u001b[32m0.6449\u001b[0m        \u001b[35m8.2533\u001b[0m  10.5408\n",
      "      2        0.5723       0.6379       17.5540  10.7427\n",
      "      3        0.5785       0.6203       48.6289  10.5920\n",
      "      4        0.5736       0.6011       63.9337  10.5545\n",
      "      5        \u001b[36m0.5710\u001b[0m       0.6369       19.1493  10.4886\n",
      "      6        0.5726       \u001b[32m0.6451\u001b[0m       46.5559  10.5423\n",
      "      7        \u001b[36m0.5663\u001b[0m       0.6412       30.9416  10.8299\n",
      "      8        0.5800       0.6112      131.3728  10.4993\n",
      "      9        0.5844       0.6013       89.5489  10.5491\n",
      "     10        0.5728       0.5752       97.6757  10.5978\n",
      "     11        0.5762       0.6176      124.7129  10.6281\n",
      "     12        0.5748       0.6270       91.0169  10.7371\n",
      "     13        0.5759       0.6270      170.7355  10.5836\n",
      "     14        0.5856       0.5887       97.7181  10.6574\n",
      "     15        0.5807       0.5720      108.4120  10.5307\n",
      "     16        0.5788       0.6232      210.1326  10.6039\n",
      "     17        0.5812       0.6372      173.2686  10.6083\n",
      "     18        0.5704       0.6321      219.1999  10.5482\n",
      "     19        0.5809       0.6335      100.5813  10.5608\n",
      "     20        0.5791       \u001b[32m0.6466\u001b[0m      203.9593  10.7013\n",
      "     21        0.5887       0.6100      273.4649  10.5641\n",
      "     22        0.6040       0.5858      182.4745  10.5265\n",
      "     23        0.5918       \u001b[32m0.6532\u001b[0m      184.4788  10.3552\n",
      "     24        0.5795       0.6016      229.5678  10.6245\n",
      "     25        0.5767       0.6408      200.4202  10.5913\n",
      "     26        0.5816       0.6484      300.4327  10.6149\n",
      "     27        0.5816       0.5923      220.9779  10.6997\n",
      "     28        0.5756       0.6407      195.5297  10.6381\n",
      "     29        0.5935       0.5984      515.9975  10.4694\n",
      "     30        0.5980       0.6073      534.1304  10.6243\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6309\u001b[0m       \u001b[32m0.5560\u001b[0m        \u001b[35m0.7028\u001b[0m  10.5511\n",
      "      2        0.6484       \u001b[32m0.5563\u001b[0m        \u001b[35m0.6956\u001b[0m  10.5961\n",
      "      3        0.6542       0.5562        \u001b[35m0.6890\u001b[0m  10.6015\n",
      "      4        0.6513       \u001b[32m0.5563\u001b[0m        0.6898  10.6240\n",
      "      5        0.6538       \u001b[32m0.5563\u001b[0m        \u001b[35m0.6869\u001b[0m  10.5987\n",
      "      6        0.6500       0.5563        0.6893  10.6046\n",
      "      7        0.6500       0.5563        0.6915  10.5690\n",
      "      8        0.6507       0.5563        0.6888  10.5069\n",
      "      9        0.6541       0.5563        \u001b[35m0.6868\u001b[0m  10.5886\n",
      "     10        0.6493       0.5563        0.6885  10.6938\n",
      "     11        0.6486       0.5563        0.6884  10.5499\n",
      "     12        0.6473       0.5563        0.6868  10.6401\n",
      "     13        0.6472       0.5563        0.6869  10.4864\n",
      "     14        0.6467       0.5563        0.6924  10.6032\n",
      "     15        0.6456       0.5563        0.6901  10.5556\n",
      "     16        0.6475       0.5563        0.6884  10.6209\n",
      "     17        0.6503       0.5563        0.6876  10.3576\n",
      "     18        0.6456       0.5563        0.6926  10.5282\n",
      "     19        0.6469       0.5563        0.6875  10.6838\n",
      "     20        0.6490       0.5563        0.6895  10.3933\n",
      "     21        0.6448       0.5563        0.6877  10.5922\n",
      "     22        0.6484       0.5563        0.6879  10.6762\n",
      "     23        0.6477       0.5563        0.6904  10.5112\n",
      "     24        0.6446       0.5563        0.6908  10.6050\n",
      "     25        0.6438       0.5563        0.6868  10.5655\n",
      "     26        0.6462       0.5563        0.6881  10.6256\n",
      "     27        0.6475       0.5563        0.6889  10.5907\n",
      "     28        0.6468       0.5563        0.6967  10.6279\n",
      "     29        0.6450       0.5563        0.6878  10.5101\n",
      "     30        0.6412       0.5563        0.6883  10.4762\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4994\u001b[0m       \u001b[32m0.5335\u001b[0m        \u001b[35m0.7183\u001b[0m  10.4918\n",
      "      2        \u001b[36m0.4954\u001b[0m       \u001b[32m0.5897\u001b[0m        0.7326  10.5864\n",
      "      3        0.5114       0.4443        0.9420  10.6275\n",
      "      4        0.5088       0.4441        0.7273  10.7548\n",
      "      5        0.5053       0.4446        0.7478  10.4901\n",
      "      6        0.5134       0.4438        0.7396  10.6672\n",
      "      7        0.5089       0.4438        0.7944  10.4940\n",
      "      8        0.5056       0.4439        0.7462  10.4691\n",
      "      9        0.5057       0.4442        0.9150  10.7012\n",
      "     10        0.5054       0.4442        0.9565  10.6770\n",
      "     11        0.5055       0.4441        0.9717  10.5900\n",
      "     12        0.5069       0.4466        2.1538  10.6471\n",
      "     13        0.5052       0.4466        2.1207  10.5754\n",
      "     14        0.5053       0.4466        2.2284  10.6239\n",
      "     15        0.5055       0.4466        2.2034  10.3808\n",
      "     16        0.5053       0.4466        2.2194  10.4914\n",
      "     17        0.5057       0.4453        2.7682  10.5706\n",
      "     18        0.5073       0.4456        1.6091  10.5982\n",
      "     19        0.5053       0.4456        1.6738  10.5995\n",
      "     20        0.5054       0.4458        2.6004  10.5940\n",
      "     21        0.5123       0.4463        3.3157  10.5817\n",
      "     22        0.5077       0.4457        1.1500  10.4208\n",
      "     23        0.5057       0.4457        1.1948  10.4998\n",
      "     24        0.5053       0.4457        1.1744  10.7648\n",
      "     25        0.5061       0.4462        2.0964  10.4435\n",
      "     26        0.5051       0.4462        2.0693  10.6200\n",
      "     27        0.5055       0.4462        2.0686  10.7281\n",
      "     28        0.5054       0.4454        2.7025  10.6584\n",
      "     29        0.5054       0.4454        2.7007  10.5035\n",
      "     30        0.5055       0.4454        2.7068  10.6256\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5716\u001b[0m       \u001b[32m0.6340\u001b[0m       \u001b[35m16.5883\u001b[0m  10.6939\n",
      "      2        0.5826       0.6070       26.3660  10.6157\n",
      "      3        0.5831       0.6035       23.4491  10.5923\n",
      "      4        0.5796       0.6054       63.4841  10.4096\n",
      "      5        0.5789       0.5950       71.6611  10.6110\n",
      "      6        0.5895       0.5927       97.3684  10.6188\n",
      "      7        0.5785       0.6165      103.9244  10.6182\n",
      "      8        \u001b[36m0.5680\u001b[0m       0.6067      181.2550  10.6563\n",
      "      9        0.5769       \u001b[32m0.6608\u001b[0m       75.1896  10.6278\n",
      "     10        0.5800       0.6086      237.8566  10.6286\n",
      "     11        0.5958       0.6109      196.8645  10.6586\n",
      "     12        0.5738       0.6146      117.0839  10.4424\n",
      "     13        0.5736       0.6017      336.3712  10.5795\n",
      "     14        0.5866       0.6028      324.4590  10.5673\n",
      "     15        0.5890       0.6000      533.3693  10.5835\n",
      "     16        0.5955       0.5857      377.6528  10.5505\n",
      "     17        0.5811       0.5827      304.6195  10.7076\n",
      "     18        0.5802       0.5984      254.7745  10.5661\n",
      "     19        0.5836       0.6417      410.5277  10.3927\n",
      "     20        0.5769       0.6047      211.1746  10.6960\n",
      "     21        0.5764       0.6453      194.4260  10.5781\n",
      "     22        0.5729       0.6075      451.1900  10.6288\n",
      "     23        0.5708       0.6455      364.9060  10.6611\n",
      "     24        0.5711       0.6462      602.3038  10.5064\n",
      "     25        0.5707       0.5993      393.2590  10.6469\n",
      "     26        0.5707       0.6574      590.5627  10.4676\n",
      "     27        0.5950       0.5632      450.6430  10.5970\n",
      "     28        0.5984       0.5940      347.4002  10.5924\n",
      "     29        0.5910       0.5950      246.0001  10.6130\n",
      "     30        0.5757       0.5989      513.0931  10.7646\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6239\u001b[0m       \u001b[32m0.5560\u001b[0m        \u001b[35m0.6933\u001b[0m  10.7005\n",
      "      2        0.6248       \u001b[32m0.5561\u001b[0m        0.7042  10.4608\n",
      "      3        0.6350       \u001b[32m0.5561\u001b[0m        \u001b[35m0.6887\u001b[0m  10.6491\n",
      "      4        0.6323       \u001b[32m0.5563\u001b[0m        \u001b[35m0.6884\u001b[0m  10.6215\n",
      "      5        0.6436       0.5563        0.6886  10.6298\n",
      "      6        0.6370       0.5563        \u001b[35m0.6873\u001b[0m  10.5854\n",
      "      7        0.6380       0.5563        0.6970  10.5393\n",
      "      8        0.6336       0.5563        0.6953  10.5016\n",
      "      9        0.6365       0.5563        0.6897  10.4920\n",
      "     10        0.6341       0.5563        0.6928  10.6791\n",
      "     11        0.6345       0.5563        0.6903  10.5420\n",
      "     12        0.6349       0.5563        0.6874  10.5974\n",
      "     13        0.6346       0.5563        0.6917  10.5188\n",
      "     14        0.6337       0.5563        0.7028  10.5417\n",
      "     15        0.6377       0.5563        \u001b[35m0.6868\u001b[0m  10.5334\n",
      "     16        0.6361       0.5563        0.6881  10.6741\n",
      "     17        0.6336       0.5563        0.6885  10.6501\n",
      "     18        0.6367       0.5563        0.7132  10.5163\n",
      "     19        0.6323       0.5563        0.6932  10.3788\n",
      "     20        0.6385       0.5563        0.6919  10.5986\n",
      "     21        0.6337       0.5563        0.6895  10.6616\n",
      "     22        0.6428       0.5563        0.6934  10.5797\n",
      "     23        0.6355       0.5563        0.6922  10.6515\n",
      "     24        0.6349       0.5563        0.6954  10.4858\n",
      "     25        0.6327       0.5563        0.6926  10.5828\n",
      "     26        0.6361       0.5563        0.6882  10.6011\n",
      "     27        0.6325       0.5563        0.6948  10.6024\n",
      "     28        0.6330       0.5563        0.6963  10.6131\n",
      "     29        0.6348       0.5563        0.6874  10.6165\n",
      "     30        0.6327       0.5563        0.6934  10.4406\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5006\u001b[0m       \u001b[32m0.4439\u001b[0m        \u001b[35m0.7339\u001b[0m  10.5205\n",
      "      2        0.5039       \u001b[32m0.4444\u001b[0m        0.8688  10.4714\n",
      "      3        0.5043       0.4438        0.8000  10.6554\n",
      "      4        \u001b[36m0.4994\u001b[0m       0.4440        0.7886  10.6054\n",
      "      5        0.5037       \u001b[32m0.4452\u001b[0m        0.9343  10.6203\n",
      "      6        \u001b[36m0.4994\u001b[0m       0.4452        1.0045  10.5888\n",
      "      7        0.4995       \u001b[32m0.4454\u001b[0m        1.0646  10.6234\n",
      "      8        0.5079       0.4440        \u001b[35m0.7174\u001b[0m  10.6025\n",
      "      9        0.5019       0.4443        1.5504  10.4492\n",
      "     10        \u001b[36m0.4994\u001b[0m       0.4437        0.7318  10.6533\n",
      "     11        0.5118       0.4436        0.7499  10.5656\n",
      "     12        \u001b[36m0.4994\u001b[0m       0.4437        0.7692  10.3959\n",
      "     13        0.4997       0.4436        0.7224  10.5551\n",
      "     14        \u001b[36m0.4993\u001b[0m       0.4439        0.7677  10.6610\n",
      "     15        \u001b[36m0.4992\u001b[0m       0.4439        0.7823  10.4900\n",
      "     16        \u001b[36m0.4991\u001b[0m       0.4448        0.8870  10.5270\n",
      "     17        \u001b[36m0.4989\u001b[0m       0.4450        1.2498  10.5860\n",
      "     18        0.4992       0.4450        1.2571  10.6483\n",
      "     19        0.4990       0.4449        1.2657  10.5065\n",
      "     20        0.4990       0.4450        1.2585  10.5860\n",
      "     21        0.4991       0.4450        1.2551  10.5183\n",
      "     22        \u001b[36m0.4989\u001b[0m       \u001b[32m0.4462\u001b[0m        1.4167  10.5798\n",
      "     23        0.4990       \u001b[32m0.4465\u001b[0m        3.9226  10.5762\n",
      "     24        0.4989       0.4465        3.9274  10.6101\n",
      "     25        \u001b[36m0.4987\u001b[0m       \u001b[32m0.4468\u001b[0m        4.6586  10.6702\n",
      "     26        0.4993       0.4465        5.2085  10.6107\n",
      "     27        0.5049       0.4462        1.8916  10.5872\n",
      "     28        0.4994       0.4462        1.8633  10.6731\n",
      "     29        0.5000       0.4462        1.8394  10.5670\n",
      "     30        0.4992       \u001b[32m0.4470\u001b[0m        3.4525  10.6621\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5774\u001b[0m       \u001b[32m0.6591\u001b[0m        \u001b[35m4.0922\u001b[0m  10.6227\n",
      "      2        \u001b[36m0.5719\u001b[0m       0.6199       27.0947  10.7137\n",
      "      3        0.5728       0.5628       16.0973  10.5973\n",
      "      4        \u001b[36m0.5711\u001b[0m       0.6244        9.1997  10.6313\n",
      "      5        0.5796       0.6088       58.7222  10.7014\n",
      "      6        0.5730       0.5784       32.8459  10.5854\n",
      "      7        0.5728       0.5928       28.9019  10.5902\n",
      "      8        0.5785       0.6312       26.7193  10.6117\n",
      "      9        0.5775       0.6044       75.5440  10.5810\n",
      "     10        0.5778       0.6284       54.9006  10.6097\n",
      "     11        0.5783       0.6390       43.4138  10.6889\n",
      "     12        0.5778       0.6343      103.1828  10.4988\n",
      "     13        0.5951       0.6175       55.8961  10.5212\n",
      "     14        0.5931       0.6336       34.1337  10.3695\n",
      "     15        0.5788       0.6147       88.3288  10.5972\n",
      "     16        0.5797       0.5942       42.8712  10.6933\n",
      "     17        0.5882       0.5881      142.2683  10.4471\n",
      "     18        0.6031       0.5935       17.0994  10.5834\n",
      "     19        0.5968       0.6163       26.0023  10.6539\n",
      "     20        0.6000       0.6073       90.8016  10.4879\n",
      "     21        0.6044       0.6387       51.4079  10.6338\n",
      "     22        0.6012       0.6416       40.1481  10.3996\n",
      "     23        0.6041       0.6137       31.8406  10.5185\n",
      "     24        0.6104       0.6027      113.9563  10.5483\n",
      "     25        0.6281       0.5896       36.8803  10.5920\n",
      "     26        0.6256       0.5755       54.8566  10.7082\n",
      "     27        0.6000       0.6180      132.1386  10.5222\n",
      "     28        0.6023       0.4913       66.9065  10.5896\n",
      "     29        0.6035       0.5074       63.7460  10.6537\n",
      "     30        0.6014       0.4879       45.7229  10.4318\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6345\u001b[0m       \u001b[32m0.5561\u001b[0m        \u001b[35m0.6769\u001b[0m  10.6273\n",
      "      2        \u001b[36m0.6310\u001b[0m       \u001b[32m0.5562\u001b[0m        \u001b[35m0.6768\u001b[0m  10.4738\n",
      "      3        \u001b[36m0.6235\u001b[0m       \u001b[32m0.5562\u001b[0m        \u001b[35m0.6680\u001b[0m  10.5635\n",
      "      4        0.6334       \u001b[32m0.5804\u001b[0m        0.6727  10.7126\n",
      "      5        0.6279       0.5715        0.6807  10.4398\n",
      "      6        0.6374       0.5563        0.6735  10.5556\n",
      "      7        0.6282       \u001b[32m0.5820\u001b[0m        \u001b[35m0.6674\u001b[0m  10.6049\n",
      "      8        0.6450       0.5563        0.6868  10.6500\n",
      "      9        0.6411       \u001b[32m0.6028\u001b[0m        \u001b[35m0.6661\u001b[0m  10.6488\n",
      "     10        0.6297       0.5563        0.6688  10.4226\n",
      "     11        0.6357       0.5563        0.6868  10.5420\n",
      "     12        0.6481       0.5563        0.6954  10.5568\n",
      "     13        0.6519       0.5563        0.6977  10.6482\n",
      "     14        0.6557       0.5563        0.6869  10.4849\n",
      "     15        0.6521       0.5563        0.6919  10.5652\n",
      "     16        0.6521       0.5563        0.6899  10.5415\n",
      "     17        0.6519       0.5563        0.6893  10.5836\n",
      "     18        0.6540       0.5563        0.6869  10.6087\n",
      "     19        0.6532       0.5563        0.6942  10.5512\n",
      "     20        0.6507       0.5563        0.6869  10.4172\n",
      "     21        0.6538       0.5563        0.6870  10.5761\n",
      "     22        0.6531       0.5563        0.6870  10.6775\n",
      "     23        0.6552       0.5563        0.6872  10.4402\n",
      "     24        0.6510       0.5563        0.6874  10.5808\n",
      "     25        0.6507       0.5563        0.6868  10.5742\n",
      "     26        0.6508       0.5563        0.6952  10.7273\n",
      "     27        0.6513       0.5563        0.6896  10.4190\n",
      "     28        0.6520       0.5563        0.6879  10.6130\n",
      "     29        0.6507       0.5563        0.6928  10.6915\n",
      "     30        0.6513       0.5563        0.6911  10.5482\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5087\u001b[0m       \u001b[32m0.4465\u001b[0m        \u001b[35m0.9626\u001b[0m  10.4807\n",
      "      2        \u001b[36m0.5029\u001b[0m       0.4436        \u001b[35m0.7229\u001b[0m  10.4356\n",
      "      3        \u001b[36m0.5001\u001b[0m       0.4444        0.7728  10.6086\n",
      "      4        0.5019       0.4459        1.2202  10.6438\n",
      "      5        \u001b[36m0.4998\u001b[0m       0.4459        1.2204  10.7147\n",
      "      6        0.5065       0.4438        0.7859  10.5552\n",
      "      7        0.5016       0.4440        0.7647  10.5669\n",
      "      8        0.5000       0.4440        0.8293  10.6414\n",
      "      9        \u001b[36m0.4997\u001b[0m       0.4454        0.9200  10.5869\n",
      "     10        \u001b[36m0.4996\u001b[0m       0.4454        0.9237  10.6347\n",
      "     11        0.5008       0.4461        3.8236  10.5950\n",
      "     12        0.5001       0.4465        1.4350  10.6464\n",
      "     13        0.5001       \u001b[32m0.4467\u001b[0m        1.4995  10.5810\n",
      "     14        0.5019       0.4466        2.2203  10.6682\n",
      "     15        0.4997       \u001b[32m0.4467\u001b[0m        2.1743  10.7060\n",
      "     16        0.5001       0.4466        3.6141  10.4891\n",
      "     17        0.4999       0.4466        5.8378  10.3809\n",
      "     18        0.4998       0.4466        6.2859  10.5479\n",
      "     19        0.4999       0.4466        6.2666  10.6457\n",
      "     20        0.4999       0.4466        6.2587  10.6436\n",
      "     21        0.5004       \u001b[32m0.4467\u001b[0m        7.7663  10.4316\n",
      "     22        0.4997       0.4467        7.7903  10.6010\n",
      "     23        0.4998       0.4467        7.7588  10.6717\n",
      "     24        0.4998       0.4467        7.7543  10.5398\n",
      "     25        0.4997       0.4467        7.7746  10.4684\n",
      "     26        0.4999       0.4467        7.9471  10.4116\n",
      "     27        0.4998       \u001b[32m0.4467\u001b[0m        7.9946  10.5668\n",
      "     28        0.4997       0.4467        7.9413  10.5411\n",
      "     29        0.4998       0.4467        7.9712  10.6111\n",
      "     30        0.4997       0.4467        7.9517  10.5528\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5643\u001b[0m       \u001b[32m0.6238\u001b[0m       \u001b[35m15.3871\u001b[0m  10.8154\n",
      "      2        0.5661       \u001b[32m0.6375\u001b[0m       26.2023  10.8537\n",
      "      3        0.5758       0.6085       62.0527  10.7694\n",
      "      4        0.5757       0.5843      108.0704  10.8670\n",
      "      5        0.5751       0.6017       52.6430  10.7409\n",
      "      6        0.5713       0.5711      173.5113  10.8049\n",
      "      7        \u001b[36m0.5610\u001b[0m       0.6279      186.2236  10.6908\n",
      "      8        0.5819       0.5868      154.1970  10.7214\n",
      "      9        0.5641       0.5271      235.7181  10.6476\n",
      "     10        0.5824       0.5966      195.6858  10.8004\n",
      "     11        0.5685       0.6050      205.3763  10.6803\n",
      "     12        0.5768       0.6079      184.5942  10.8302\n",
      "     13        0.5699       0.5983      364.2608  10.6318\n",
      "     14        0.5622       0.6018      303.9715  10.7521\n",
      "     15        0.5754       \u001b[32m0.6408\u001b[0m      206.5597  10.7570\n",
      "     16        0.5777       0.6193      131.7600  10.8210\n",
      "     17        0.5676       \u001b[32m0.6442\u001b[0m      148.2088  10.7463\n",
      "     18        \u001b[36m0.5547\u001b[0m       0.6426      306.5271  10.8019\n",
      "     19        0.5584       0.5987      348.5179  10.6949\n",
      "     20        0.5626       0.6017      474.6776  10.8553\n",
      "     21        0.5768       0.5923      482.6944  10.6123\n",
      "     22        0.5648       0.5909      657.1346  10.7524\n",
      "     23        0.5805       0.5996      577.0783  10.7086\n",
      "     24        0.5684       0.6419      710.9875  10.8417\n",
      "     25        0.5685       \u001b[32m0.6467\u001b[0m      747.5022  10.7018\n",
      "     26        0.5886       \u001b[32m0.6472\u001b[0m      432.0355  10.8023\n",
      "     27        0.5894       0.6384      272.5849  10.7225\n",
      "     28        0.5666       0.6453      250.4161  10.6889\n",
      "     29        0.5715       0.6341      748.7056  10.8140\n",
      "     30        0.5690       0.6278      495.7619  10.6699\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6256\u001b[0m       \u001b[32m0.5563\u001b[0m        \u001b[35m0.6950\u001b[0m  10.7174\n",
      "      2        \u001b[36m0.6232\u001b[0m       0.5563        \u001b[35m0.6876\u001b[0m  10.8035\n",
      "      3        0.6233       \u001b[32m0.5564\u001b[0m        \u001b[35m0.6875\u001b[0m  10.6214\n",
      "      4        \u001b[36m0.6226\u001b[0m       0.5563        0.6981  10.7106\n",
      "      5        0.6304       0.5563        0.6958  10.8387\n",
      "      6        0.6283       0.5563        0.6929  10.5638\n",
      "      7        0.6229       0.5563        0.6877  10.7587\n",
      "      8        0.6336       0.5563        0.6878  10.7068\n",
      "      9        0.6377       0.5563        0.6928  10.8312\n",
      "     10        0.6353       0.5563        0.6879  10.6173\n",
      "     11        0.6365       0.5563        0.6885  10.7174\n",
      "     12        0.6327       0.5563        0.6891  10.6533\n",
      "     13        0.6324       0.5563        0.6934  10.7607\n",
      "     14        0.6317       0.5563        0.6903  10.6429\n",
      "     15        0.6311       0.5563        0.6904  10.7448\n",
      "     16        0.6348       0.5563        0.6934  10.6115\n",
      "     17        0.6321       0.5563        \u001b[35m0.6874\u001b[0m  10.7414\n",
      "     18        0.6362       0.5563        0.6982  10.6826\n",
      "     19        0.6328       0.5563        0.6907  10.6711\n",
      "     20        0.6362       0.5563        0.6891  10.7809\n",
      "     21        0.6387       0.5563        0.6896  10.6443\n",
      "     22        0.6349       0.5563        0.6923  10.7485\n",
      "     23        0.6306       0.5563        0.6951  10.6896\n",
      "     24        0.6328       0.5563        0.6965  10.8421\n",
      "     25        0.6309       0.5563        0.6883  10.6773\n",
      "     26        0.6318       0.5563        0.6949  10.8134\n",
      "     27        0.6381       0.5563        0.6896  10.7038\n",
      "     28        0.6310       0.5563        0.6880  10.7997\n",
      "     29        0.6319       0.5563        0.6906  10.6666\n",
      "     30        0.6311       0.5563        \u001b[35m0.6870\u001b[0m  10.7471\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4979\u001b[0m       \u001b[32m0.4445\u001b[0m        \u001b[35m0.7692\u001b[0m  10.7408\n",
      "      2        \u001b[36m0.4978\u001b[0m       \u001b[32m0.4446\u001b[0m        \u001b[35m0.7546\u001b[0m  10.7093\n",
      "      3        0.5036       \u001b[32m0.4450\u001b[0m        0.7586  10.8030\n",
      "      4        0.5032       \u001b[32m0.4450\u001b[0m        0.7637  10.6744\n",
      "      5        0.5028       0.4449        0.7861  10.7540\n",
      "      6        0.5021       \u001b[32m0.4466\u001b[0m        0.7956  10.7434\n",
      "      7        0.5043       0.4450        0.8700  10.7093\n",
      "      8        0.5024       \u001b[32m0.4530\u001b[0m        0.9425  10.7309\n",
      "      9        0.5043       0.4462        1.1630  10.6551\n",
      "     10        0.5030       0.4455        1.0493  10.7398\n",
      "     11        0.5051       0.4440        0.7640  10.6610\n",
      "     12        0.5030       0.4443        0.8437  10.7839\n",
      "     13        0.5043       0.4438        \u001b[35m0.7125\u001b[0m  10.6651\n",
      "     14        0.5038       0.4449        0.8133  10.7947\n",
      "     15        0.5035       0.4448        0.7523  10.6253\n",
      "     16        0.5021       0.4457        0.8269  10.7432\n",
      "     17        0.5022       0.4459        0.8836  10.6603\n",
      "     18        0.5022       0.4459        0.9218  10.8098\n",
      "     19        0.5020       0.4456        0.8824  10.6983\n",
      "     20        0.5168       0.4441        0.7829  10.7572\n",
      "     21        0.5119       0.4454        0.8366  10.7172\n",
      "     22        0.5079       0.4443        0.7413  10.6783\n",
      "     23        0.5045       0.4446        0.8294  10.7238\n",
      "     24        0.5026       0.4453        0.8529  10.6970\n",
      "     25        0.5023       0.4451        0.9602  10.6272\n",
      "     26        0.5022       0.4450        1.0660  10.7397\n",
      "     27        0.5025       0.4461        1.4941  10.7116\n",
      "     28        0.5062       0.4457        1.1707  10.7562\n",
      "     29        0.5027       0.4459        1.1260  10.6285\n",
      "     30        0.5021       0.4461        1.4027  10.8004\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5674\u001b[0m       \u001b[32m0.5903\u001b[0m       \u001b[35m15.3391\u001b[0m  10.6875\n",
      "      2        0.5721       \u001b[32m0.5991\u001b[0m       24.8290  10.7108\n",
      "      3        0.5729       \u001b[32m0.6234\u001b[0m        \u001b[35m9.8832\u001b[0m  10.8029\n",
      "      4        0.5685       0.5928       47.2161  10.6670\n",
      "      5        0.5758       0.5864      107.9870  10.7267\n",
      "      6        \u001b[36m0.5642\u001b[0m       0.5380       45.2903  10.6129\n",
      "      7        0.5712       0.5538       53.7892  10.7609\n",
      "      8        0.5662       0.4941      143.8329  10.6119\n",
      "      9        0.5701       0.5121      127.4929  10.7887\n",
      "     10        0.5861       0.5659       71.9251  10.6659\n",
      "     11        0.5709       0.5980      207.1961  10.7626\n",
      "     12        0.5778       0.4914      186.9169  10.6480\n",
      "     13        0.5671       0.5954      224.2824  10.7797\n",
      "     14        0.5992       0.5648      129.9435  10.5901\n",
      "     15        0.5766       0.5628      161.9667  10.6732\n",
      "     16        0.5752       0.5943      126.9572  10.8463\n",
      "     17        0.5753       0.5090      299.1777  10.5867\n",
      "     18        0.5756       0.5044       95.8375  10.7795\n",
      "     19        0.5811       0.5709      147.7524  10.6420\n",
      "     20        0.5841       0.5765      254.0830  10.7036\n",
      "     21        0.5925       0.5992      234.4678  10.6481\n",
      "     22        0.5800       0.4855       64.4267  10.7066\n",
      "     23        0.5870       0.5729      445.9685  10.5918\n",
      "     24        0.6021       0.5936      317.4855  10.6941\n",
      "     25        0.5837       0.5020      318.8368  10.8290\n",
      "     26        0.5861       0.5252      230.7235  10.5677\n",
      "     27        0.5771       0.5450      222.4412  10.7531\n",
      "     28        0.5758       0.5335      206.4151  10.6419\n",
      "     29        0.5745       0.5258      400.4603  10.6904\n",
      "     30        0.5840       0.5970      661.8947  10.6655\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6275\u001b[0m       \u001b[32m0.5481\u001b[0m        \u001b[35m0.6762\u001b[0m  10.7557\n",
      "      2        \u001b[36m0.6233\u001b[0m       \u001b[32m0.6005\u001b[0m        \u001b[35m0.6696\u001b[0m  10.7160\n",
      "      3        \u001b[36m0.6201\u001b[0m       0.5563        0.6828  10.7910\n",
      "      4        0.6209       0.5563        0.6799  10.5206\n",
      "      5        0.6268       0.5563        0.6905  10.7474\n",
      "      6        0.6403       0.5563        0.6870  10.6909\n",
      "      7        0.6402       0.5563        0.6970  10.7767\n",
      "      8        0.6359       0.5563        0.6945  10.6672\n",
      "      9        0.6365       0.5563        0.6906  10.6348\n",
      "     10        0.6358       0.5563        0.6894  10.7541\n",
      "     11        0.6451       0.5563        0.6885  10.7621\n",
      "     12        0.6358       0.5563        0.6981  10.6611\n",
      "     13        0.6428       0.5563        0.6905  10.7864\n",
      "     14        0.6333       0.5563        0.6886  10.5994\n",
      "     15        0.6369       0.5563        0.6868  10.7120\n",
      "     16        0.6327       0.5563        0.6919  10.6826\n",
      "     17        0.6331       0.5562        0.7025  10.7451\n",
      "     18        0.6375       0.5563        0.6917  10.6790\n",
      "     19        0.6352       0.5563        0.6921  10.8100\n",
      "     20        0.6342       0.5563        0.6883  10.6828\n",
      "     21        0.6292       0.5563        0.6883  10.8288\n",
      "     22        0.6372       0.5563        0.6920  10.6637\n",
      "     23        0.6307       0.5563        0.6889  10.7828\n",
      "     24        0.6288       0.5563        0.6875  10.6205\n",
      "     25        0.6514       0.5563        0.6890  10.6938\n",
      "     26        0.6296       0.5563        0.6985  10.9036\n",
      "     27        0.6307       0.5563        0.6896  10.6161\n",
      "     28        0.6426       0.5563        0.6875  10.8083\n",
      "     29        0.6314       0.5563        0.6871  10.7055\n",
      "     30        0.6306       0.5563        0.6967  10.7769\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5013\u001b[0m       \u001b[32m0.4437\u001b[0m        \u001b[35m0.7548\u001b[0m  10.7017\n",
      "      2        \u001b[36m0.4969\u001b[0m       \u001b[32m0.4484\u001b[0m        \u001b[35m0.7546\u001b[0m  10.6616\n",
      "      3        0.4982       0.4450        \u001b[35m0.7494\u001b[0m  10.8241\n",
      "      4        \u001b[36m0.4960\u001b[0m       0.4437        \u001b[35m0.7491\u001b[0m  10.7057\n",
      "      5        0.5054       0.4440        \u001b[35m0.7151\u001b[0m  10.8022\n",
      "      6        0.5005       0.4436        0.7499  10.6580\n",
      "      7        0.5039       0.4444        0.8140  10.7063\n",
      "      8        0.5000       0.4444        0.7910  10.5914\n",
      "      9        0.5001       0.4446        0.8100  10.7509\n",
      "     10        0.4995       0.4461        0.7714  10.6474\n",
      "     11        0.5007       0.4444        0.8578  10.7282\n",
      "     12        0.5039       0.4440        0.7953  10.6311\n",
      "     13        0.4992       0.4443        0.8429  10.6935\n",
      "     14        0.4984       0.4444        0.8290  10.6324\n",
      "     15        0.4983       0.4441        0.7350  10.6993\n",
      "     16        0.4980       0.4441        0.7627  10.6361\n",
      "     17        0.4981       0.4441        0.7697  10.7249\n",
      "     18        0.4981       0.4441        0.7417  10.6569\n",
      "     19        0.4982       0.4456        1.1987  10.7138\n",
      "     20        0.4981       0.4456        1.1544  10.7016\n",
      "     21        0.4983       0.4456        1.1709  10.6345\n",
      "     22        0.4984       0.4455        1.1475  10.7646\n",
      "     23        0.4981       0.4455        1.2333  10.7134\n",
      "     24        0.5062       0.4458        1.1547  10.7882\n",
      "     25        0.5011       0.4464        2.0539  10.7455\n",
      "     26        0.4985       0.4461        1.9480  10.8036\n",
      "     27        0.4981       0.4461        2.0003  10.6078\n",
      "     28        0.4982       0.4460        2.3427  10.8226\n",
      "     29        0.5072       0.4457        1.6398  11.7908\n",
      "     30        0.4984       0.4458        1.6472  12.3117\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5685\u001b[0m       \u001b[32m0.6512\u001b[0m       \u001b[35m21.1975\u001b[0m  13.1450\n",
      "      2        \u001b[36m0.5662\u001b[0m       0.6305       30.0671  12.6835\n",
      "      3        \u001b[36m0.5661\u001b[0m       0.5992       83.4452  12.6611\n",
      "      4        0.5712       0.5878      128.8444  12.6329\n",
      "      5        0.5715       0.6006      161.6887  12.6547\n",
      "      6        0.5692       0.6372      104.2659  12.6389\n",
      "      7        0.5716       0.6246       92.1803  12.6236\n",
      "      8        0.5803       0.6272      161.2812  12.6299\n",
      "      9        0.5688       0.6171      131.4650  12.6165\n",
      "     10        \u001b[36m0.5647\u001b[0m       0.6260      114.3307  12.6048\n",
      "     11        0.5699       0.6028       37.2555  12.6495\n",
      "     12        0.5658       0.6196      118.2193  12.5790\n",
      "     13        0.5677       0.6260      285.4202  12.6411\n",
      "     14        0.5810       0.6491      215.9857  12.6145\n",
      "     15        0.5713       0.6481      284.7645  12.6721\n",
      "     16        0.5706       \u001b[32m0.6518\u001b[0m      201.4757  12.6469\n",
      "     17        0.5734       \u001b[32m0.6589\u001b[0m      294.0351  12.6498\n",
      "     18        0.5738       0.6387      299.3618  12.6077\n",
      "     19        0.5796       0.5932      319.3869  12.7258\n",
      "     20        0.5796       0.5849      795.3311  12.6659\n",
      "     21        0.5956       0.5732      421.1148  12.6617\n",
      "     22        0.5761       0.6083      321.7228  12.6617\n",
      "     23        0.5656       0.6331      349.0239  12.6044\n",
      "     24        \u001b[36m0.5644\u001b[0m       0.6377      516.7530  12.5855\n",
      "     25        0.5661       0.6212      568.7909  12.6850\n",
      "     26        0.5685       0.6413      419.5251  12.6879\n",
      "     27        0.5761       0.5903      531.2062  12.6491\n",
      "     28        0.5652       0.6406      580.2947  12.6654\n",
      "     29        0.5907       0.5971      523.2577  12.5773\n",
      "     30        0.5775       0.6044      543.0512  12.6045\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6152\u001b[0m       \u001b[32m0.5567\u001b[0m        \u001b[35m0.6744\u001b[0m  13.0740\n",
      "      2        0.6160       0.5563        \u001b[35m0.6705\u001b[0m  12.6441\n",
      "      3        0.6163       0.5563        \u001b[35m0.6680\u001b[0m  12.6562\n",
      "      4        0.6162       \u001b[32m0.5968\u001b[0m        \u001b[35m0.6667\u001b[0m  12.5908\n",
      "      5        0.6324       0.5563        0.6887  12.6072\n",
      "      6        0.6392       0.5563        0.6683  12.6255\n",
      "      7        0.6246       0.5563        0.6894  12.6423\n",
      "      8        0.6453       0.5563        0.6870  12.6762\n",
      "      9        0.6369       0.5563        0.6921  12.6539\n",
      "     10        0.6313       0.5563        0.6879  12.6115\n",
      "     11        0.6325       0.5563        0.6962  12.6502\n",
      "     12        0.6353       0.5563        0.6901  12.6449\n",
      "     13        0.6349       0.5563        0.6909  12.5891\n",
      "     14        0.6376       0.5563        0.6870  12.6304\n",
      "     15        0.6332       0.5563        0.6960  12.6129\n",
      "     16        0.6328       0.5563        0.6963  12.6448\n",
      "     17        0.6334       0.5563        0.6903  12.6230\n",
      "     18        0.6299       0.5563        0.6880  12.5969\n",
      "     19        0.6286       0.5563        0.6890  12.6161\n",
      "     20        0.6303       0.5563        0.6938  12.6547\n",
      "     21        0.6315       0.5563        0.6884  12.6443\n",
      "     22        0.6339       0.5563        0.6900  12.6011\n",
      "     23        0.6318       0.5563        0.6935  12.6163\n",
      "     24        0.6320       0.5563        0.6941  12.6724\n",
      "     25        0.6292       0.5563        0.6983  12.6178\n",
      "     26        0.6354       0.5563        0.6889  12.6313\n",
      "     27        0.6302       0.5563        0.6977  12.6203\n",
      "     28        0.6359       0.5563        0.6904  12.6513\n",
      "     29        0.6272       0.5563        0.6940  12.6631\n",
      "     30        0.6372       0.5563        0.6896  12.6570\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5028\u001b[0m       \u001b[32m0.4437\u001b[0m        \u001b[35m0.7361\u001b[0m  13.0600\n",
      "      2        \u001b[36m0.4997\u001b[0m       \u001b[32m0.4440\u001b[0m        0.7652  12.6587\n",
      "      3        0.5027       0.4437        \u001b[35m0.7140\u001b[0m  12.6560\n",
      "      4        0.5014       0.4438        0.7341  12.6424\n",
      "      5        0.5096       \u001b[32m0.4447\u001b[0m        0.8142  12.6417\n",
      "      6        \u001b[36m0.4986\u001b[0m       0.4443        0.8842  12.6730\n",
      "      7        0.4990       0.4437        0.7336  12.6535\n",
      "      8        \u001b[36m0.4984\u001b[0m       0.4437        0.7195  12.6312\n",
      "      9        \u001b[36m0.4981\u001b[0m       0.4437        0.7472  12.6323\n",
      "     10        0.4995       0.4438        0.7648  10.5486\n",
      "     11        0.5005       0.4440        0.8111  10.7381\n",
      "     12        0.4987       0.4442        0.8523  10.7934\n",
      "     13        0.4982       0.4443        0.8359  10.6344\n",
      "     14        0.4984       0.4444        0.8402  10.7000\n",
      "     15        0.5160       0.4442        0.8140  10.5933\n",
      "     16        0.5017       \u001b[32m0.4448\u001b[0m        0.8110  10.7130\n",
      "     17        0.4983       \u001b[32m0.4451\u001b[0m        0.8346  10.6369\n",
      "     18        0.4987       0.4450        1.0712  10.6743\n",
      "     19        0.4984       \u001b[32m0.4463\u001b[0m        3.9487  10.6389\n",
      "     20        0.5067       0.4459        1.7658  10.6779\n",
      "     21        0.4988       \u001b[32m0.4464\u001b[0m        2.3056  10.6114\n",
      "     22        0.4982       \u001b[32m0.4467\u001b[0m        2.3467  10.7659\n",
      "     23        0.4981       \u001b[32m0.4468\u001b[0m        2.4534  10.5893\n",
      "     24        0.4981       0.4467        2.5027  10.7178\n",
      "     25        0.5083       0.4454        1.1253  10.6177\n",
      "     26        0.5047       0.4459        1.7642  10.7206\n",
      "     27        0.4985       0.4460        1.8582  10.7763\n",
      "     28        0.4984       0.4461        1.8969  10.8609\n",
      "     29        0.4988       0.4453        1.3903  10.6406\n",
      "     30        0.4987       0.4465        1.9675  10.6853\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5714\u001b[0m       \u001b[32m0.6154\u001b[0m       \u001b[35m14.3602\u001b[0m  12.7661\n",
      "      2        \u001b[36m0.5678\u001b[0m       \u001b[32m0.6187\u001b[0m       45.9830  12.7106\n",
      "      3        0.5704       \u001b[32m0.6373\u001b[0m       32.7778  12.3862\n",
      "      4        0.5686       0.6177       23.4302  12.7014\n",
      "      5        0.5733       \u001b[32m0.6417\u001b[0m      101.5879  12.6975\n",
      "      6        0.5691       0.5582       72.8656  12.6342\n",
      "      7        0.5692       0.5986      144.5309  11.9133\n",
      "      8        \u001b[36m0.5629\u001b[0m       0.6122      177.3989  12.7888\n",
      "      9        0.5730       0.6201      196.7213  12.6601\n",
      "     10        0.5690       0.6079      269.0964  12.6631\n",
      "     11        0.5711       0.6243      130.3032  12.6814\n",
      "     12        0.5741       0.6367      163.0629  12.6784\n",
      "     13        0.5702       0.5983       77.0046  12.6370\n",
      "     14        0.5669       0.6330      241.9624  12.6391\n",
      "     15        \u001b[36m0.5593\u001b[0m       0.6354      298.6579  12.6835\n",
      "     16        0.5767       0.6237      267.2564  12.6142\n",
      "     17        0.5719       0.5790      321.0557  12.6344\n",
      "     18        0.5691       0.5935      286.9609  12.6479\n",
      "     19        0.5763       0.6108      215.7579  12.5972\n",
      "     20        0.5616       0.6360      377.5092  12.6306\n",
      "     21        0.5723       0.6294      237.9447  12.6191\n",
      "     22        0.5745       0.6191      347.0070  12.5989\n",
      "     23        0.5849       0.5445      166.2429  12.6458\n",
      "     24        0.5839       0.6337      465.7233  12.2108\n",
      "     25        0.5755       0.5977      287.5474  12.5818\n",
      "     26        0.5770       0.6320      587.2936  12.6383\n",
      "     27        0.5789       0.6286      454.8066  12.6342\n",
      "     28        0.5761       0.6369      220.1632  12.7040\n",
      "     29        0.5732       \u001b[32m0.6479\u001b[0m      340.4145  12.6432\n",
      "     30        0.5708       0.6227      175.8055  12.7091\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6205\u001b[0m       \u001b[32m0.5547\u001b[0m        \u001b[35m0.6831\u001b[0m  13.0842\n",
      "      2        0.6233       \u001b[32m0.5865\u001b[0m        \u001b[35m0.6791\u001b[0m  12.6627\n",
      "      3        0.6361       0.5562        0.6926  12.6920\n",
      "      4        \u001b[36m0.6200\u001b[0m       0.5754        0.6874  12.6681\n",
      "      5        0.6230       0.5563        0.6832  12.6628\n",
      "      6        0.6214       0.5561        0.7017  12.6788\n",
      "      7        \u001b[36m0.6161\u001b[0m       0.5563        0.6821  12.6223\n",
      "      8        \u001b[36m0.6145\u001b[0m       \u001b[32m0.5884\u001b[0m        \u001b[35m0.6728\u001b[0m  12.6576\n",
      "      9        \u001b[36m0.6142\u001b[0m       0.5563        0.6821  12.6533\n",
      "     10        \u001b[36m0.6110\u001b[0m       \u001b[32m0.5894\u001b[0m        \u001b[35m0.6659\u001b[0m  12.6918\n",
      "     11        0.6168       0.5563        0.6660  12.6933\n",
      "     12        0.6149       \u001b[32m0.6080\u001b[0m        \u001b[35m0.6642\u001b[0m  12.6832\n",
      "     13        0.6144       0.5932        0.6660  12.6321\n",
      "     14        0.6166       0.5892        0.6782  12.5945\n",
      "     15        0.6380       0.5563        0.6964  12.6121\n",
      "     16        0.6328       0.5770        0.6904  12.6244\n",
      "     17        0.6192       0.5563        0.6868  12.6070\n",
      "     18        \u001b[36m0.6099\u001b[0m       0.5931        0.6745  12.6474\n",
      "     19        0.6310       0.5563        0.6833  12.6058\n",
      "     20        0.6143       0.5563        0.6716  12.6304\n",
      "     21        0.6154       0.5563        0.6938  12.6228\n",
      "     22        0.6218       0.5563        0.6672  12.6647\n",
      "     23        0.6232       0.5563        0.6881  12.6009\n",
      "     24        0.6336       0.5563        0.6886  12.6719\n",
      "     25        0.6355       0.5563        0.6915  12.6921\n",
      "     26        0.6442       0.5563        0.6926  12.6889\n",
      "     27        0.6306       0.5563        0.6913  12.6736\n",
      "     28        0.6316       0.5563        0.6878  12.6498\n",
      "     29        0.6334       0.5563        0.6878  12.7039\n",
      "     30        0.6340       0.5563        0.6875  12.6174\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5012\u001b[0m       \u001b[32m0.4435\u001b[0m        \u001b[35m0.7427\u001b[0m  13.1789\n",
      "      2        0.5082       \u001b[32m0.4438\u001b[0m        0.7558  12.8546\n",
      "      3        0.5052       \u001b[32m0.4450\u001b[0m        0.8758  12.6878\n",
      "      4        \u001b[36m0.5006\u001b[0m       \u001b[32m0.4454\u001b[0m        0.9916  12.6298\n",
      "      5        0.5015       0.4454        0.9571  12.6633\n",
      "      6        \u001b[36m0.4987\u001b[0m       0.4444        0.7637  12.6200\n",
      "      7        \u001b[36m0.4982\u001b[0m       0.4452        0.8315  12.6414\n",
      "      8        0.5163       0.4438        0.7643  12.6348\n",
      "      9        0.5007       0.4451        1.4811  12.6624\n",
      "     10        0.4983       0.4451        1.4137  12.6340\n",
      "     11        0.4984       0.4441        \u001b[35m0.7241\u001b[0m  12.6372\n",
      "     12        0.4983       0.4453        1.3120  12.6227\n",
      "     13        0.5031       \u001b[32m0.4472\u001b[0m        3.4506  12.6249\n",
      "     14        0.5042       0.4464        4.0238  12.6576\n",
      "     15        0.4983       0.4464        2.8442  12.6585\n",
      "     16        \u001b[36m0.4981\u001b[0m       0.4464        3.2511  12.6683\n",
      "     17        0.4981       0.4466        4.7700  12.6646\n",
      "     18        0.4982       0.4465        4.9117  12.6410\n",
      "     19        0.4982       0.4465        4.9675  12.6365\n",
      "     20        \u001b[36m0.4979\u001b[0m       0.4466        5.4948  12.6476\n",
      "     21        0.4981       0.4466        5.5387  12.6191\n",
      "     22        0.4980       0.4466        5.5608  12.6553\n",
      "     23        0.4980       0.4466        5.5351  12.6222\n",
      "     24        0.4979       0.4466        7.6300  12.6128\n",
      "     25        0.4979       0.4466        7.6162  12.6470\n",
      "     26        \u001b[36m0.4979\u001b[0m       0.4466        7.6253  12.6907\n",
      "     27        \u001b[36m0.4979\u001b[0m       0.4467       11.1421  12.5402\n",
      "     28        0.4983       0.4469       13.9536  12.6087\n",
      "     29        0.4991       0.4468       13.4712  12.6057\n",
      "     30        0.4987       0.4468       16.5312  12.6350\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5754\u001b[0m       \u001b[32m0.6451\u001b[0m        \u001b[35m6.7686\u001b[0m  13.1261\n",
      "      2        \u001b[36m0.5697\u001b[0m       0.6189       52.5979  12.6660\n",
      "      3        0.5725       0.5777       16.7497  12.6478\n",
      "      4        \u001b[36m0.5646\u001b[0m       0.6155       27.9378  12.7286\n",
      "      5        0.5777       0.6290       17.4808  12.6946\n",
      "      6        0.5744       0.5940       93.2963  12.6772\n",
      "      7        0.5761       0.6165       39.7803  12.6261\n",
      "      8        0.5750       0.6065       58.2041  12.6152\n",
      "      9        0.5814       0.5930      104.1069  12.6277\n",
      "     10        0.5896       0.5859      175.0091  12.6925\n",
      "     11        0.5999       0.6101      144.9819  12.6575\n",
      "     12        0.5707       0.6314       53.0335  12.5845\n",
      "     13        0.5751       0.6092      138.1680  12.6425\n",
      "     14        0.5743       0.6242       39.8289  12.6296\n",
      "     15        0.5752       0.5583      101.7337  12.6170\n",
      "     16        0.5771       0.6158      119.8579  12.6577\n",
      "     17        0.5757       0.6169      138.3308  12.6401\n",
      "     18        0.5760       0.6087      190.2091  12.6226\n",
      "     19        0.5811       0.6060      167.0775  12.6773\n",
      "     20        0.5774       0.6000      110.5951  12.6324\n",
      "     21        0.5739       0.6046      191.0153  12.6678\n",
      "     22        0.5815       0.6052       78.7219  12.6254\n",
      "     23        0.5817       0.5805      166.8315  12.6524\n",
      "     24        0.5684       0.6191      128.0461  12.6530\n",
      "     25        0.5832       0.5473      212.4479  12.6771\n",
      "     26        0.5782       0.5783      274.9781  12.6553\n",
      "     27        0.5771       0.6158      139.6607  12.6779\n",
      "     28        0.5665       0.6413      114.4483  12.7094\n",
      "     29        0.5777       0.6432      162.2539  12.6433\n",
      "     30        0.5775       0.6296      331.7267  12.7019\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6219\u001b[0m       \u001b[32m0.5768\u001b[0m        \u001b[35m0.6989\u001b[0m  13.1459\n",
      "      2        \u001b[36m0.6164\u001b[0m       0.5556        0.8561  12.7435\n",
      "      3        0.6228       0.5560        0.7316  12.7192\n",
      "      4        0.6374       0.5563        \u001b[35m0.6870\u001b[0m  12.7130\n",
      "      5        0.6326       0.5563        0.6951  12.6667\n",
      "      6        0.6323       0.5562        0.6878  12.6989\n",
      "      7        0.6274       0.5563        0.6900  12.6756\n",
      "      8        0.6346       0.5563        0.6890  12.6611\n",
      "      9        0.6402       0.5563        \u001b[35m0.6869\u001b[0m  12.6465\n",
      "     10        0.6374       0.5563        0.6870  12.6339\n",
      "     11        0.6358       0.5563        0.6893  12.1494\n",
      "     12        0.6397       0.5563        0.6890  10.7202\n",
      "     13        0.6345       0.5563        0.6920  11.2094\n",
      "     14        0.6367       0.5563        0.6917  10.8189\n",
      "     15        0.6380       0.5563        0.6880  10.7846\n",
      "     16        0.6351       0.5563        0.6889  10.7780\n",
      "     17        0.6423       0.5563        0.6944  10.7754\n",
      "     18        0.6363       0.5564        0.6872  10.8035\n",
      "     19        0.6376       0.5563        0.6870  10.7828\n",
      "     20        0.6323       0.5563        0.6907  10.6024\n",
      "     21        0.6361       0.5563        0.6870  10.7443\n",
      "     22        0.6359       0.5563        0.6901  10.7483\n",
      "     23        0.6352       0.5563        0.6920  11.1281\n",
      "     24        0.6373       0.5563        0.6911  10.7061\n",
      "     25        0.6341       0.5563        0.6896  10.9640\n",
      "     26        0.6525       0.5563        0.6869  10.7579\n",
      "     27        0.6343       0.5563        0.6921  10.8871\n",
      "     28        0.6415       0.5563        0.6889  10.8663\n",
      "     29        0.6363       0.5563        0.6924  10.7889\n",
      "     30        0.6359       0.5563        0.6884  10.8465\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5301\u001b[0m       \u001b[32m0.4438\u001b[0m        \u001b[35m0.7484\u001b[0m  10.6632\n",
      "      2        \u001b[36m0.5167\u001b[0m       0.4438        \u001b[35m0.7081\u001b[0m  10.7459\n",
      "      3        0.5177       0.4437        0.7643  10.8160\n",
      "      4        \u001b[36m0.5077\u001b[0m       \u001b[32m0.4439\u001b[0m        0.7680  10.8933\n",
      "      5        0.5091       0.4437        0.7661  10.6889\n",
      "      6        \u001b[36m0.5056\u001b[0m       0.4438        0.7784  10.9015\n",
      "      7        0.5094       0.4437        0.7843  10.6244\n",
      "      8        0.5061       0.4437        0.7443  10.8441\n",
      "      9        0.5066       0.4437        0.7178  10.8179\n",
      "     10        0.5196       0.4437        0.7297  10.6542\n",
      "     11        0.5103       0.4438        0.8010  10.7723\n",
      "     12        0.5085       0.4437        0.7379  10.7196\n",
      "     13        0.5080       0.4438        0.7688  10.8334\n",
      "     14        0.5077       0.4436        0.7170  10.7488\n",
      "     15        0.5073       0.4437        0.7202  10.8486\n",
      "     16        0.5058       0.4437        0.7513  10.7349\n",
      "     17        0.5057       0.4438        0.7327  10.7869\n",
      "     18        0.5152       0.4439        0.7382  10.7925\n",
      "     19        0.5070       0.4438        0.7576  10.7079\n",
      "     20        0.5075       \u001b[32m0.4440\u001b[0m        0.7282  10.8044\n",
      "     21        0.5073       0.4437        0.7259  10.8602\n",
      "     22        0.5067       0.4438        0.7550  10.7022\n",
      "     23        0.5068       0.4440        0.7825  10.7901\n",
      "     24        0.5120       0.4437        0.7904  10.6994\n",
      "     25        0.5061       0.4437        0.7346  10.8638\n",
      "     26        0.5057       0.4437        0.7281  10.7249\n",
      "     27        0.5092       0.4436        0.7795  10.7893\n",
      "     28        0.5059       0.4437        0.7587  10.8603\n",
      "     29        0.5060       0.4437        0.7629  10.6962\n",
      "     30        0.5140       0.4437        0.7088  10.8098\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5618\u001b[0m       \u001b[32m0.6233\u001b[0m       \u001b[35m27.3331\u001b[0m  10.8036\n",
      "      2        0.5639       0.5897       32.5366  10.8263\n",
      "      3        0.5678       0.5911       91.8367  10.7523\n",
      "      4        0.5747       0.6221       90.2159  10.7937\n",
      "      5        0.5702       0.6103       76.7575  10.7821\n",
      "      6        0.5707       0.5988      101.7746  10.8262\n",
      "      7        0.5725       0.5922      166.8668  10.7949\n",
      "      8        \u001b[36m0.5617\u001b[0m       \u001b[32m0.6298\u001b[0m      217.0408  10.8290\n",
      "      9        0.5654       \u001b[32m0.6362\u001b[0m      100.9027  10.6876\n",
      "     10        0.5686       0.5871      136.1687  10.7865\n",
      "     11        0.5793       0.5969      285.9586  10.7086\n",
      "     12        0.5749       0.6039      218.6092  10.9382\n",
      "     13        0.5711       0.6300      170.1393  10.6039\n",
      "     14        \u001b[36m0.5545\u001b[0m       0.6307      330.3814  10.8181\n",
      "     15        0.5790       0.6303      433.6163  10.8425\n",
      "     16        0.5643       0.5790      452.5184  10.6338\n",
      "     17        0.5638       0.6262      224.5302  10.7670\n",
      "     18        0.5651       0.6291      333.6807  10.7690\n",
      "     19        0.5878       0.6314      767.8956  10.7829\n",
      "     20        0.5858       0.5997      294.9905  10.8310\n",
      "     21        0.5700       0.6101      560.4909  10.7006\n",
      "     22        0.5694       0.6009      486.7860  10.8187\n",
      "     23        0.5675       \u001b[32m0.6382\u001b[0m      595.7997  10.6008\n",
      "     24        0.5773       0.6196      296.0011  10.8160\n",
      "     25        0.5590       0.5953      594.0270  10.8453\n",
      "     26        0.5626       \u001b[32m0.6386\u001b[0m      325.8881  10.7821\n",
      "     27        0.5704       0.6313      617.6389  10.8637\n",
      "     28        0.5616       0.5938      903.1759  10.9054\n",
      "     29        0.5835       0.5812      642.1460  10.7854\n",
      "     30        0.5905       0.5820      914.2611  10.7811\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6190\u001b[0m       \u001b[32m0.5842\u001b[0m        \u001b[35m0.6813\u001b[0m  10.8094\n",
      "      2        0.6199       0.5561        0.6817  10.7654\n",
      "      3        0.6207       0.5820        \u001b[35m0.6750\u001b[0m  10.9457\n",
      "      4        0.6230       0.5572        0.7108  10.7254\n",
      "      5        0.6257       0.5563        0.6965  10.8995\n",
      "      6        0.6271       0.5563        0.6944  10.7716\n",
      "      7        0.6378       0.5563        0.6925  10.8732\n",
      "      8        0.6348       0.5563        0.6871  10.7503\n",
      "      9        0.6392       0.5563        0.6904  10.6368\n",
      "     10        0.6331       0.5563        0.6949  10.8354\n",
      "     11        0.6395       0.5563        0.6901  10.7426\n",
      "     12        0.6299       0.5563        0.6891  10.8673\n",
      "     13        0.6348       0.5563        0.6953  10.7776\n",
      "     14        0.6322       0.5563        0.6886  10.8046\n",
      "     15        0.6331       0.5563        0.7083  10.7852\n",
      "     16        0.6291       0.5563        0.6966  10.7390\n",
      "     17        0.6404       0.5563        0.6876  10.8427\n",
      "     18        0.6281       0.5563        0.6870  10.7209\n",
      "     19        0.6398       0.5563        0.6969  10.7962\n",
      "     20        0.6376       0.5563        0.6990  10.8586\n",
      "     21        0.6335       0.5563        0.6931  10.7162\n",
      "     22        0.6341       0.5563        0.6900  10.8446\n",
      "     23        0.6333       0.5563        0.6907  10.7477\n",
      "     24        0.6545       0.5563        0.6868  10.8584\n",
      "     25        0.6372       0.5563        0.6868  10.6231\n",
      "     26        0.6270       0.5563        0.6893  10.7725\n",
      "     27        0.6287       0.5563        0.6916  10.7409\n",
      "     28        0.6304       0.5563        0.6906  10.9574\n",
      "     29        0.6432       0.5563        0.7016  10.8257\n",
      "     30        0.6313       0.5563        0.6868  10.6715\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5080\u001b[0m       \u001b[32m0.4436\u001b[0m        \u001b[35m0.7359\u001b[0m  10.6629\n",
      "      2        \u001b[36m0.5028\u001b[0m       \u001b[32m0.4437\u001b[0m        \u001b[35m0.7262\u001b[0m  10.9250\n",
      "      3        0.5079       0.4437        0.7531  10.8389\n",
      "      4        \u001b[36m0.4991\u001b[0m       0.4437        0.7323  10.8332\n",
      "      5        0.5084       0.4437        0.7283  10.7726\n",
      "      6        0.5023       \u001b[32m0.4438\u001b[0m        0.7374  10.6210\n",
      "      7        0.5013       0.4437        0.7381  10.7733\n",
      "      8        0.5008       0.4437        0.7424  10.6523\n",
      "      9        0.5020       0.4437        0.7341  10.7543\n",
      "     10        0.5004       0.4437        0.7603  10.8141\n",
      "     11        0.5028       0.4437        \u001b[35m0.7146\u001b[0m  10.7474\n",
      "     12        0.5112       0.4437        0.7199  10.8080\n",
      "     13        0.5121       0.4437        0.7324  10.7822\n",
      "     14        \u001b[36m0.4987\u001b[0m       0.4437        0.7582  10.6811\n",
      "     15        0.5026       0.4437        0.7280  10.8032\n",
      "     16        0.4998       0.4436        0.7576  10.7002\n",
      "     17        0.5044       0.4437        0.8041  10.7264\n",
      "     18        0.5007       0.4437        0.7542  10.7341\n",
      "     19        0.5077       0.4437        0.7178  10.8537\n",
      "     20        0.5003       0.4436        0.7326  10.5546\n",
      "     21        0.4989       0.4437        \u001b[35m0.7143\u001b[0m  10.7573\n",
      "     22        0.5021       0.4437        0.7784  10.8331\n",
      "     23        0.5004       0.4437        0.7319  10.6889\n",
      "     24        0.5049       0.4437        0.7793  10.8640\n",
      "     25        0.5025       0.4437        0.7566  10.6780\n",
      "     26        \u001b[36m0.4985\u001b[0m       0.4438        0.7788  10.8054\n",
      "     27        \u001b[36m0.4984\u001b[0m       0.4437        0.7821  10.6798\n",
      "     28        0.4987       \u001b[32m0.4439\u001b[0m        1.0694  10.8193\n",
      "     29        0.5060       0.4438        0.8373  10.6088\n",
      "     30        0.5023       0.4438        0.7916  10.7405\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5641\u001b[0m       \u001b[32m0.6260\u001b[0m       \u001b[35m20.6711\u001b[0m  10.7763\n",
      "      2        0.5744       0.6167       60.4530  10.8576\n",
      "      3        0.5702       0.6135       48.0101  10.7233\n",
      "      4        0.5692       0.5962       56.4420  10.8210\n",
      "      5        0.5653       0.5685      169.3929  10.7252\n",
      "      6        0.5780       \u001b[32m0.6313\u001b[0m      202.0697  10.8925\n",
      "      7        \u001b[36m0.5636\u001b[0m       0.6281      131.6191  10.7155\n",
      "      8        0.5638       \u001b[32m0.6419\u001b[0m      126.3424  10.6742\n",
      "      9        0.5662       \u001b[32m0.6615\u001b[0m      166.0082  10.7690\n",
      "     10        \u001b[36m0.5610\u001b[0m       0.6535      328.7296  10.7036\n",
      "     11        0.5652       0.6449      343.2009  10.7447\n",
      "     12        \u001b[36m0.5604\u001b[0m       0.5991      481.3857  10.7271\n",
      "     13        0.5831       0.5811      423.7540  10.6694\n",
      "     14        0.5777       0.6078      257.2210  10.7684\n",
      "     15        0.5687       0.6413      528.4052  10.7365\n",
      "     16        0.5649       0.6421      655.6522  10.8354\n",
      "     17        0.5655       0.6397      715.5238  10.8196\n",
      "     18        0.5643       0.6326      570.8037  10.7243\n",
      "     19        0.5950       0.5963      615.6899  10.7981\n",
      "     20        0.5675       0.6035      563.5969  10.7083\n",
      "     21        0.5706       0.5826      581.5195  10.8326\n",
      "     22        0.5727       0.6105      820.7240  10.8178\n",
      "     23        0.5799       0.5733      478.5476  10.7104\n",
      "     24        0.5793       0.6265      603.4692  10.8276\n",
      "     25        0.5794       0.6169      781.9283  10.7399\n",
      "     26        0.5609       0.5823      580.0738  10.8786\n",
      "     27        0.5605       0.5587      878.0345  10.7358\n",
      "     28        0.5658       0.5901      491.5342  10.8002\n",
      "     29        0.5761       0.5726      726.4357  10.8557\n",
      "     30        0.5705       0.5813      623.4696  10.6963\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6286\u001b[0m       \u001b[32m0.5563\u001b[0m        \u001b[35m0.6885\u001b[0m  10.6553\n",
      "      2        \u001b[36m0.6233\u001b[0m       \u001b[32m0.5644\u001b[0m        0.6886  10.8296\n",
      "      3        0.6487       0.5562        0.7001  10.7003\n",
      "      4        0.6423       0.5563        \u001b[35m0.6871\u001b[0m  10.8906\n",
      "      5        0.6456       0.5563        0.6930  10.7290\n",
      "      6        0.6365       0.5563        0.7039  10.8140\n",
      "      7        0.6460       0.5563        0.6957  10.7600\n",
      "      8        0.6322       0.5563        0.6958  10.7602\n",
      "      9        0.6400       0.5563        0.6920  10.7448\n",
      "     10        0.6389       0.5563        0.6875  10.7439\n",
      "     11        0.6321       0.5563        0.6993  10.8072\n",
      "     12        0.6494       0.5563        \u001b[35m0.6868\u001b[0m  10.6371\n",
      "     13        0.6308       0.5563        0.6881  10.8176\n",
      "     14        0.6352       0.5563        0.6894  10.6950\n",
      "     15        0.6298       0.5563        0.6906  10.8760\n",
      "     16        0.6459       0.5563        0.6902  10.5676\n",
      "     17        0.6309       0.5563        0.6899  10.7366\n",
      "     18        0.6275       0.5563        0.6991  10.8307\n",
      "     19        0.6593       0.5563        0.6919  10.6857\n",
      "     20        0.6315       0.5563        0.6887  10.7822\n",
      "     21        0.6323       0.5563        0.6925  10.7081\n",
      "     22        0.6413       0.5563        0.6878  10.7451\n",
      "     23        0.6347       0.5563        0.6975  10.7183\n",
      "     24        0.6270       0.5563        0.6965  10.7420\n",
      "     25        0.6419       0.5563        \u001b[35m0.6868\u001b[0m  10.8477\n",
      "     26        0.6278       0.5563        0.6898  10.7045\n",
      "     27        0.6313       0.5563        0.6874  10.7967\n",
      "     28        0.6260       0.5563        0.6871  10.7481\n",
      "     29        0.6444       0.5563        0.6913  10.9448\n",
      "     30        0.6496       0.5563        0.6924  10.6277\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5046\u001b[0m       \u001b[32m0.4437\u001b[0m        \u001b[35m0.7415\u001b[0m  10.7059\n",
      "      2        \u001b[36m0.5034\u001b[0m       \u001b[32m0.4437\u001b[0m        0.7512  10.6946\n",
      "      3        0.5061       \u001b[32m0.4439\u001b[0m        0.7438  10.8950\n",
      "      4        \u001b[36m0.5025\u001b[0m       0.4437        \u001b[35m0.7368\u001b[0m  10.7857\n",
      "      5        0.5056       \u001b[32m0.4457\u001b[0m        1.4971  10.9335\n",
      "      6        \u001b[36m0.4986\u001b[0m       \u001b[32m0.4460\u001b[0m        2.0421  10.8024\n",
      "      7        \u001b[36m0.4985\u001b[0m       \u001b[32m0.4461\u001b[0m        2.3829  10.8949\n",
      "      8        0.5011       \u001b[32m0.4461\u001b[0m        1.4771  10.8698\n",
      "      9        0.5017       \u001b[32m0.4465\u001b[0m        0.9244  10.6245\n",
      "     10        0.5024       0.4454        0.9221  10.7844\n",
      "     11        0.5002       0.4455        0.9083  10.8652\n",
      "     12        \u001b[36m0.4982\u001b[0m       0.4455        1.2437  10.7304\n",
      "     13        0.5013       0.4438        1.0799  10.7958\n",
      "     14        0.5028       0.4450        2.9728  10.7612\n",
      "     15        \u001b[36m0.4981\u001b[0m       0.4450        2.9677  10.6943\n",
      "     16        0.5117       0.4459        4.5067  10.7312\n",
      "     17        0.4982       0.4459        4.5052  10.6646\n",
      "     18        \u001b[36m0.4981\u001b[0m       0.4459        4.5301  10.8297\n",
      "     19        \u001b[36m0.4980\u001b[0m       0.4459        4.8252  10.7814\n",
      "     20        \u001b[36m0.4980\u001b[0m       0.4462        5.2802  10.7327\n",
      "     21        0.5093       0.4454        1.6116  10.7269\n",
      "     22        0.5020       0.4455        3.7936  10.6512\n",
      "     23        0.4986       0.4460        6.6094  10.8629\n",
      "     24        0.4986       0.4450        2.4927  10.6138\n",
      "     25        0.4989       0.4444        0.8737  10.8694\n",
      "     26        \u001b[36m0.4980\u001b[0m       0.4445        0.9111  10.8507\n",
      "     27        0.5184       0.4439        0.7720  10.6319\n",
      "     28        0.5016       0.4443        1.0246  10.7723\n",
      "     29        0.4990       0.4444        1.4471  10.7047\n",
      "     30        0.4984       0.4456        1.8442  10.7627\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5751\u001b[0m       \u001b[32m0.5839\u001b[0m       \u001b[35m28.8548\u001b[0m  10.6367\n",
      "      2        \u001b[36m0.5735\u001b[0m       0.5348       63.8189  10.8706\n",
      "      3        0.5781       \u001b[32m0.6061\u001b[0m      113.7029  10.7430\n",
      "      4        \u001b[36m0.5638\u001b[0m       \u001b[32m0.6259\u001b[0m      158.0959  10.8456\n",
      "      5        0.5733       \u001b[32m0.6406\u001b[0m      272.7697  10.7526\n",
      "      6        0.5725       0.6393      138.2017  10.9347\n",
      "      7        0.5680       \u001b[32m0.6695\u001b[0m      281.9797  10.8170\n",
      "      8        0.5695       0.6549      355.3863  10.8062\n",
      "      9        \u001b[36m0.5615\u001b[0m       0.6372      284.1350  10.8911\n",
      "     10        0.5750       0.6479      197.3690  11.1376\n",
      "     11        0.5625       0.6451      428.7817  10.9676\n",
      "     12        0.5662       0.6383      451.2598  10.7172\n",
      "     13        0.5658       0.6385      385.6157  10.7893\n",
      "     14        0.5622       0.6197      791.8918  10.6864\n",
      "     15        0.5635       0.6596      904.6716  10.7279\n",
      "     16        0.5898       0.6604      978.0382  10.8164\n",
      "     17        0.5713       0.6581      617.2932  10.6783\n",
      "     18        0.5742       0.6121      780.4495  10.8371\n",
      "     19        0.5735       0.6499      628.7711  11.0192\n",
      "     20        0.5689       0.6273      838.5701  10.8314\n",
      "     21        0.5711       0.6306      466.4060  10.9291\n",
      "     22        0.5722       0.6324      780.7577  10.8014\n",
      "     23        0.5627       0.5991      685.7816  10.6833\n",
      "     24        0.5807       0.6224      735.5322  10.7753\n",
      "     25        0.5944       0.5819     1332.9589  10.6987\n",
      "     26        0.6080       0.5862      469.1844  10.8445\n",
      "     27        0.5881       0.5939      464.4314  10.7753\n",
      "     28        0.5645       0.5841      829.6370  10.8273\n",
      "     29        0.5659       0.6115     1105.8217  10.7949\n",
      "     30        0.5718       0.5709      614.5683  10.7731\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6335\u001b[0m       \u001b[32m0.5562\u001b[0m        \u001b[35m0.6783\u001b[0m  10.6528\n",
      "      2        \u001b[36m0.6284\u001b[0m       \u001b[32m0.5563\u001b[0m        0.6890  10.8517\n",
      "      3        0.6305       \u001b[32m0.6094\u001b[0m        \u001b[35m0.6669\u001b[0m  10.7777\n",
      "      4        \u001b[36m0.6193\u001b[0m       0.5569        \u001b[35m0.6654\u001b[0m  10.8767\n",
      "      5        0.6269       0.5563        0.6782  10.7860\n",
      "      6        \u001b[36m0.6156\u001b[0m       0.6008        \u001b[35m0.6649\u001b[0m  10.8002\n",
      "      7        \u001b[36m0.6139\u001b[0m       0.5931        0.6665  10.8623\n",
      "      8        0.6161       0.5563        0.6767  10.6984\n",
      "      9        0.6265       0.5976        0.6694  10.8056\n",
      "     10        0.6294       0.5563        0.6881  10.9171\n",
      "     11        0.6254       0.6057        0.6666  10.8602\n",
      "     12        0.6177       0.6076        \u001b[35m0.6634\u001b[0m  10.8177\n",
      "     13        0.6231       0.5564        0.6876  10.7145\n",
      "     14        0.6291       0.5564        0.6930  10.8126\n",
      "     15        0.6419       0.5563        0.6885  10.8710\n",
      "     16        0.6318       0.5563        0.6860  10.7449\n",
      "     17        0.6311       0.5563        0.6917  10.7517\n",
      "     18        0.6285       0.5563        0.6869  10.8634\n",
      "     19        0.6301       0.5563        0.6871  10.8435\n",
      "     20        0.6290       0.5563        0.6881  10.7977\n",
      "     21        0.6295       0.5563        0.6881  10.6938\n",
      "     22        0.6277       0.5563        0.6943  10.8168\n",
      "     23        0.6354       0.5563        0.6888  10.7818\n",
      "     24        0.6256       0.5563        0.6930  10.7857\n",
      "     25        0.6311       0.5563        0.6942  10.8416\n",
      "     26        0.6281       0.5563        0.6940  10.7806\n",
      "     27        0.6441       0.5563        0.6869  10.6674\n",
      "     28        0.6294       0.5563        0.6908  10.8430\n",
      "     29        0.6256       0.5563        0.6883  10.7417\n",
      "     30        0.6300       0.5563        0.7038  10.7849\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4998\u001b[0m       \u001b[32m0.4439\u001b[0m        \u001b[35m0.7221\u001b[0m  10.6555\n",
      "      2        \u001b[36m0.4929\u001b[0m       \u001b[32m0.4451\u001b[0m        0.7983  10.8503\n",
      "      3        0.4986       \u001b[32m0.4456\u001b[0m        0.8646  10.6655\n",
      "      4        0.4969       \u001b[32m0.4457\u001b[0m        1.1324  10.8540\n",
      "      5        0.4993       \u001b[32m0.5648\u001b[0m        1.5658  10.7673\n",
      "      6        0.5003       0.4467        1.4734  10.8399\n",
      "      7        0.4984       0.4469        1.7286  10.7902\n",
      "      8        0.5052       0.4468        1.1222  10.7772\n",
      "      9        0.4987       0.4468        1.0616  10.7878\n",
      "     10        0.5015       0.4449        1.0729  10.7998\n",
      "     11        0.4994       0.4454        1.0787  10.7671\n",
      "     12        0.4981       0.4454        1.1081  10.7950\n",
      "     13        0.4987       0.4467        2.5274  10.8168\n",
      "     14        0.4980       0.4467        2.8096  10.7756\n",
      "     15        0.5006       0.4466        3.6620  10.8328\n",
      "     16        0.4981       0.4466        3.8144  10.7878\n",
      "     17        0.5129       0.4468        2.9334  10.8609\n",
      "     18        0.5010       0.4467        2.1235  10.7062\n",
      "     19        0.4980       0.4467        2.0937  10.7622\n",
      "     20        0.5039       0.4470        3.1859  10.7753\n",
      "     21        0.5015       0.4464        4.2257  10.7502\n",
      "     22        0.4987       0.4466        3.4104  10.8153\n",
      "     23        0.4982       0.4464        3.0230  10.6965\n",
      "     24        0.4981       0.4466        2.8415  10.8022\n",
      "     25        0.4980       0.4466        3.1558  10.7600\n",
      "     26        0.4981       0.4466        3.1510  10.7723\n",
      "     27        0.4961       0.4465        3.6633  10.8010\n",
      "     28        0.5254       0.4477        3.7612  10.6747\n",
      "     29        0.4985       0.4478        4.2128  10.7696\n",
      "     30        0.5028       0.4460        2.0528  10.7136\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5611\u001b[0m       \u001b[32m0.6153\u001b[0m       \u001b[35m22.7673\u001b[0m  10.8126\n",
      "      2        0.5677       0.6136       33.9233  10.8209\n",
      "      3        0.5820       \u001b[32m0.6185\u001b[0m       25.3263  10.8960\n",
      "      4        0.5777       \u001b[32m0.6388\u001b[0m      104.9993  10.7617\n",
      "      5        0.5992       0.5990       97.6784  10.7854\n",
      "      6        0.5814       0.6230      114.7752  10.7958\n",
      "      7        0.5781       0.5810      116.6243  10.8157\n",
      "      8        0.5708       \u001b[32m0.6501\u001b[0m      274.8611  10.7635\n",
      "      9        0.5827       0.6337       37.1770  10.8525\n",
      "     10        0.5740       0.5917      120.5804  10.7457\n",
      "     11        0.5792       0.6235      232.0895  10.7669\n",
      "     12        0.5666       0.5077      118.8665  10.7937\n",
      "     13        0.5672       0.5820      365.3135  10.8225\n",
      "     14        0.5869       0.5429      355.7430  10.7818\n",
      "     15        0.5714       0.5366      231.4753  10.7222\n",
      "     16        0.5745       0.5419      309.1957  10.8045\n",
      "     17        0.5715       0.5679      498.1263  10.7938\n",
      "     18        0.5770       0.6197      406.0275  10.8159\n",
      "     19        0.5769       0.6368      324.8280  10.8312\n",
      "     20        0.5690       0.6343      388.7522  10.7537\n",
      "     21        0.5806       \u001b[32m0.6504\u001b[0m      480.4557  10.8322\n",
      "     22        0.5954       0.6430      300.2918  10.8546\n",
      "     23        0.5640       0.6429      504.4942  10.8342\n",
      "     24        0.5694       0.6337      815.3631  10.7473\n",
      "     25        0.5710       0.6070      395.3057  10.7774\n",
      "     26        0.5766       0.6478      728.5256  10.8186\n",
      "     27        0.5674       0.6372      685.9441  10.8057\n",
      "     28        0.5754       0.6483      572.1251  10.7629\n",
      "     29        0.5691       0.5942      603.5692  10.8535\n",
      "     30        0.5928       0.5870      871.9746  10.9118\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6353\u001b[0m       \u001b[32m0.5563\u001b[0m        \u001b[35m0.6896\u001b[0m  10.9043\n",
      "      2        0.6456       0.5563        \u001b[35m0.6896\u001b[0m  10.9427\n",
      "      3        0.6446       0.5563        \u001b[35m0.6870\u001b[0m  10.7238\n",
      "      4        0.6378       0.5563        0.6904  10.9189\n",
      "      5        0.6459       0.5563        0.6963  10.7626\n",
      "      6        0.6402       0.5563        0.6932  10.8308\n",
      "      7        \u001b[36m0.6287\u001b[0m       0.5563        0.6891  10.8183\n",
      "      8        0.6449       0.5563        0.6877  10.7340\n",
      "      9        0.6343       0.5563        0.6883  10.8228\n",
      "     10        0.6333       0.5563        0.6919  10.9669\n",
      "     11        0.6356       0.5563        0.6920  10.7159\n",
      "     12        0.6355       0.5563        0.6924  10.8148\n",
      "     13        0.6354       0.5563        0.7036  10.7395\n",
      "     14        0.6346       0.5563        0.6988  10.7599\n",
      "     15        0.6319       0.5563        \u001b[35m0.6869\u001b[0m  10.7766\n",
      "     16        \u001b[36m0.6267\u001b[0m       0.5563        0.6939  10.7780\n",
      "     17        0.6313       0.5563        0.7007  10.7364\n",
      "     18        0.6356       0.5563        0.6892  10.7299\n",
      "     19        0.6346       0.5563        0.6869  10.8534\n",
      "     20        0.6396       0.5563        0.6912  10.8294\n",
      "     21        \u001b[36m0.6250\u001b[0m       0.5563        0.6896  10.7864\n",
      "     22        0.6325       0.5563        0.6928  10.7082\n",
      "     23        0.6301       0.5563        0.6914  10.7670\n",
      "     24        0.6400       0.5563        0.6884  10.8588\n",
      "     25        0.6306       0.5563        0.6893  10.8414\n",
      "     26        0.6267       0.5563        0.6910  10.7844\n",
      "     27        0.6288       0.5563        0.6976  10.8168\n",
      "     28        \u001b[36m0.6238\u001b[0m       0.5563        0.6894  10.8545\n",
      "     29        0.6301       0.5563        0.6891  10.7203\n",
      "     30        0.6477       0.5563        0.7004  10.8085\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5086\u001b[0m       \u001b[32m0.4438\u001b[0m        \u001b[35m0.7178\u001b[0m  10.9273\n",
      "      2        0.5117       0.4437        0.7355  10.8624\n",
      "      3        \u001b[36m0.5009\u001b[0m       0.4437        0.7530  10.7593\n",
      "      4        0.5023       0.4437        0.7219  10.8743\n",
      "      5        0.5022       0.4437        0.7598  10.8110\n",
      "      6        0.5066       0.4437        0.7380  10.7457\n",
      "      7        0.5081       0.4437        0.7195  10.8897\n",
      "      8        0.5027       0.4437        0.7260  10.8194\n",
      "      9        \u001b[36m0.5003\u001b[0m       0.4437        0.7273  10.7896\n",
      "     10        \u001b[36m0.5003\u001b[0m       0.4437        0.7526  10.7709\n",
      "     11        0.5022       0.4437        0.7342  10.6578\n",
      "     12        \u001b[36m0.5000\u001b[0m       0.4438        0.7354  10.8183\n",
      "     13        \u001b[36m0.4998\u001b[0m       0.4438        \u001b[35m0.7057\u001b[0m  10.7594\n",
      "     14        0.5059       0.4437        0.7179  10.7939\n",
      "     15        0.5015       0.4437        0.7253  11.0230\n",
      "     16        0.5061       0.4437        0.7735  10.8036\n",
      "     17        0.5038       0.4437        0.7322  10.7954\n",
      "     18        0.5028       0.4437        0.7394  10.8071\n",
      "     19        0.5002       0.4437        0.7370  10.7858\n",
      "     20        0.5012       0.4437        0.7345  10.7435\n",
      "     21        0.5005       \u001b[32m0.4441\u001b[0m        0.7330  10.7643\n",
      "     22        0.5060       0.4437        0.7195  10.8396\n",
      "     23        0.5000       0.4437        0.7428  10.7201\n",
      "     24        0.4999       0.4437        0.7292  10.8308\n",
      "     25        0.4999       0.4437        0.7358  10.8504\n",
      "     26        0.4999       0.4437        0.7388  10.8211\n",
      "     27        0.5000       0.4437        0.7660  10.8817\n",
      "     28        0.5004       0.4440        0.7233  10.7341\n",
      "     29        0.5039       0.4437        0.7459  10.7494\n",
      "     30        0.5001       0.4440        0.7816  10.8114\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5713\u001b[0m       \u001b[32m0.6451\u001b[0m       \u001b[35m11.1577\u001b[0m  10.9655\n",
      "      2        \u001b[36m0.5695\u001b[0m       0.5959       28.7454  10.7438\n",
      "      3        0.5702       0.5884       80.8154  10.8547\n",
      "      4        0.5841       0.5434      134.3562  10.8022\n",
      "      5        0.5738       0.6035      203.3763  10.8671\n",
      "      6        0.5810       0.5463      122.9220  10.7507\n",
      "      7        0.5857       0.5725      224.3359  10.8840\n",
      "      8        0.5744       0.5673      229.7582  10.8786\n",
      "      9        \u001b[36m0.5666\u001b[0m       0.5974      339.0494  10.7520\n",
      "     10        0.5686       0.6254      444.4405  10.8015\n",
      "     11        \u001b[36m0.5586\u001b[0m       0.6345      368.2065  10.8722\n",
      "     12        \u001b[36m0.5581\u001b[0m       0.6237      503.2575  10.7681\n",
      "     13        0.5862       0.5909      140.2007  10.7393\n",
      "     14        0.5753       0.6177      550.9941  10.7767\n",
      "     15        0.5658       0.6417      445.7199  10.8302\n",
      "     16        0.5669       0.6381      548.7900  10.7211\n",
      "     17        0.5631       \u001b[32m0.6451\u001b[0m      520.1249  10.7095\n",
      "     18        0.5740       0.6441      439.7809  10.7565\n",
      "     19        0.5664       0.6371      603.9857  10.7776\n",
      "     20        0.5714       0.6284      896.1878  10.7446\n",
      "     21        0.5958       0.5990      970.6098  10.7754\n",
      "     22        0.5846       0.6303      809.2354  10.8283\n",
      "     23        0.5847       0.6052      850.3564  10.7146\n",
      "     24        0.6007       0.5932      582.8003  10.6804\n",
      "     25        0.6031       0.6304      416.0603  10.8398\n",
      "     26        0.5764       0.6069      891.7289  10.8138\n",
      "     27        0.5837       0.5985      505.6121  10.8230\n",
      "     28        0.5751       0.5800      570.4592  10.8318\n",
      "     29        0.5959       0.5573      794.5313  10.8791\n",
      "     30        0.5705       0.5541      666.6103  10.8699\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6431\u001b[0m       \u001b[32m0.5563\u001b[0m        \u001b[35m0.6897\u001b[0m  10.8397\n",
      "      2        0.6553       0.5563        0.6915  10.6920\n",
      "      3        0.6613       0.5563        \u001b[35m0.6883\u001b[0m  10.8512\n",
      "      4        0.6467       0.5563        0.6980  10.7466\n",
      "      5        \u001b[36m0.6363\u001b[0m       0.5563        0.6909  10.7814\n",
      "      6        0.6425       0.5563        0.6941  10.8919\n",
      "      7        0.6418       0.5563        0.6907  10.8529\n",
      "      8        0.6432       0.5563        0.6960  10.7856\n",
      "      9        0.6409       0.5563        0.6895  10.8150\n",
      "     10        0.6418       0.5563        0.6898  10.8703\n",
      "     11        0.6394       0.5563        0.6914  10.8524\n",
      "     12        \u001b[36m0.6340\u001b[0m       0.5563        0.6909  10.8283\n",
      "     13        0.6388       0.5563        0.6931  10.8222\n",
      "     14        \u001b[36m0.6291\u001b[0m       0.5563        0.6885  10.8272\n",
      "     15        0.6483       0.5563        0.6936  10.7627\n",
      "     16        0.6510       0.5563        \u001b[35m0.6882\u001b[0m  10.8157\n",
      "     17        0.6449       0.5563        \u001b[35m0.6878\u001b[0m  10.8339\n",
      "     18        0.6311       0.5563        0.6904  10.7683\n",
      "     19        0.6304       0.5563        0.6922  10.8208\n",
      "     20        0.6317       0.5563        0.6950  10.6337\n",
      "     21        0.6359       0.5563        0.6906  10.7891\n",
      "     22        0.6406       0.5563        \u001b[35m0.6869\u001b[0m  10.7376\n",
      "     23        0.6352       0.5563        0.6869  10.7824\n",
      "     24        \u001b[36m0.6266\u001b[0m       0.5563        0.6905  10.8270\n",
      "     25        0.6311       0.5563        0.6906  10.6833\n",
      "     26        0.6280       0.5563        0.6878  10.7464\n",
      "     27        0.6286       0.5563        0.6931  10.7443\n",
      "     28        0.6364       0.5563        0.6888  10.7903\n",
      "     29        0.6325       0.5563        0.6900  10.8031\n",
      "     30        0.6294       0.5563        0.6896  10.7567\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5056\u001b[0m       \u001b[32m0.4437\u001b[0m        \u001b[35m0.7613\u001b[0m  10.7752\n",
      "      2        0.5099       0.4437        \u001b[35m0.7406\u001b[0m  10.6895\n",
      "      3        \u001b[36m0.5027\u001b[0m       0.4437        0.7429  10.7632\n",
      "      4        0.5251       0.4437        \u001b[35m0.7305\u001b[0m  10.8292\n",
      "      5        0.5053       0.4436        0.7527  10.6832\n",
      "      6        0.5049       0.4437        0.7466  10.7353\n",
      "      7        0.5032       0.4437        0.7640  10.7633\n",
      "      8        \u001b[36m0.4984\u001b[0m       0.4437        \u001b[35m0.7261\u001b[0m  10.8822\n",
      "      9        0.4988       \u001b[32m0.4438\u001b[0m        0.7695  10.8849\n",
      "     10        0.5114       0.4437        0.7616  10.7420\n",
      "     11        0.5089       0.4437        0.7342  10.7846\n",
      "     12        0.5184       0.4437        0.7264  10.6745\n",
      "     13        0.5013       0.4437        0.7321  10.7334\n",
      "     14        0.5004       0.4437        0.7422  10.7660\n",
      "     15        0.5020       0.4437        0.7472  10.7886\n",
      "     16        0.4988       0.4437        \u001b[35m0.7217\u001b[0m  10.7892\n",
      "     17        0.5210       \u001b[32m0.4454\u001b[0m        1.0066  10.7886\n",
      "     18        0.4988       0.4447        0.9897  10.7453\n",
      "     19        0.5009       0.4439        0.7477  10.7678\n",
      "     20        0.5030       0.4436        0.7730  10.7867\n",
      "     21        0.4984       0.4437        1.0916  10.8180\n",
      "     22        0.4994       0.4438        1.1555  10.7345\n",
      "     23        0.5067       0.4438        1.4982  10.7624\n",
      "     24        0.4998       0.4438        0.8081  10.8328\n",
      "     25        \u001b[36m0.4980\u001b[0m       0.4439        0.7961  10.7467\n",
      "     26        0.5067       0.4439        0.9628  10.8301\n",
      "     27        0.5049       0.4440        0.7872  10.6946\n",
      "     28        0.5036       0.4440        0.7598  10.7104\n",
      "     29        0.5535       0.4440        0.8100  10.8956\n",
      "     30        0.5094       0.4437        0.7343  10.9686\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5711\u001b[0m       \u001b[32m0.6190\u001b[0m       \u001b[35m26.0417\u001b[0m  11.3400\n",
      "      2        0.5884       0.4949       84.8176  10.7793\n",
      "      3        0.5794       0.5776       51.9904  10.7535\n",
      "      4        0.5802       0.5378       87.2084  10.8860\n",
      "      5        0.5853       0.5322      177.6692  10.9031\n",
      "      6        0.5857       \u001b[32m0.6314\u001b[0m      132.3231  10.7242\n",
      "      7        0.5743       0.6127      211.4028  10.8221\n",
      "      8        0.5724       0.5626      192.9924  10.8552\n",
      "      9        \u001b[36m0.5708\u001b[0m       0.5947      341.0900  10.7265\n",
      "     10        0.5757       0.5848      363.7393  10.8360\n",
      "     11        0.5732       0.6052      593.1545  10.8218\n",
      "     12        0.5876       0.6051      506.1611  10.8159\n",
      "     13        0.5778       0.6155      382.2850  10.7567\n",
      "     14        0.5873       0.6051      821.4802  10.7701\n",
      "     15        0.5834       0.6159      967.2925  10.8350\n",
      "     16        \u001b[36m0.5659\u001b[0m       0.6146      727.4722  10.8144\n",
      "     17        0.5681       0.5680      393.4600  10.7873\n",
      "     18        \u001b[36m0.5592\u001b[0m       0.6280      903.9666  10.8104\n",
      "     19        0.5810       \u001b[32m0.6824\u001b[0m      313.1896  10.7762\n",
      "     20        0.5772       0.6185     1008.0880  10.8747\n",
      "     21        0.5768       0.6264     1002.1500  10.8274\n",
      "     22        0.5731       0.6200      760.7036  10.7271\n",
      "     23        0.5978       0.6021     1881.1871  10.7734\n",
      "     24        0.6134       0.5967      926.8647  10.8486\n",
      "     25        0.5749       0.6409     1106.5624  10.7270\n",
      "     26        0.5755       0.6060      664.2222  10.7339\n",
      "     27        \u001b[36m0.5569\u001b[0m       0.5660      622.0705  10.8145\n",
      "     28        0.5617       0.6374     2303.8208  10.8309\n",
      "     29        0.5618       0.5975     1975.1056  10.8078\n",
      "     30        0.5852       0.6005     1215.7300  10.7804\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6466\u001b[0m       \u001b[32m0.5563\u001b[0m        \u001b[35m0.6955\u001b[0m  10.8769\n",
      "      2        \u001b[36m0.6367\u001b[0m       0.5563        \u001b[35m0.6923\u001b[0m  11.1903\n",
      "      3        0.6600       0.5563        0.6958  10.6550\n",
      "      4        0.6460       0.5563        \u001b[35m0.6908\u001b[0m  10.8188\n",
      "      5        0.6395       0.5563        0.6909  10.8340\n",
      "      6        0.6394       0.5563        0.6936  10.8523\n",
      "      7        0.6415       0.5563        0.6956  10.7275\n",
      "      8        0.6382       0.5563        \u001b[35m0.6875\u001b[0m  10.7085\n",
      "      9        \u001b[36m0.6296\u001b[0m       0.5563        0.6880  10.8566\n",
      "     10        0.6393       0.5563        0.6920  10.6960\n",
      "     11        0.6318       0.5563        0.6891  10.8324\n",
      "     12        0.6445       0.5563        0.6906  10.7596\n",
      "     13        0.6366       0.5563        0.6942  10.7909\n",
      "     14        0.6371       0.5563        0.6906  10.8207\n",
      "     15        0.6350       0.5563        0.6973  10.7552\n",
      "     16        \u001b[36m0.6267\u001b[0m       0.5563        0.6920  10.7537\n",
      "     17        0.6284       0.5563        0.6967  10.6476\n",
      "     18        0.6431       0.5563        0.6943  10.7034\n",
      "     19        0.6345       0.5563        0.7033  10.7772\n",
      "     20        0.6324       0.5563        \u001b[35m0.6874\u001b[0m  10.7550\n",
      "     21        \u001b[36m0.6264\u001b[0m       0.5563        0.6894  10.7130\n",
      "     22        0.6351       0.5563        0.6919  10.7596\n",
      "     23        0.6436       0.5563        0.6906  10.9473\n",
      "     24        0.6324       0.5563        0.6935  11.2816\n",
      "     25        \u001b[36m0.6259\u001b[0m       0.5563        0.6917  11.1901\n",
      "     26        0.6318       0.5563        0.6971  10.8573\n",
      "     27        0.6492       0.5563        0.6935  10.8290\n",
      "     28        0.6310       0.5563        0.6911  10.7660\n",
      "     29        0.6301       0.5563        0.6898  10.7912\n",
      "     30        0.6423       0.5563        0.6916  10.9026\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.4945\u001b[0m       \u001b[32m0.5622\u001b[0m        \u001b[35m0.7352\u001b[0m  10.7832\n",
      "      2        \u001b[36m0.4942\u001b[0m       0.4453        0.8294  10.8149\n",
      "      3        0.5069       0.4440        0.8231  10.7347\n",
      "      4        0.5035       0.4448        0.7605  10.8725\n",
      "      5        0.4990       0.4437        \u001b[35m0.7028\u001b[0m  10.9105\n",
      "      6        0.4991       0.4438        0.7862  10.8904\n",
      "      7        0.5014       0.4437        0.7175  10.8796\n",
      "      8        0.4996       0.4436        0.7546  10.7540\n",
      "      9        0.4991       0.4441        0.8837  10.7546\n",
      "     10        0.4989       0.4444        0.7848  10.8791\n",
      "     11        0.5001       0.4437        0.8399  10.6810\n",
      "     12        0.4986       0.4438        0.8945  10.7891\n",
      "     13        0.5021       0.4452        0.9425  10.8557\n",
      "     14        0.4984       0.4450        0.9146  10.6499\n",
      "     15        0.4984       0.4449        0.9144  10.7546\n",
      "     16        0.4981       0.4449        0.9333  10.8625\n",
      "     17        0.4985       0.4452        1.1031  10.8240\n",
      "     18        0.4999       0.4461        1.8989  10.8993\n",
      "     19        0.5124       0.4440        1.0005  10.6252\n",
      "     20        0.5174       0.4447        1.5549  10.7601\n",
      "     21        0.4992       0.4446        0.9799  10.7943\n",
      "     22        0.4982       0.4446        0.9937  10.7627\n",
      "     23        0.4982       0.4446        0.9550  10.7027\n",
      "     24        0.4983       0.4447        1.0024  10.7886\n",
      "     25        0.4983       0.4448        1.0970  10.8597\n",
      "     26        0.4980       0.4448        1.0945  10.8394\n",
      "     27        0.4981       0.4458        1.1606  10.7013\n",
      "     28        0.4991       0.4458        1.8708  10.8219\n",
      "     29        0.4989       0.4448        0.8659  10.8071\n",
      "     30        0.5002       0.4453        0.9338  10.7377\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5777\u001b[0m       \u001b[32m0.5555\u001b[0m       \u001b[35m22.8151\u001b[0m  10.6547\n",
      "      2        \u001b[36m0.5586\u001b[0m       \u001b[32m0.6170\u001b[0m       47.8029  10.8557\n",
      "      3        0.5793       0.4957       88.4550  10.7711\n",
      "      4        0.5673       0.5800      210.7482  10.9759\n",
      "      5        0.5780       \u001b[32m0.6243\u001b[0m      108.6736  10.8230\n",
      "      6        0.5605       \u001b[32m0.6281\u001b[0m      256.4886  10.7772\n",
      "      7        0.5720       0.5813      169.6160  10.8523\n",
      "      8        0.5659       \u001b[32m0.6310\u001b[0m      275.8511  10.8570\n",
      "      9        0.5729       \u001b[32m0.6452\u001b[0m      178.8237  10.7456\n",
      "     10        0.5612       0.5566      406.5481  10.8480\n",
      "     11        0.5698       0.5790      268.1123  10.6977\n",
      "     12        0.5810       0.6223      471.4010  10.7052\n",
      "     13        0.5771       0.6302      357.4520  10.7707\n",
      "     14        0.5738       0.5940      171.9710  10.8131\n",
      "     15        0.5619       0.6321      267.1364  10.6903\n",
      "     16        0.5744       0.6210      350.3691  10.7951\n",
      "     17        0.5756       0.5993      339.0746  10.6812\n",
      "     18        0.5635       \u001b[32m0.6470\u001b[0m      440.5013  10.7362\n",
      "     19        \u001b[36m0.5584\u001b[0m       0.6315      492.6255  10.7623\n",
      "     20        0.5608       0.6148      631.5774  10.7756\n",
      "     21        \u001b[36m0.5539\u001b[0m       0.6411      433.0393  10.7714\n",
      "     22        0.5588       0.6422      377.8074  10.7401\n",
      "     23        0.5739       \u001b[32m0.6495\u001b[0m      403.4260  10.7655\n",
      "     24        0.5622       0.6489      741.1504  10.6582\n",
      "     25        0.5691       0.6481      761.1395  10.8354\n",
      "     26        0.5734       \u001b[32m0.6587\u001b[0m      733.1968  10.7588\n",
      "     27        0.5760       0.6432      671.2630  10.8954\n",
      "     28        0.5692       0.6499      643.3858  10.6840\n",
      "     29        0.5820       0.6584      650.9592  10.7461\n",
      "     30        0.5623       0.6532      873.5664  10.7445\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6277\u001b[0m       \u001b[32m0.5563\u001b[0m        \u001b[35m0.6935\u001b[0m  10.7974\n",
      "      2        0.6296       0.5563        \u001b[35m0.6869\u001b[0m  10.7865\n",
      "      3        0.6286       0.5563        0.6869  10.7928\n",
      "      4        0.6478       0.5563        \u001b[35m0.6868\u001b[0m  10.8672\n",
      "      5        0.6291       0.5563        0.6906  10.7982\n",
      "      6        0.6282       0.5563        0.7002  10.8729\n",
      "      7        \u001b[36m0.6264\u001b[0m       0.5563        0.6899  10.9013\n",
      "      8        \u001b[36m0.6263\u001b[0m       0.5563        0.6873  10.7971\n",
      "      9        \u001b[36m0.6251\u001b[0m       0.5563        0.6880  10.7878\n",
      "     10        \u001b[36m0.6169\u001b[0m       0.5563        0.6871  10.8229\n",
      "     11        \u001b[36m0.6146\u001b[0m       0.5563        0.6915  10.9026\n",
      "     12        0.6219       0.5563        0.6880  10.8407\n",
      "     13        0.6254       0.5563        0.6930  10.8093\n",
      "     14        0.6193       0.5563        0.6874  10.7643\n",
      "     15        0.6227       0.5563        0.6920  10.7886\n",
      "     16        \u001b[36m0.6145\u001b[0m       0.5563        0.6894  10.7198\n",
      "     17        0.6203       0.5563        0.6873  10.6919\n",
      "     18        0.6368       0.5563        0.6869  10.7885\n",
      "     19        \u001b[36m0.6120\u001b[0m       0.5563        0.6868  10.6664\n",
      "     20        0.6245       0.5563        0.6965  10.7222\n",
      "     21        0.6180       0.5563        0.6871  10.8039\n",
      "     22        0.6338       0.5563        0.6883  10.8377\n",
      "     23        0.6230       0.5563        0.6900  10.9091\n",
      "     24        0.6138       0.5563        0.6873  10.7870\n",
      "     25        0.6172       0.5563        0.6908  10.8140\n",
      "     26        0.6170       0.5563        0.6880  10.6961\n",
      "     27        0.6150       0.5563        0.6868  10.7088\n",
      "     28        0.6379       0.5563        0.6872  10.7860\n",
      "     29        0.6381       0.5563        0.6881  10.8292\n",
      "     30        0.6242       0.5563        0.6901  10.8415\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.5041\u001b[0m       \u001b[32m0.4437\u001b[0m        \u001b[35m0.7152\u001b[0m  10.7376\n",
      "      2        0.5074       \u001b[32m0.4439\u001b[0m        0.7726  10.7741\n",
      "      3        \u001b[36m0.5013\u001b[0m       \u001b[32m0.4441\u001b[0m        0.7282  10.8257\n",
      "      4        \u001b[36m0.4999\u001b[0m       0.4437        0.7509  10.8367\n",
      "      5        0.5067       \u001b[32m0.4450\u001b[0m        0.7833  10.8339\n",
      "      6        0.5053       0.4439        0.7891  10.8903\n",
      "      7        0.5002       0.4444        0.7549  10.8826\n",
      "      8        \u001b[36m0.4989\u001b[0m       0.4450        1.7929  10.8517\n",
      "      9        0.5104       \u001b[32m0.4452\u001b[0m        1.2931  10.8288\n",
      "     10        0.5016       \u001b[32m0.4453\u001b[0m        1.0508  10.7430\n",
      "     11        0.5062       0.4451        1.0335  10.8255\n",
      "     12        \u001b[36m0.4980\u001b[0m       0.4452        1.0358  10.8558\n",
      "     13        0.4984       \u001b[32m0.4457\u001b[0m        2.0415  10.7494\n",
      "     14        0.4981       0.4456        2.1214  10.8036\n",
      "     15        0.4982       \u001b[32m0.4463\u001b[0m        2.1053  10.8992\n",
      "     16        0.4986       \u001b[32m0.4464\u001b[0m        2.6236  10.7435\n",
      "     17        0.4980       \u001b[32m0.4464\u001b[0m        2.7169  10.8454\n",
      "     18        0.4980       0.4464        2.7248  10.7967\n",
      "     19        0.4981       0.4464        2.2007  10.8283\n",
      "     20        0.5078       0.4463        2.2049  10.7379\n",
      "     21        0.5029       0.4457        1.8699  10.8581\n",
      "     22        0.4982       0.4463        2.3308  10.9103\n",
      "     23        \u001b[36m0.4978\u001b[0m       0.4463        2.3357  10.7268\n",
      "     24        0.4980       0.4462        2.3279  10.7652\n",
      "     25        0.4981       0.4462        2.3338  10.7470\n",
      "     26        0.4981       0.4462        2.5755  10.7927\n",
      "     27        0.4982       0.4463        2.5949  10.7769\n",
      "     28        0.5074       0.4462        1.0563  10.7202\n",
      "     29        0.4980       0.4461        1.0542  10.8642\n",
      "     30        0.5121       0.4450        1.2035  10.8596\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.6331\u001b[0m       \u001b[32m0.5806\u001b[0m        \u001b[35m1.0359\u001b[0m  15.8510\n",
      "      2        \u001b[36m0.6269\u001b[0m       0.5770        \u001b[35m0.8467\u001b[0m  16.0371\n",
      "      3        \u001b[36m0.6258\u001b[0m       0.5450        1.4663  15.8753\n",
      "      4        \u001b[36m0.6245\u001b[0m       0.5453        2.8442  16.1434\n",
      "      5        \u001b[36m0.6241\u001b[0m       0.5249        3.3526  15.8095\n",
      "      6        \u001b[36m0.6234\u001b[0m       0.5237        3.5588  16.0897\n",
      "      7        \u001b[36m0.6233\u001b[0m       0.5296        4.9505  15.9090\n",
      "      8        0.6240       0.5015        4.3464  15.8936\n",
      "      9        0.6241       0.5131        4.2660  15.9412\n",
      "     10        \u001b[36m0.6223\u001b[0m       0.5189        8.4238  15.9613\n",
      "     11        \u001b[36m0.6219\u001b[0m       0.5135        8.1652  15.8762\n",
      "     12        0.6225       0.5237        6.0863  15.9852\n",
      "     13        0.6222       0.5222        5.9896  15.8560\n",
      "     14        0.6237       0.5124        6.4379  15.9128\n",
      "     15        \u001b[36m0.6217\u001b[0m       0.5081        7.3855  16.0841\n",
      "     16        0.6223       0.5133        7.9024  15.9555\n",
      "     17        0.6219       0.5530        7.6575  15.9735\n",
      "     18        \u001b[36m0.6217\u001b[0m       0.5258       21.2664  16.0919\n",
      "     19        \u001b[36m0.6217\u001b[0m       0.5055        9.6361  15.8544\n",
      "     20        0.6229       0.5371        9.3477  17.2221\n",
      "     21        0.6221       0.5241       15.8867  18.3770\n",
      "     22        0.6217       0.5056       12.2911  18.7431\n",
      "     23        \u001b[36m0.6215\u001b[0m       0.5156       10.4613  18.9679\n",
      "     24        \u001b[36m0.6211\u001b[0m       0.5110       14.7457  18.9300\n",
      "     25        \u001b[36m0.6210\u001b[0m       0.5084       23.1698  18.9617\n",
      "     26        0.6210       0.5107       11.6498  18.9913\n",
      "     27        0.6210       0.5162       22.1062  18.9023\n",
      "     28        0.6219       0.5275       17.3887  19.0028\n",
      "     29        \u001b[36m0.6205\u001b[0m       0.5185       14.9688  18.9450\n",
      "     30        \u001b[36m0.6205\u001b[0m       0.5212       12.5033  18.9457\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "}\n",
       "\n",
       "#sk-container-id-2.light {\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: black;\n",
       "  --sklearn-color-background: white;\n",
       "  --sklearn-color-border-box: black;\n",
       "  --sklearn-color-icon: #696969;\n",
       "}\n",
       "\n",
       "#sk-container-id-2.dark {\n",
       "  --sklearn-color-text-on-default-background: white;\n",
       "  --sklearn-color-background: #111;\n",
       "  --sklearn-color-border-box: white;\n",
       "  --sklearn-color-icon: #878787;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: center;\n",
       "  justify-content: center;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table {\n",
       "    font-family: monospace;\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table summary::marker {\n",
       "    font-size: 0.7rem;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       "/*\n",
       "    `table td`is set in notebook with right text-align.\n",
       "    We need to overwrite it.\n",
       "*/\n",
       ".estimator-table table td.param {\n",
       "    text-align: left;\n",
       "    position: relative;\n",
       "    padding: 0;\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td.value {\n",
       "    color:rgb(255, 94, 0);\n",
       "    background-color: transparent;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "/*\n",
       "    Styles for parameter documentation links\n",
       "    We need styling for visited so jupyter doesn't overwrite it\n",
       "*/\n",
       "a.param-doc-link,\n",
       "a.param-doc-link:link,\n",
       "a.param-doc-link:visited {\n",
       "    text-decoration: underline dashed;\n",
       "    text-underline-offset: .3em;\n",
       "    color: inherit;\n",
       "    display: block;\n",
       "    padding: .5em;\n",
       "}\n",
       "\n",
       "/* \"hack\" to make the entire area of the cell containing the link clickable */\n",
       "a.param-doc-link::before {\n",
       "    position: absolute;\n",
       "    content: \"\";\n",
       "    inset: 0;\n",
       "}\n",
       "\n",
       ".param-doc-description {\n",
       "    display: none;\n",
       "    position: absolute;\n",
       "    z-index: 9999;\n",
       "    left: 0;\n",
       "    padding: .5ex;\n",
       "    margin-left: 1.5em;\n",
       "    color: var(--sklearn-color-text);\n",
       "    box-shadow: .3em .3em .4em #999;\n",
       "    width: max-content;\n",
       "    text-align: left;\n",
       "    max-height: 10em;\n",
       "    overflow-y: auto;\n",
       "\n",
       "    /* unfitted */\n",
       "    background: var(--sklearn-color-unfitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       "/* Fitted state for parameter tooltips */\n",
       ".fitted .param-doc-description {\n",
       "    /* fitted */\n",
       "    background: var(--sklearn-color-fitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".param-doc-link:hover .param-doc-description {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=NeuralNetClassifier(_params_to_validate={&#x27;module__input_dim&#x27;, &#x27;iterator_train__shuffle&#x27;}, batch_size=128, callbacks=None, compile=False, criterion=BCEWithLogitsLoss(), dataset=&lt;class &#x27;skorch.dataset.Dataset&#x27;&gt;, device=&#x27;cuda&#x27;, iterator_train=&lt;class &#x27;torch.utils.data.dataloader.DataLoader&#x27;&gt;, iterator_train__shuffle=True, iterator_valid...ochs=30, module=&lt;class &#x27;__main__.FlightNNClf&#x27;&gt;, module__input_dim=4038, optimizer=&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;, predict_nonlinearity=&#x27;auto&#x27;, torch_load_kwargs=None, use_caching=&#x27;auto&#x27;, verbose=1, warm_start=False),\n",
       "             n_jobs=1,\n",
       "             param_grid={&#x27;lr&#x27;: [0.01, 0.05, 0.1],\n",
       "                         &#x27;module__hidden_dim1&#x27;: [10, 25, 35, 50],\n",
       "                         &#x27;module__hidden_dim2&#x27;: [10, 25, 35, 50]},\n",
       "             scoring=&#x27;balanced_accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('estimator',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=estimator,-estimator%20object\">\n",
       "            estimator\n",
       "            <span class=\"param-doc-description\">estimator: estimator object<br><br>This is assumed to implement the scikit-learn estimator interface.<br>Either estimator needs to provide a ``score`` function,<br>or ``scoring`` must be passed.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&lt;class &#x27;skorc...ut_dim=4038,\n",
       ")</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('param_grid',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=param_grid,-dict%20or%20list%20of%20dictionaries\">\n",
       "            param_grid\n",
       "            <span class=\"param-doc-description\">param_grid: dict or list of dictionaries<br><br>Dictionary with parameters names (`str`) as keys and lists of<br>parameter settings to try as values, or a list of such<br>dictionaries, in which case the grids spanned by each dictionary<br>in the list are explored. This enables searching over any sequence<br>of parameter settings.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">{&#x27;lr&#x27;: [0.01, 0.05, ...], &#x27;module__hidden_dim1&#x27;: [10, 25, ...], &#x27;module__hidden_dim2&#x27;: [10, 25, ...]}</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('scoring',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=scoring,-str%2C%20callable%2C%20list%2C%20tuple%20or%20dict%2C%20default%3DNone\">\n",
       "            scoring\n",
       "            <span class=\"param-doc-description\">scoring: str, callable, list, tuple or dict, default=None<br><br>Strategy to evaluate the performance of the cross-validated model on<br>the test set.<br><br>If `scoring` represents a single score, one can use:<br><br>- a single string (see :ref:`scoring_string_names`);<br>- a callable (see :ref:`scoring_callable`) that returns a single value;<br>- `None`, the `estimator`'s<br>  :ref:`default evaluation criterion <scoring_api_overview>` is used.<br><br>If `scoring` represents multiple scores, one can use:<br><br>- a list or tuple of unique strings;<br>- a callable returning a dictionary where the keys are the metric<br>  names and the values are the metric scores;<br>- a dictionary with metric names as keys and callables as values.<br><br>See :ref:`multimetric_grid_search` for an example.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;balanced_accuracy&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=n_jobs,-int%2C%20default%3DNone\">\n",
       "            n_jobs\n",
       "            <span class=\"param-doc-description\">n_jobs: int, default=None<br><br>Number of jobs to run in parallel.<br>``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.<br>``-1`` means using all processors. See :term:`Glossary <n_jobs>`<br>for more details.<br><br>.. versionchanged:: v0.20<br>   `n_jobs` default changed from 1 to None</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('refit',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=refit,-bool%2C%20str%2C%20or%20callable%2C%20default%3DTrue\">\n",
       "            refit\n",
       "            <span class=\"param-doc-description\">refit: bool, str, or callable, default=True<br><br>Refit an estimator using the best found parameters on the whole<br>dataset.<br><br>For multiple metric evaluation, this needs to be a `str` denoting the<br>scorer that would be used to find the best parameters for refitting<br>the estimator at the end.<br><br>Where there are considerations other than maximum score in<br>choosing a best estimator, ``refit`` can be set to a function which<br>returns the selected ``best_index_`` given ``cv_results_``. In that<br>case, the ``best_estimator_`` and ``best_params_`` will be set<br>according to the returned ``best_index_`` while the ``best_score_``<br>attribute will not be available.<br><br>The refitted estimator is made available at the ``best_estimator_``<br>attribute and permits using ``predict`` directly on this<br>``GridSearchCV`` instance.<br><br>Also for multiple metric evaluation, the attributes ``best_index_``,<br>``best_score_`` and ``best_params_`` will only be available if<br>``refit`` is set and all of them will be determined w.r.t this specific<br>scorer.<br><br>See ``scoring`` parameter to know more about multiple metric<br>evaluation.<br><br>See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`<br>to see how to design a custom selection strategy using a callable<br>via `refit`.<br><br>See :ref:`this example<br><sphx_glr_auto_examples_model_selection_plot_grid_search_refit_callable.py>`<br>for an example of how to use ``refit=callable`` to balance model<br>complexity and cross-validated score.<br><br>.. versionchanged:: 0.20<br>    Support for callable added.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('cv',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=cv,-int%2C%20cross-validation%20generator%20or%20an%20iterable%2C%20default%3DNone\">\n",
       "            cv\n",
       "            <span class=\"param-doc-description\">cv: int, cross-validation generator or an iterable, default=None<br><br>Determines the cross-validation splitting strategy.<br>Possible inputs for cv are:<br><br>- None, to use the default 5-fold cross validation,<br>- integer, to specify the number of folds in a `(Stratified)KFold`,<br>- :term:`CV splitter`,<br>- An iterable yielding (train, test) splits as arrays of indices.<br><br>For integer/None inputs, if the estimator is a classifier and ``y`` is<br>either binary or multiclass, :class:`StratifiedKFold` is used. In all<br>other cases, :class:`KFold` is used. These splitters are instantiated<br>with `shuffle=False` so the splits will be the same across calls.<br><br>Refer :ref:`User Guide <cross_validation>` for the various<br>cross-validation strategies that can be used here.<br><br>.. versionchanged:: 0.22<br>    ``cv`` default value if None changed from 3-fold to 5-fold.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=verbose,-int\">\n",
       "            verbose\n",
       "            <span class=\"param-doc-description\">verbose: int<br><br>Controls the verbosity: the higher, the more messages.<br><br>- >1 : the computation time for each fold and parameter candidate is<br>  displayed;<br>- >2 : the score is also displayed;<br>- >3 : the fold and candidate parameter indexes are also displayed<br>  together with the starting time of the computation.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('pre_dispatch',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=pre_dispatch,-int%2C%20or%20str%2C%20default%3D%272%2An_jobs%27\">\n",
       "            pre_dispatch\n",
       "            <span class=\"param-doc-description\">pre_dispatch: int, or str, default='2*n_jobs'<br><br>Controls the number of jobs that get dispatched during parallel<br>execution. Reducing this number can be useful to avoid an<br>explosion of memory consumption when more jobs get dispatched<br>than CPUs can process. This parameter can be:<br><br>- None, in which case all the jobs are immediately created and spawned. Use<br>  this for lightweight and fast-running jobs, to avoid delays due to on-demand<br>  spawning of the jobs<br>- An int, giving the exact number of total jobs that are spawned<br>- A str, giving an expression as a function of n_jobs, as in '2*n_jobs'</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;2*n_jobs&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('error_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=error_score,-%27raise%27%20or%20numeric%2C%20default%3Dnp.nan\">\n",
       "            error_score\n",
       "            <span class=\"param-doc-description\">error_score: 'raise' or numeric, default=np.nan<br><br>Value to assign to the score if an error occurs in estimator fitting.<br>If set to 'raise', the error is raised. If a numeric value is given,<br>FitFailedWarning is raised. This parameter does not affect the refit<br>step, which will always raise the error.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">nan</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('return_train_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=return_train_score,-bool%2C%20default%3DFalse\">\n",
       "            return_train_score\n",
       "            <span class=\"param-doc-description\">return_train_score: bool, default=False<br><br>If ``False``, the ``cv_results_`` attribute will not include training<br>scores.<br>Computing training scores is used to get insights on how different<br>parameter settings impact the overfitting/underfitting trade-off.<br>However computing the scores on the training set can be computationally<br>expensive and is not strictly required to select the parameters that<br>yield the best generalization performance.<br><br>.. versionadded:: 0.19<br><br>.. versionchanged:: 0.21<br>    Default value was changed from ``True`` to ``False``</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: NeuralNetClassifier</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___\"><pre>&lt;class &#x27;skorch.classifier.NeuralNetClassifier&#x27;&gt;[initialized](\n",
       "  module_=FlightNNClf(\n",
       "    (fc1): Linear(in_features=4038, out_features=10, bias=True)\n",
       "    (nonlin): ReLU()\n",
       "    (dropout1): Dropout(p=0.5, inplace=False)\n",
       "    (fc2): Linear(in_features=10, out_features=25, bias=True)\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "    (output): Linear(in_features=25, out_features=1, bias=True)\n",
       "  ),\n",
       ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>NeuralNetClassifier</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('module',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">module</td>\n",
       "            <td class=\"value\">&lt;class &#x27;__main__.FlightNNClf&#x27;&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('criterion',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">criterion</td>\n",
       "            <td class=\"value\">BCEWithLogitsLoss()</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('train_split',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">train_split</td>\n",
       "            <td class=\"value\">&lt;skorch.datas...0029A52D53950&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('classes',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">classes</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('optimizer',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">optimizer</td>\n",
       "            <td class=\"value\">&lt;class &#x27;torch...im.adam.Adam&#x27;&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('lr',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">lr</td>\n",
       "            <td class=\"value\">0.05</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_epochs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_epochs</td>\n",
       "            <td class=\"value\">30</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('batch_size',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">batch_size</td>\n",
       "            <td class=\"value\">128</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('iterator_train',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">iterator_train</td>\n",
       "            <td class=\"value\">&lt;class &#x27;torch...r.DataLoader&#x27;&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('iterator_valid',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">iterator_valid</td>\n",
       "            <td class=\"value\">&lt;class &#x27;torch...r.DataLoader&#x27;&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dataset',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dataset</td>\n",
       "            <td class=\"value\">&lt;class &#x27;skorc...aset.Dataset&#x27;&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('predict_nonlinearity',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">predict_nonlinearity</td>\n",
       "            <td class=\"value\">&#x27;auto&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">warm_start</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('device',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">device</td>\n",
       "            <td class=\"value\">&#x27;cuda&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('compile',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">compile</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('use_caching',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">use_caching</td>\n",
       "            <td class=\"value\">&#x27;auto&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('torch_load_kwargs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">torch_load_kwargs</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('_params_to_validate',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">_params_to_validate</td>\n",
       "            <td class=\"value\">{&#x27;iterator_train__shuffle&#x27;, &#x27;module__hidden_dim1&#x27;, &#x27;module__hidden_dim2&#x27;, &#x27;module__input_dim&#x27;}</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('module__input_dim',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">module__input_dim</td>\n",
       "            <td class=\"value\">4038</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('iterator_train__shuffle',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">iterator_train__shuffle</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('module__hidden_dim1',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">module__hidden_dim1</td>\n",
       "            <td class=\"value\">10</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('module__hidden_dim2',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">module__hidden_dim2</td>\n",
       "            <td class=\"value\">25</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__epoch_timer',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__epoch_timer</td>\n",
       "            <td class=\"value\">&lt;skorch.callb...0029A274C59A0&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__train_loss',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__train_loss</td>\n",
       "            <td class=\"value\">&lt;skorch.callb...0029A12179B00&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__train_loss__name',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__train_loss__name</td>\n",
       "            <td class=\"value\">&#x27;train_loss&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__train_loss__lower_is_better',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__train_loss__lower_is_better</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__train_loss__on_train',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__train_loss__on_train</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__valid_loss',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__valid_loss</td>\n",
       "            <td class=\"value\">&lt;skorch.callb...0029A12179BE0&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__valid_loss__name',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__valid_loss__name</td>\n",
       "            <td class=\"value\">&#x27;valid_loss&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__valid_loss__lower_is_better',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__valid_loss__lower_is_better</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__valid_loss__on_train',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__valid_loss__on_train</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__valid_acc',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__valid_acc</td>\n",
       "            <td class=\"value\">&lt;skorch.callb...0029A52DD90D0&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__valid_acc__scoring',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__valid_acc__scoring</td>\n",
       "            <td class=\"value\">&#x27;accuracy&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__valid_acc__lower_is_better',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__valid_acc__lower_is_better</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__valid_acc__on_train',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__valid_acc__on_train</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__valid_acc__name',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__valid_acc__name</td>\n",
       "            <td class=\"value\">&#x27;valid_acc&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__valid_acc__target_extractor',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__valid_acc__target_extractor</td>\n",
       "            <td class=\"value\">&lt;function to_...0029A0EF7E400&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__valid_acc__use_caching',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__valid_acc__use_caching</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__print_log',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__print_log</td>\n",
       "            <td class=\"value\">&lt;skorch.callb...0029AC7296FD0&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__print_log__keys_ignored',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__print_log__keys_ignored</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__print_log__sink',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__print_log__sink</td>\n",
       "            <td class=\"value\">&lt;built-in function print&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__print_log__tablefmt',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__print_log__tablefmt</td>\n",
       "            <td class=\"value\">&#x27;simple&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__print_log__floatfmt',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__print_log__floatfmt</td>\n",
       "            <td class=\"value\">&#x27;.4f&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks__print_log__stralign',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks__print_log__stralign</td>\n",
       "            <td class=\"value\">&#x27;right&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling\n",
       "        .textContent.trim().split(' ')[0];\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "\n",
       "\n",
       "/**\n",
       " * Adapted from Skrub\n",
       " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
       " * @returns \"light\" or \"dark\"\n",
       " */\n",
       "function detectTheme(element) {\n",
       "    const body = document.querySelector('body');\n",
       "\n",
       "    // Check VSCode theme\n",
       "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
       "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
       "\n",
       "    if (themeKindAttr && themeNameAttr) {\n",
       "        const themeKind = themeKindAttr.toLowerCase();\n",
       "        const themeName = themeNameAttr.toLowerCase();\n",
       "\n",
       "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
       "            return \"dark\";\n",
       "        }\n",
       "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
       "            return \"light\";\n",
       "        }\n",
       "    }\n",
       "\n",
       "    // Check Jupyter theme\n",
       "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
       "        return 'dark';\n",
       "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
       "        return 'light';\n",
       "    }\n",
       "\n",
       "    // Guess based on a parent element's color\n",
       "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
       "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
       "    if (match) {\n",
       "        const [r, g, b] = [\n",
       "            parseFloat(match[1]),\n",
       "            parseFloat(match[2]),\n",
       "            parseFloat(match[3])\n",
       "        ];\n",
       "\n",
       "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
       "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
       "\n",
       "        if (luma > 180) {\n",
       "            // If the text is very bright we have a dark theme\n",
       "            return 'dark';\n",
       "        }\n",
       "        if (luma < 75) {\n",
       "            // If the text is very dark we have a light theme\n",
       "            return 'light';\n",
       "        }\n",
       "        // Otherwise fall back to the next heuristic.\n",
       "    }\n",
       "\n",
       "    // Fallback to system preference\n",
       "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
       "}\n",
       "\n",
       "\n",
       "function forceTheme(elementId) {\n",
       "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
       "    if (estimatorElement === null) {\n",
       "        console.error(`Element with id ${elementId} not found.`);\n",
       "    } else {\n",
       "        const theme = detectTheme(estimatorElement);\n",
       "        estimatorElement.classList.add(theme);\n",
       "    }\n",
       "}\n",
       "\n",
       "forceTheme('sk-container-id-2');</script></body>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=NeuralNetClassifier(_params_to_validate={'module__input_dim', 'iterator_train__shuffle'}, batch_size=128, callbacks=None, compile=False, criterion=BCEWithLogitsLoss(), dataset=<class 'skorch.dataset.Dataset'>, device='cuda', iterator_train=<class 'torch.utils.data.dataloader.DataLoader'>, iterator_train__shuffle=True, iterator_valid...ochs=30, module=<class '__main__.FlightNNClf'>, module__input_dim=4038, optimizer=<class 'torch.optim.adam.Adam'>, predict_nonlinearity='auto', torch_load_kwargs=None, use_caching='auto', verbose=1, warm_start=False),\n",
       "             n_jobs=1,\n",
       "             param_grid={'lr': [0.01, 0.05, 0.1],\n",
       "                         'module__hidden_dim1': [10, 25, 35, 50],\n",
       "                         'module__hidden_dim2': [10, 25, 35, 50]},\n",
       "             scoring='balanced_accuracy')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# X_train_nn = best_forest_model.named_steps['data_transformer'].transform(X_train)\n",
    "# X_test_nn = best_forest_model.named_steps['data_transformer'].transform(X_test)\n",
    "\n",
    "# import gc\n",
    "\n",
    "# # get rid of other grid searches\n",
    "# del best_forest_model\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "nn_clf = NeuralNetClassifier(\n",
    "    FlightNNClf,\n",
    "    module__input_dim=X_train_nn.shape[1],\n",
    "    max_epochs=30,\n",
    "    criterion=nn.BCEWithLogitsLoss(),\n",
    "    optimizer=optim.Adam,\n",
    "    lr=0.1,\n",
    "    device='cuda',\n",
    "    iterator_train__shuffle=True # shuffle training data on each epoch\n",
    ")\n",
    "\n",
    "# We will be tuning our decision tree\n",
    "# param_distribs = {'max_epochs': randint(low=10, high=50),\n",
    "#                   'lr': loguniform(1e-5, 1e-1),\n",
    "#                   'module__hidden_dim1': randint(low=10, high=50),\n",
    "#                   'module__hidden_dim2': randint(low=10, high=50),\n",
    "#                   'module__dropout_rate1': uniform(loc=0, scale=1),\n",
    "#                   'module__dropout_rate2': uniform(loc=0, scale=1)}\n",
    "# nn_search = RandomizedSearchCV(nn_clf, param_distributions=param_distribs, \n",
    "#                                 n_iter=10, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "params = {\n",
    "    'lr': [0.01, 0.05, 0.1],\n",
    "    'module__hidden_dim1': [10, 25, 35, 50],\n",
    "    'module__hidden_dim2': [10, 25, 35, 50],\n",
    "    # 'module__dropout_rate1': [0, 0.25, 0.5],\n",
    "    # 'module__dropout_rate2': [0, 0.25, 0.5]\n",
    "}\n",
    "nn_search = GridSearchCV(nn_clf, params, cv=3, n_jobs=1, scoring='balanced_accuracy')\n",
    "nn_search.fit(X_train_nn, y_train.values.ravel().reshape(-1,1).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af8ce689-4c04-4211-8a41-d419d459b372",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42215738520060797 {'lr': 0.05, 'module__hidden_dim1': 10, 'module__hidden_dim2': 25}\n"
     ]
    }
   ],
   "source": [
    "print(nn_search.best_score_, nn_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "e991189a-b957-4a26-98da-c32c40669b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "class CustomTabularDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.length = len(labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "train_dataset = CustomTabularDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = CustomTabularDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = CustomTabularDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 256\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# targets = y_train_tensor.view(-1).long()\n",
    "\n",
    "# # 2. Calculate weights for each class (Inverse of frequency)\n",
    "# class_sample_count = torch.tensor([(targets == 0).sum(), (targets == 1).sum()])\n",
    "# weight = 1. / class_sample_count.float()\n",
    "\n",
    "# # 3. Assign a weight to every individual sample in the dataset\n",
    "# samples_weight = torch.tensor([weight[t] for t in targets])\n",
    "\n",
    "# # 4. Create the sampler\n",
    "# sampler = WeightedRandomSampler(\n",
    "#     weights=samples_weight, \n",
    "#     num_samples=len(samples_weight), \n",
    "#     replacement=True\n",
    "# )\n",
    "\n",
    "# # 5. Update DataLoader (Remove 'shuffle=True' because sampler handles it)\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset, \n",
    "#     batch_size=64, \n",
    "#     sampler=sampler  # This is the key change\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "17dc44e9-e22d-4582-988f-9c510df2c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import sigmoid\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, 128)\n",
    "        self.bn = nn.BatchNorm1d(128)    #re-center the sum of features to prevent predicting 0 every time\n",
    "        self.activation = nn.LeakyReLU(0.01)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.layer_2 = nn.Linear(128, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) # 1 output unit for binary classification\n",
    "        # self.layer_out = nn.Linear(512, 1) # 1 output unit for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.activation(self.layer_2(x))\n",
    "        # x = sigmoid(self.layer_out(x)) # Sigmoid activation for binary output\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "\n",
    "model = BinaryClassifier(input_dim=X_train_nn.shape[1])\n",
    "\n",
    "# def init_weights(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
    "#         if m.bias is not None:\n",
    "#             nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "738031ef-7ba8-408f-9a15-110385ca94c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryClassifier(\n",
       "  (layer_1): Linear(in_features=4038, out_features=128, bias=True)\n",
       "  (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (activation): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (layer_2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure I am using the GPU\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "8c7e3834-3fbc-4a5b-b7d2-8b9050520e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "# pos_weight = torch.tensor([3.0]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss() # bias that the delayed classification is 3x more important\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.98)\n",
    "\n",
    "# # 1. Force the model to be extremely 'optimistic'\n",
    "# with torch.no_grad():\n",
    "#     # If using a single layer named 'layer1':\n",
    "#     # model.layer1.bias.fill_(10.0) \n",
    "    \n",
    "#     # If using nn.Sequential named 'net':\n",
    "#     # model.net[-1].bias.fill_(10.0)\n",
    "    \n",
    "#     # Generic version that finds the last linear layer:\n",
    "#     for module in reversed(list(model.modules())):\n",
    "#         if isinstance(module, nn.Linear):\n",
    "#             module.bias.fill_(10.0)\n",
    "#             print(\"Manual Override: Bias set to 10.0\")\n",
    "#             break\n",
    "\n",
    "# # 2. Run ONE inference step immediately (before training)\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     sample_input, _ = next(iter(train_loader))\n",
    "#     sample_output = torch.sigmoid(model(sample_input.to(device)))\n",
    "#     print(f\"Probabilities after override: {sample_output.mean().item():.4f}\")\n",
    "\n",
    "# model.train()\n",
    "# for i, (inputs, labels) in enumerate(train_loader):\n",
    "#     if i > 20: break \n",
    "    \n",
    "#     inputs, labels = inputs.to(device), labels.to(device).view(-1, 1).float()\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(inputs)\n",
    "#     loss = criterion(outputs, labels)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     probs = torch.sigmoid(outputs)\n",
    "#     print(f\"Step {i} Avg Prob: {prob:.4f}\")\n",
    "#     print(f\"Prob Min: {probs.min().item():.4f} | Max: {probs.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "a037e0a2-37d6-4ef3-9767-0f266edc59a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 0.6567 | Val Loss: 0.6187\n",
      "Epoch 2/15 | Train Loss: 0.6254 | Val Loss: 0.6077\n",
      "Epoch 3/15 | Train Loss: 0.6197 | Val Loss: 0.6046\n",
      "Epoch 4/15 | Train Loss: 0.6156 | Val Loss: 0.6028\n",
      "Epoch 5/15 | Train Loss: 0.6132 | Val Loss: 0.6021\n",
      "Epoch 6/15 | Train Loss: 0.6114 | Val Loss: 0.6013\n",
      "Epoch 7/15 | Train Loss: 0.6103 | Val Loss: 0.6011\n",
      "Epoch 8/15 | Train Loss: 0.6092 | Val Loss: 0.6006\n",
      "Epoch 9/15 | Train Loss: 0.6075 | Val Loss: 0.6000\n",
      "Epoch 10/15 | Train Loss: 0.6067 | Val Loss: 0.6002\n",
      "Epoch 11/15 | Train Loss: 0.6059 | Val Loss: 0.5997\n",
      "Epoch 12/15 | Train Loss: 0.6054 | Val Loss: 0.5996\n",
      "Epoch 13/15 | Train Loss: 0.6043 | Val Loss: 0.5996\n",
      "Epoch 14/15 | Train Loss: 0.6034 | Val Loss: 0.5994\n",
      "Epoch 15/15 | Train Loss: 0.6029 | Val Loss: 0.5995\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.76      0.73     50704\n",
      "         1.0       0.67      0.60      0.63     40435\n",
      "\n",
      "    accuracy                           0.69     91139\n",
      "   macro avg       0.69      0.68      0.68     91139\n",
      "weighted avg       0.69      0.69      0.69     91139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = [] # Storage for plotting\n",
    "val_losses = []   # Storage for plotting\n",
    "epochs = 15\n",
    "\n",
    "# --- 4. Training & Validation Loop ---\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    batch_train_losses = []\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).float().view(-1, 1) # SHAPE GUARD\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_train_losses.append(loss.item())\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    batch_val_losses = []\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float().view(-1, 1)\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            batch_val_losses.append(loss.item())\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Store average losses for this epoch\n",
    "    epoch_train_loss = np.mean(batch_train_losses)\n",
    "    epoch_val_loss = np.mean(batch_val_losses)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "# Final classification check\n",
    "print(\"\\nFinal Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "58ad7b7c-9fff-439d-b8e2-2919228aca6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='red')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('BCE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show() # Displays the chart after training is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "9da6ebe3-e59d-4775-be1b-5cbca0e97003",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "# Ensure model is on the correct device (CPU or GPU)\n",
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        # _, preds = torch.max(outputs, 1) # Get the predicted class index\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs > 0.5).float()\n",
    "        \n",
    "        # Move to CPU and convert to NumPy for sklearn compatibility\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "7055477b-b358-4751-b891-651e60e03aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73     82230\n",
      "           1       0.60      0.57      0.59     56081\n",
      "\n",
      "    accuracy                           0.67    138311\n",
      "   macro avg       0.66      0.66      0.66    138311\n",
      "weighted avg       0.67      0.67      0.67    138311\n",
      "\n",
      "Final Test Accuracy: 67.48%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print a comprehensive summary per class\n",
    "report = classification_report(all_labels, all_preds)\n",
    "print(report)\n",
    "\n",
    "# For a single score across the whole dataset\n",
    "final_accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Final Test Accuracy: {final_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675420b8-7265-4626-812e-f35ef79fc224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
